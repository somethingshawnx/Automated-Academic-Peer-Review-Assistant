{
  "0": {
    "pdf_path": "data/pdfs\\machine learning_paper_1.pdf",
    "text_excerpt": "lecture notes:\nOptimization for Machine Learning\nversion 0.57\nAll rights reserved.\nElad Hazan1\n1www.cs.princeton.edu/ ~ehazanarXiv:1909.03550v1  [cs.LG]  8 Sep 2019iiPreface\nThis text was written to accompany a series of lectures given at the Machine\nLearning Summer School Buenos Aires, following a lecture series at the\nSimons Center for Theoretical Computer Science, Berkeley. It was extended\nfor the course COS 598D - Optimization for Machine Learning, Princeton\nUniversity, Spring 2019.\nI am grateful to Paula Gradu for proofreading parts of this manuscript.\nI'm also thankful for the help of the following students and colleagues for\ncorrections and suggestions to this text: Udaya Ghai, John Hallman, No\u0013 e\nPion, Xinyi Chen.\niiiiv Preface\nFigure 1: Professor Arkadi Nemirovski, Pioneer of mathematical optimiza-\ntionContents\nPreface iii\n1 Introduction 3\n1.1 Examples of optimization problems in machine learning . . . 4\n1.1.1 Empirical Risk Minimization . . . . . . . . . . . . . . 4\n1.1.2 Matrix completion and recommender systems . . . . . 6\n1.1.3 Learning in Linear Dynamical Systems . . . . . . . . 7\n1.2 Why is mathematical programming hard? . . . . . . . . . . . 8\n1.2.1 The computational model . . . . . . . . . . . . . . . . 8\n1.2.2 Hardness of constrained mathematical programming . 9\n2 Basic concepts in optimization and analysis 11\n2.1 Basic de\fnitions and the notion of convexity . . . . . . . . . . 11\n2.1.1 Projections onto convex sets . . . . . . . . . . . . . . . 13\n2.1.2 Introduction to optimality conditions . . . . . . . . . . 14\n2.1.3 Solution concepts for non-convex optimization . . . . 15\n2.2 Potentials for distance to optimality . . . . . . . . . . . . . . 16\n2.3 Gradient descent and the Polyak stepsize . . . . . . . . . . . 18\n2.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.5 Bibliographic remarks . . . . . . . . . . . . . . . . . . . . . . 23\n3 Stochastic Gradient Descent 25\n3.1 Training feedforward neural networks . . . . . . . .",
    "title": "Lecture Notes: Optimization for Machine Learning",
    "abstract": "Lecture notes on optimization for machine learning, derived from a course at\nPrinceton University and tutorials given in MLSS, Buenos Aires, as well as\nSimons Foundation, Berkeley.",
    "link": "http://arxiv.org/abs/1909.03550v1",
    "published": "2019-09-08T21:49:42Z"
  },
  "1": {
    "pdf_path": "data/pdfs\\machine learning_paper_10.pdf",
    "text_excerpt": "arXiv:1911.06612v1  [cs.LG]  12 Nov 2019Position Paper: Towards Transparent Machine\nLearning\nDustin Juliano\nNov. 1, 2019\nAbstract\nTransparent machine learning is introduced as an alternati ve form of\nmachine learning, where both the model and the learning syst em are rep-\nresented in source code form. The goal of this project is to en able direct\nhuman understanding of machine learning models, giving us t he ability to\nlearn, verify, and reﬁne them as programs. If solved, this te chnology could\nrepresent a best-case scenario for the safety and security o f AI systems\ngoing forward.\n1 Introduction\nCurrent machine learning (ML) systems produce models that a re diﬃcult or\nimpossible to understand. This poses clear challenges with security, safety, and\nbias in these deployments. Opaque models also make it diﬃcul t to gain insight\ninto the automated decision-making process.\nAs a result of this, interpretable or explainable ML have bec ome active areas\nof research [ 1,2], with a notable interest from DARPA [ 3]. However, those\napproaches are focused on analyzing models that are inheren tly resistant to\nhuman understanding due to the way in which they are represen ted.\nTransparent machine learning (TML) is intended to solve the se problems by\nproducing models and data that we can understand. It would do this by repre-\nsenting and modifying source code representations. This, i n turn, would result\nin a potentially self-contained executable that could be de ployed directly.\nIn addition to the source code model, the TML system itself ma y be embedded\ninto the output program. This would give it the ability to con tinuously update\nitself. Embedding, however, is not a requirement; the learn er may be suppressed\nso that it is not emitted with the ﬁnal program. This could be f or safety purposes\nor to ensure stability of the deployment.\nA complex program may require auxiliary information, such a s labels, lookup\ntables, or a database of some kind. Transparent machine lear ning sys",
    "title": "Position Paper: Towards Transparent Machine Learning",
    "abstract": "Transparent machine learning is introduced as an alternative form of machine\nlearning, where both the model and the learning system are represented in\nsource code form. The goal of this project is to enable direct human\nunderstanding of machine learning models, giving us the ability to learn,\nverify, and refine them as programs. If solved, this technology could represent\na best-case scenario for the safety and security of AI systems going forward.",
    "link": "http://arxiv.org/abs/1911.06612v1",
    "published": "2019-11-12T10:49:55Z"
  },
  "2": {
    "pdf_path": "data/pdfs\\machine learning_paper_100.pdf",
    "text_excerpt": "Chapter 2\nClassic machine learning\nmethods\nJohann Faouzi*1and Olivier Colliot2\n1CREST, ENSAI, Campus de Ker-Lann, 51 Rue Blaise Pascal, BP 37203 –\n35172 Bruz Cedex, France\n2Sorbonne Universit´ e, Institut du Cerveau - Paris Brain Institute - ICM,\nCNRS, Inria, Inserm, AP-HP, Hˆ opital de la Piti´ e-Salpˆ etri` ere, F-75013, Paris,\nFrance\n*Corresponding author: e-mail address: johann.faouzi@gmail.com\nAbstract\nIn this chapter, we present the main classic machine learning methodss.\nA large part of the chapter is devoted to supervised learning techniques\nfor classification and regression, including nearest-neighbor methods, lin-\near and logistic regressions, support vector machines and tree-based algo-\nrithms. We also describe the problem of overfitting as well as strategies\nto overcome it. We finally provide a brief overview of unsupervised learn-\ning methods, namely for clustering and dimensionality reduction. The\nchapter does not cover neural networks and deep learning as these will\nbe presented in Chapters 3, 4, 5 and 6.\nKeywords: machine learning, classification, regression, clustering,\ndimensionality reduction\n1. Introduction\nThis chapter presents the main classic machine learning (ML) methods.\nThere is a focus on supervised learning methods for classification and\nregression, but we also describe some unsupervised approaches. The\nTo appear in\nO. Colliot (Ed.), Machine Learning for Brain Disorders , SpringerarXiv:2310.11470v1  [cs.LG]  24 May 20232 Faouzi and Colliot\nchapter is meant to be readable by someone with no background in ma-\nchine learning. It is nevertheless necessary to have some basic notions\nof linear algebra, probabilities and statistics. If this is not the case, we\nrefer the reader to chapters 2 and 3 of [1].\nThe rest of this chapter is organized as follows. Rather than grouping\nmethods by categories (for instance classification or regression methods),\nwe chose to present methods by increasing order of complexity. We first\nprovide the notations in Se",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "3": {
    "pdf_path": "data/pdfs\\machine learning_paper_101.pdf",
    "text_excerpt": "arXiv:1501.04309v1  [cs.IT]  18 Jan 2015Information Theory and its Relation to\nMachine Learning\nBao-Gang Hu\nNational Laboratory of Pattern Recognition, Institute of A utomation,\nChinese Academy of Sciences, Beijing 100190, China\n{hubg@nlpr.ia.ac.cn}\nAbstract. In this position paper, I ﬁrst describe a new perspective on\nmachinelearning( ML)byfourbasic problems(orlevels), namely, “What\nto learn?” ,“How to learn?” ,“What to evaluate?” , and“What to ad-\njust?”. The paper stresses more on the ﬁrst level of “What to learn?” ,\nor“Learning Target Selection” . Towards this primary problem within\nthe four levels, I brieﬂy review the existing studies about t he connection\nbetween information theoretical learning ( ITL[1]) and machine learn-\ning. A theorem is given on the relation between the empirical ly-deﬁned\nsimilarity measure and information measures. Finally, a co njecture is\nproposed for pursuing a uniﬁed mathematical interpretatio n to learning\ntarget selection.\nKeywords: Machine learning, learning target selection, entropy, inf or-\nmation theory, similarity, conjecture\n“From the Tao comes one, from one comes two, from two comes thr ee,\nand from three comes all things.” [2]\n- by Lao Tzu (ca. 600-500 BCE)\n“Natureis the realization of the simplest conceivable math ematical ideas.”\n[3]\n- by Albert Einstein (1879-1955)\n1 Introduction\nMachine learning is the study and construction of systems that can learn from\ndata.Thesystemsarecalled learning machines . When BigDataemergesincreas-\ningly, more learning machines are developed and applied in diﬀerent dom ains.\nHowever, the ultimate goal of machine learning study is insight, not machine\nitself. By the term insight I mean learning mechanisms in descriptions of mathe-\nmatical principles. In a loose sense, learning mechanisms can be rega rded as the\nnatural entity. As the “Tao(道)” reﬂects the most fundamental of the universe\nby Lao Tzu (老子), Einstein suggests that we should pursue the simplest mathe-\nmatical interpretations to th",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "4": {
    "pdf_path": "data/pdfs\\machine learning_paper_102.pdf",
    "text_excerpt": "Discussion on Mechanical Learning and\nLearning Machine\u0003\nChuyu Xiong\nIndependent researcher, New York, USA\nEmail: chuyux99@gmail.com\nNovember 10, 2021\nAbstract\nMechanical learning is a computing system that is based on a set of simple and \fxed rules, and\ncan learn from incoming data. A learning machine is a system that realizes mechanical learning.\nImportantly, we emphasis that it is based on a set of simple and \fxed rules, contrasting to often\ncalled machine learning that is sophisticated software based on very complicated mathematical\ntheory, and often needs human intervene for software \fne tune and manual adjustments. Here,\nwe discuss some basic facts and principles of such system, and try to lay down a framework for\nfurther study. We propose 2 directions to approach mechanical learning, just like Church-Turing\npair: one is trying to realize a learning machine, another is trying to well describe the mechanical\nlearning.\nKeywords: Mechanical Learning, Learning Machine, Spatial Learning, Church-Turing\nThesis\nThe only way to rectify our reasonings is to make them as tangible\nas those of the Mathematicians, so that we can \fnd our error at a glance,\nand when there are disputes among persons, we can simply say:\nLet us calculate, without further ado, to see who is right.\n|-Gottfried Leibniz\n1 Introduction\nIn recent years, machine learning becomes hot topic of research and IT development. Yet, there\nare still some very fundamental problems need to be addressed. In the e\u000bort to understand these\nproblems, we brought up the term mechanical learning. Here, we will try to lay down the discussion\nframework for mechanical learning.\nWhile electronic devices can do numerical computation e\u000bectively, and actually can do many com-\nplicated even intelligent things, however, inside the device there is a core that is very mechanical ,\ni.e. the device is governed by a set of simple and \fxed rules . The ability of electronic device doing\ncomplicated information processing comes from that ",
    "title": "On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/2e5d2f2dc01b150dffc163a9f457848e9b5b5c38",
    "published": "2020-07-30"
  },
  "5": {
    "pdf_path": "data/pdfs\\machine learning_paper_103.pdf",
    "text_excerpt": "1 \n A systematic review of fuzzing b ased on machine \nlearning techniques  \nYan Wanga, Peng Jiaa, Luping Liub, Jiayong Liua1 \na College of Cybersecurity Sichuan University ，No.24 South Section 1, Yihuan Road, Chengdu,  China  \nb College of Electronics and Information E ngineering  Sichuan University ，No.24 South Section 1, Yihuan Road, \nChengdu,  China  \nAbstract - Security vulnerabilities play a vital role in network security system. Fuzzing technology is widely used \nas a vulnerability discovery technology to reduce damage in advance. However, traditional fuzzing techniques have \nmany challenges, such as how to mutate input seed files, how to increase code coverage, and how to effectively \nbypass verification. Machine learning technology has been introduced as a new method into f uzzing test to alleviate \nthese challenges. This paper reviews the research progress of using machine learning technology for fuzzing test in \nrecent years, analyzes how machine learning improve the fuzz process and results, and sheds light on future work in  \nfuzzing. Firstly, this paper discusses the reasons why machine learning techniques can be used for fuzzing scenarios \nand identifies six different stages in which machine learning have been used. Then this paper systematically study \nthe machine learning ba sed fuzzing models from selection of machine learning algorithm , pre-processing methods , \ndatasets , evaluation metrics , and hyperparameters setting. Next, this paper assesses the performance of the machine \nlearning models based on the frequently used evalua tion metrics. The results of the evaluation prove that machine \nlearning technology has an acceptable capability  of categorize  predictive for fuzzing. Finally, the comparison on \ncapability of discovering vulnerabilities between traditional fuzzing tools and  machine learning based fuzzing tools \nis analyzed. The results depict that the introduction of machine learning technology can improve the performance of \nfuzzing ",
    "title": "ilastik: interactive machine learning for (bio)image analysis",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/5d433da6d0f143f20936379910104d2bb139d4ae",
    "published": "2019-09-30"
  },
  "6": {
    "pdf_path": "data/pdfs\\machine learning_paper_104.pdf",
    "text_excerpt": "Human-Like Active Learning:\nMachines Simulating the Human Learning Process\nJaeseo Lim\u00031Hwiyeol Jo\u00032;3Byoung-Tak Zhangy1;3;4Jooyong Parky1;5\n1Interdisciplinary Program in Cognitive Science, Seoul National University\n2Institute of Computer Technology, Seoul National University\n3Department of Computer Science and Engineering, Seoul National University\n4SNU AI Institute, Seoul National University\n5Department of Psychology, Seoul National University\n{jaeseolim,jooypark}@snu.ac.kr hwiyeolj@gmail.com btzhang@bi.snu.ac.kr\nAbstract\nAlthough the use of active learning to increase learners’ engagement has recently\nbeen introduced in a variety of methods, empirical experiments are lacking. In this\nstudy, we attempted to align two experiments in order to (1) make a hypothesis for\nmachine and (2) empirically conﬁrm the effect of active learning on learning. In\nExperiment 1,we compared the effect of a passive form of learning to active form\nof learning. The results showed that active learning had a greater learning outcomes\nthan passive learning. In the machine experiment based on the human result, we\nimitated the human active learning as a form of knowledge distillation. The active\nlearning framework performed better than the passive learning framework. In\nthe end, we showed not only that we can make build better machine training\nframework through the human experiment result, but also empirically conﬁrm the\nresult of human experiment through imitated machine experiments; human-like\nactive learning have crucial effect on learning performance.\n1 Introduction\nThe current educational environment often utilizes passive teaching methods that simply delivers\ninformation since it requires students to learn a large amount of knowledge at a limited amount of\ntime. Although passive learning have the advantage of being able to deliver a lot of knowledge, such\ncharacteristic this does not directly lead to learners’ achievement. Rather, there are many studies that\nshow the problems of passive ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "7": {
    "pdf_path": "data/pdfs\\machine learning_paper_105.pdf",
    "text_excerpt": "Matched Machine Learning: A Generalized\nFramework for Treatment E\u000bect Inference\nWith Learned Metrics\nMarco Morucci1, Cynthia Rudin2, and Alexander Volfovsky3\n1Center for Data Science, New York University\n2Department of Computer Science, Duke University\n3Department of Statistical Science, Duke University\nAbstract\nWe introduce Matched Machine Learning, a framework that combines the \rexibility\nof machine learning black boxes with the interpretability of matching, a longstanding\ntool in observational causal inference. Interpretability is paramount in many high-\nstakes application of causal inference. Current tools for nonparametric estimation of\nboth average and individualized treatment e\u000bects are black-boxes that do not allow for\nhuman auditing of estimates. Our framework uses machine learning to learn an optimal\nmetric for matching units and estimating outcomes, thus achieving the performance of\nmachine learning black-boxes, while being interpretable. Our general framework en-\ncompasses several published works as special cases. We provide asymptotic inference\ntheory for our proposed framework, enabling users to construct approximate con\fdence\nintervals around estimates of both individualized and average treatment e\u000bects. We\nshow empirically that instances of Matched Machine Learning perform on par with\nblack-box machine learning methods and better than existing matching methods for\nsimilar problems. Finally, in our application we show how Matched Machine Learning\ncan be used to perform causal inference even when covariate data are highly com-\nplex: we study an image dataset, and produce high quality matches and estimates of\ntreatment e\u000bects.\nKeywords: Matching, Causal Inference, Nonparametric, Dimension Reduction\n1arXiv:2304.01316v1  [stat.ME]  3 Apr 20231 Introduction\nMatching methods have a long history in observational causal inference. Their simplicity\nmakes them interpretable even by non-technical audiences, as well as guarantees fast execu-\ntion of causal analys",
    "title": "SoilGrids250m: Global gridded soil information based on machine learning",
    "abstract": "This paper describes the technical development and accuracy assessment of the most recent and improved version of the SoilGrids system at 250m resolution (June 2016 update). SoilGrids provides global predictions for standard numeric soil properties (organic carbon, bulk density, Cation Exchange Capacity (CEC), pH, soil texture fractions and coarse fragments) at seven standard depths (0, 5, 15, 30, 60, 100 and 200 cm), in addition to predictions of depth to bedrock and distribution of soil classes based on the World Reference Base (WRB) and USDA classification systems (ca. 280 raster layers in total). Predictions were based on ca. 150,000 soil profiles used for training and a stack of 158 remote sensing-based soil covariates (primarily derived from MODIS land products, SRTM DEM derivatives, climatic images and global landform and lithology maps), which were used to fit an ensemble of machine learning methods—random forest and gradient boosting and/or multinomial logistic regression—as implemented in the R packages ranger, xgboost, nnet and caret. The results of 10–fold cross-validation show that the ensemble models explain between 56% (coarse fragments) and 83% (pH) of variation with an overall average of 61%. Improvements in the relative accuracy considering the amount of variation explained, in comparison to the previous version of SoilGrids at 1 km spatial resolution, range from 60 to 230%. Improvements can be attributed to: (1) the use of machine learning instead of linear regression, (2) to considerable investments in preparing finer resolution covariate layers and (3) to insertion of additional soil profiles. Further development of SoilGrids could include refinement of methods to incorporate input uncertainties and derivation of posterior probability distributions (per pixel), and further automation of spatial modeling so that soil maps can be generated for potentially hundreds of soil variables. Another area of future research is the development of methods for multiscale merging of SoilGrids predictions with local and/or national gridded soil products (e.g. up to 50 m spatial resolution) so that increasingly more accurate, complete and consistent global soil information can be produced. SoilGrids are available under the Open Data Base License.",
    "link": "https://www.semanticscholar.org/paper/9e27190f2d9b2167d4a66b88696def4585072fd5",
    "published": "2017-02-16"
  },
  "8": {
    "pdf_path": "data/pdfs\\machine learning_paper_106.pdf",
    "text_excerpt": "Quantum-enhanced machine learning\nVedran Dunjko,1,\u0003Jacob M. Taylor,2, 3,yand Hans J. Briegel1,z\n1Institut f ur Theoretische Physik, Universit at Innsbruck, Technikerstra\u0019e 21a, A-6020 Innsbruck, Austria\n2Joint Quantum Institute, National Institute of Standards and Technology, Gaithersburg, MD 20899 USA\n3Joint Center for Quantum Information and Computer Science,\nUniversity of Maryland, College Park, MD 20742 USA\n(Dated: October 27, 2016)\nThe emerging \feld of quantum machine learning has the potential to substantially aid in the\nproblems and scope of arti\fcial intelligence. This is only enhanced by recent successes in the \feld\nof classical machine learning. In this work we propose an approach for the systematic treatment\nof machine learning, from the perspective of quantum information. Our approach is general and\ncovers all three main branches of machine learning: supervised, unsupervised and reinforcement\nlearning. While quantum improvements in supervised and unsupervised learning have been reported,\nreinforcement learning has received much less attention. Within our approach, we tackle the problem\nof quantum enhancements in reinforcement learning as well, and propose a systematic scheme for\nproviding improvements. As an example, we show that quadratic improvements in learning e\u000eciency,\nand exponential improvements in performance over limited time periods, can be obtained for a broad\nclass of learning problems.\nPACS numbers: 03.67.-a, 03.67.Ac, 03.65.Aa, 03.67.Hk, 03.67.Lx\nIntroduction.{ The \feld of arti\fcial intelligence (AI)\nhas lately had remarkable successes, especially in the area\nof machine learning [1, 2]. A recent milestone, until re-\ncently believed to be decades away { a computer beating\nan expert human player in the game of Go [3] { clearly il-\nlustrates the potential of learning machines. In parallel,\nwe are witnessing the emergence of a new \feld: quan-\ntum machine learning (QML), which has a further, pro-\nfound potential to revolutionize the \feld of AI",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "9": {
    "pdf_path": "data/pdfs\\machine learning_paper_107.pdf",
    "text_excerpt": "arXiv:1302.0406v1  [cs.LG]  2 Feb 2013Generalization Guarantees for a Binary Classiﬁcation Fram ework\nfor Two-Stage Multiple Kernel Learning\nPurushottam Kar\nDepartment of Computer Science and Engineering\nIIT Kanpur\npurushot@cse.iitk.ac.in\nSeptember 8, 2018\nAbstract\nWe present generalization bounds for the TS-MKL framework f or two stage multiple kernel\nlearning. We also present bounds for sparse kernel learning formulations within the TS-MKL\nframework.\n1 Introduction\nRecently Kumar et al[6] proposed a framework for two-stage multiple kernel lear ning that combines\nthe idea of target kernel alignment and the notion of a goodkernel proposed in [1] to learn a\ngood Mercer kernel. More speciﬁcally, given a ﬁnite set of ba se kernels K1,...,K pover some\ncommon domain X, we wish to ﬁnd some combination of these base kernels that is well suited\nto the learning task at hand. The paper considers learning a p ositive linear combination of the\nkernelsKµ=/summationtextp\ni=1µiKifor some µ∈Rp,µ≥0. It is assumed that the kernels are uniformly\nbounded i.e. for all x1,x2∈ Xandi= 1...p, we have Ki(x1,x2)≤κ2\nifor some κi>0. Let\nκ=/parenleftbig\nκ2\n1,...,κ2\np/parenrightbig\n∈Rp. Note that κ≥0. Also note that for any µand any x1,x2∈ X, we have\nKµ(x1,x2)≤ /an}bracketle{tµ,κ/an}bracketri}ht.\nThe notion of suitability used in [6] is that of kernel-goodness ﬁrst proposed in [1] for classiﬁcation\ntasks. For sake of simplicity, we shall henceforth consider only binary classiﬁcation tasks, the\nextension to multi-class classiﬁcation tasks being straig htforward. We present below the notion\nof goodness used in [6]. For any binary classiﬁcation task ov er a domain Xcharacterized by a\ndistribution DoverX ×{±1}, a Mercer kernel K:X ×X → Rwith associated Reproducing Kernel\nHilbert Space HKand feature map ΦK:X → H Kis said to be (ǫ,γ)-kernel good if there exists a\nunit norm vector w∈ HKsuch that /bardblw/bardblHK= 1and the following holds\nE\n(x,y)∼D/LARGEllbracket/bracketleftbigg\n1−y/an}bracketle{tw,Φ(x)",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "10": {
    "pdf_path": "data/pdfs\\machine learning_paper_108.pdf",
    "text_excerpt": "Machine learning manuscript No.\n(will be inserted by the editor)\nBeneﬁcial and Harmful Explanatory Machine\nLearning\nLun Ai ·Stephen H. Muggleton ·Céline\nHocquette ·Mark Gromowski ·Ute Schmid\nReceived: date / Accepted: date\nAbstract Given the recent successes of Deep Learning in AI there has been increased\ninterest in the role and need for explanations in machine learned theories. A distinct\nnotion in this context is that of Michie’s deﬁnition of Ultra-Strong Machine Learning\n(USML). USML is demonstrated by a measurable increase in human performance of a\ntask following provision to the human of a symbolic machine learned theory for task\nperformance. A recent paper demonstrates the beneﬁcial eﬀect of a machine learned\nlogic theory for a classiﬁcation task, yet no existing work to our knowledge has exam-\nined the potential harmfulness of machine’s involvement for human comprehension\nduring learning. This paper investigates the explanatory eﬀects of a machine learned\ntheory in the context of simple two person games and proposes a framework for\nidentifying the harmfulness of machine explanations based on the Cognitive Science\nliterature. The approach involves a cognitive window consisting of two quantiﬁable\nbounds and it is supported by empirical evidence collected from human trials. Our\nquantitative and qualitative results indicate that human learning aided by a symbolic\nmachine learned theory which satisﬁes a cognitive window has achieved signiﬁcantly\nhigher performance than human self learning. Results also demonstrate that human\nlearning aided by a symbolic machine learned theory that fails to satisfy this window\nleads to signiﬁcantly worse performance than unaided human learning.\nLun Ai\nDepartment of Computing, Imperial College London, London, UK\nE-mail: lun.ai15@imperial.ac.uk\nStephen H. Muggleton\nDepartment of Computing, Imperial College London, London, UK\nE-mail: s.muggleton@imperial.ac.uk\nCéline Hocquette\nDepartment of Computing, Imperial College London, London,",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "11": {
    "pdf_path": "data/pdfs\\machine learning_paper_109.pdf",
    "text_excerpt": "arXiv:1708.07826v1  [stat.ML]  24 Aug 2017Logistic Regression as Soft Perceptron\nLearning\nRa´ ul Rojas\nNovember 25, 2021\nAbstract\nWe show that gradient ascent for logistic regression has a co nnec-\ntion with the perceptron learning algorithm. Logistic lear ning is the\n“soft” variant of perceptron learning.\nLogistic Regression is used to build classiﬁers with a function which can be\ngiven a probabilistic interpretation. We are given the data set ( xi,yi), for\ni= 1,...,N, where the xiare (n+1)-dimensional extended vectors and the\nyiare zero or one (representing the positive or negative class, resp ectively).\nWe would like to build a function p(x,β), which depends on a single ( n+1)-\ndimensional parameter vector β(like in linear regression) but where p(x,β)\napproaches one when xbelongs to the positive class, and zero if not. An\nextended vector xis one in which the collection of features has been aug-\nmented by attaching the additional component 1 to nfeatures, so that the\nscalar product of the βandxvectors can be written as\nβTx=β0+β1x1+···+βnxn\nThe extra component allows us to handle a constant term β0in the scalar\nproduct in an elegant way.\n1A proposal for a function such as the one described above is\np(x,β) =exp(βTx)/(1+exp(βTx))\nwherep(x,β) denotes the probability that xbelongs to the positive class.\nThe function is always positive and never greater than one. It satu rates\nasymptotically to 1 in the direction of β. Note that the probability of x\nbelonging to the negative class is given by:\n1−p(x,β) = 1/(1+exp(βTx))\nWith this interpretation we can adjust βso that the data has maximum\nlikelihood. If N1is the number of data points in the positive class and N2\nthe number o data points in the negative class, the likelihood is given by the\nproduct of all points probabilities\nL(β) =N1/productdisplay\np(xi,β)N2/productdisplay\n(1−p(xi,β))\nWewant to maximize thelikelihood ofthe data, but we usually maximize th e\nlog-likelihood,sincethelogarithmisamonotonicfunction. Thelog-like",
    "title": "Multimodal Machine Learning: A Survey and Taxonomy",
    "abstract": "Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.",
    "link": "https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91",
    "published": "2017-05-26"
  },
  "12": {
    "pdf_path": "data/pdfs\\machine learning_paper_11.pdf",
    "text_excerpt": "© 20 18 IEEE. Personal use of this material is permitted. Permission from IEEE must be  obtained for all other uses, in any current or future media, including  \nreprinting/republishing this material for advertising or promotional purposes, creating new  collective works, for resale or redistribution to servers or lists, or reuse \nof any copyrighted  component of this work in other w orks.  A Unified Analytical Framework for Trustable Machine \nLearning and Automation Running with Blockchain  \nTao Wang  \nSAS Institute Inc.  \nCary, USA  \nt.wang@sas.com  \nAbstract—Traditional machine learning algorithm s use data \nfrom database s that are mutable , and ther efore the data cannot \nbe fully trust ed. Also, the machine learning process  is difficult to \nautomate . This paper proposes build ing a trustable machine \nlearning system by using  blockchain technology , which can store \ndata in a permanent and immutable way. In addition , smart \ncontract s are used to automate the machine learning process. \nThis paper makes  three contributions. First, it establishes a link \nbetween machine learning technology and blockchain technology. \nPreviously, machine learning and blockchain have been \nconsidered two independent technologies without an obvious link. \nSecond, it proposes a unified analytical framework for trustable \nmachine learning by using  blockchain technology. Th is unified \nframework solves both the trust ability  and automation issue s in \nmachine learning. Third, it enables  a computer to translate core \nmachine learning impl ementation from a single  thread  on a  \nsingle  machine to multi ple thread s on multi ple machine s running  \nwith blockchain by using a unified approach.  The paper uses \nassociation rule mining  as an example  to demonstrate how \ntrustable machine learning can be implemented  with blockchain, \nand it shows how this approach can be used to analyze  opioid \nprescription s to help combat the opioid crisis.   \nKeywords —machine learning, tru st,",
    "title": "A Unified Analytical Framework for Trustable Machine Learning and\n  Automation Running with Blockchain",
    "abstract": "Traditional machine learning algorithms use data from databases that are\nmutable, and therefore the data cannot be fully trusted. Also, the machine\nlearning process is difficult to automate. This paper proposes building a\ntrustable machine learning system by using blockchain technology, which can\nstore data in a permanent and immutable way. In addition, smart contracts are\nused to automate the machine learning process. This paper makes three\ncontributions. First, it establishes a link between machine learning technology\nand blockchain technology. Previously, machine learning and blockchain have\nbeen considered two independent technologies without an obvious link. Second,\nit proposes a unified analytical framework for trustable machine learning by\nusing blockchain technology. This unified framework solves both the\ntrustability and automation issues in machine learning. Third, it enables a\ncomputer to translate core machine learning implementation from a single thread\non a single machine to multiple threads on multiple machines running with\nblockchain by using a unified approach. The paper uses association rule mining\nas an example to demonstrate how trustable machine learning can be implemented\nwith blockchain, and it shows how this approach can be used to analyze opioid\nprescriptions to help combat the opioid crisis.",
    "link": "http://arxiv.org/abs/1903.08801v1",
    "published": "2019-03-21T02:17:08Z"
  },
  "13": {
    "pdf_path": "data/pdfs\\machine learning_paper_110.pdf",
    "text_excerpt": "Learning proofs for the classi\fcation of nilpotent\nsemigroups\nCarlos Simpson\nAbstract\nMachine learning is applied to \fnd proofs, with smaller or smallest numbers of\nnodes, for the classi\fcation of 4-nilpotent semigroups.\n1 Introduction\nWe are interested in the classi\fcation of \fnite semigroups. Distler [4, 5, 6] has provided a list\nof isomorphism classes for sizes n\u001410, but at great computational expense. The question\nwe pose here is whether arti\fcial intelligence, in the form of deep learning, can learn to do\na classi\fcation proof for these objects.\nTo be more precise, we are going to look at the question of whether a process designed to\nlearn how to do proofs using neural networks can learn to do \\better\" proofs, as measured\nby the number of nodes in the proof tree.\nLet's point out right away that the process will not, in its current state, be useful for\nimproving in practical terms the computational time for a classi\fcation proof. Even though\nwe are able to \fnd proofs with small numbers of nodes, potentially close to the minimum,\nthe training time necessary to do that is signi\fcantly bigger than the gain with respect to a\nreasonable benchmark process. Therefore, this study should be considered more for what it\nsays about the general capacity of a deep learning process to learn how to do proofs.\nUnsurprisingly, the motivation for this question is the recent phenomenal success obtained\nby Alpha Go and Alpha Zero [17, 18] at guiding complex strategy games. If we think of a\nmathematical proof as a strategy problem, then it seems logical to suppose that the same\nkind of technology could guide the strategy of a proof.\n1arXiv:2106.03015v1  [cs.LG]  6 Jun 2021In turn, this investigation serves as a convenient and fun way to experiment with some\nbasic Deep Learning programming. In recent years there have been an increasing number\nof studies of the application of machine learning to mathematics, starting from [11, 3] and\ncontinuing, to cite just a very few, with [1, 12, 20",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "14": {
    "pdf_path": "data/pdfs\\machine learning_paper_111.pdf",
    "text_excerpt": "PyKale: Knowledge-Aware Machine Learning from Multiple\nSources in Python\nHaiping Lu\nThe University of Sheffield\nSheffield, United Kingdom\nh.lu@sheffield.ac.ukXianyuan Liu\nThe University of Sheffield\nSheffield, United Kingdom\nxianyuan.liu@sheffield.ac.ukRobert Turner\nThe University of Sheffield\nSheffield, United Kingdom\nr.d.turner@sheffield.ac.uk\nPeizhen Bai\nThe University of Sheffield\nSheffield, United Kingdom\nPBai2@sheffield.ac.ukRaivo E Koot\nThe University of Sheffield\nSheffield, United Kingdom\nrekoot1@sheffield.ac.ukShuo Zhou\nThe University of Sheffield\nSheffield, United Kingdom\nSZhou20@sheffield.ac.uk\nMustafa Chasmai\nIndian Institute of Technology, Delhi\nNew Delhi, India\ncs1190341@iitd.ac.inLawrence Schobs\nThe University of Sheffield\nSheffield, United Kingdom\nlaschobs1@sheffield.ac.uk\nABSTRACT\nMachine learning is a general-purpose technology holding promises\nfor many interdisciplinary research problems. However, significant\nbarriers exist in crossing disciplinary boundaries when most ma-\nchine learning tools are developed in different areas separately.\nWe present Pykale – a Python library for knowledge-aware ma-\nchine learning on graphs, images, texts, and videos to enable and\naccelerate interdisciplinary research. We formulate new green ma-\nchine learning guidelines based on standard software engineering\npractices and propose a novel pipeline -based application program-\nming interface (API). PyKale focuses on leveraging knowledge from\nmultiple sources for accurate and interpretable prediction, thus\nsupporting multimodal learning and transfer learning (particularly\ndomain adaptation) with latest deep learning and dimensionality\nreduction models. We build PyKale on PyTorch and leverage the\nrich PyTorch ecosystem. Our pipeline-based API design enforces\nstandardization and minimalism, embracing green machine learning\nconcepts via reducing repetitions and redundancy, reusing existing\nresources, and recycling learning models across areas. We demon-\nstrate its interdi",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "15": {
    "pdf_path": "data/pdfs\\machine learning_paper_112.pdf",
    "text_excerpt": "Is `Unsupervised Learning' a Misconceived\nTerm?\nStephen G. Odaibo,\nM.D.,M.S.(Math),M.S.(Comp. Sci.)\n.\nRETINA-AI Health, Inc.\nApril 9, 2019\nAbstract\nIs all of machine learning supervised to some degree? The \feld\nof machine learning has traditionally been categorized pedagogically\ninto supervised vsunsupervised learning ; where supervised learning\nhas typically referred to learning from labeled data, while unsuper-\nvised learning has typically referred to learning from unlabeled data.\nIn this paper, we assert that all machine learning is in fact supervised\nto some degree, and that the scope of supervision is necessarily com-\nmensurate to the scope of learning potential. In particular, we argue\nthat clustering algorithms such as k-means, and dimensionality re-\nduction algorithms such as principal component analysis, variational\nautoencoders, and deep belief networks are each internally supervised\nby the data itself to learn their respective representations of its fea-\ntures. Furthermore, these algorithms are not capable of external in-\nference until their respective outputs (clusters, principal components,\nor representation codes) have been identi\fed and externally labeled\nin e\u000bect. As such, they do not su\u000ece as examples of unsupervised\nlearning. We propose that the categorization `supervised vs unsuper-\nvised learning' be dispensed with, and instead, learning algorithms be\ncategorized as either internally or externally supervised (or both). We\nbelieve this change in perspective will yield new fundamental insights\ninto the structure and character of data and of learning algorithms.\nCorrespondence Email:\nstephen.odaibo@retina-ai.comarXiv:1904.03259v1  [cs.LG]  5 Apr 20191 Introduction\nFigure 1: Prototypical Illustration of Supervised vs Unsupervised Learning\nTraditional thinking has been to pedagogically divide machine learn-\ning into supervised vs unsupervised learning, as depicted in Figure(1). In\nthis paper, we challenge that scheme by arguing that all machine learn-",
    "title": "Large-Scale Machine Learning with Stochastic Gradient Descent",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/fbc6562814e08e416e28a268ce7beeaa3d0708c8",
    "published": null
  },
  "16": {
    "pdf_path": "data/pdfs\\machine learning_paper_113.pdf",
    "text_excerpt": "JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nDeep Learning and Its Applications to Machine\nHealth Monitoring: A Survey\nRui Zhao, Ruqiang Yan, Zhenghua Chen, Kezhi Mao, Peng Wang, and Robert X. Gao\nAbstract —Since 2006, deep learning (DL) has become a rapidly\ngrowing research direction, redeﬁning state-of-the-art perfor-\nmances in a wide range of areas such as object recognition,\nimage segmentation, speech recognition and machine translation.\nIn modern manufacturing systems, data-driven machine health\nmonitoring is gaining in popularity due to the widespread\ndeployment of low-cost sensors and their connection to the\nInternet. Meanwhile, deep learning provides useful tools for\nprocessing and analyzing these big machinery data. The main\npurpose of this paper is to review and summarize the emerging\nresearch work of deep learning on machine health monitoring.\nAfter the brief introduction of deep learning techniques, the\napplications of deep learning in machine health monitoring\nsystems are reviewed mainly from the following aspects: Auto-\nencoder (AE) and its variants, Restricted Boltzmann Machines\nand its variants including Deep Belief Network (DBN) and Deep\nBoltzmann Machines (DBM), Convolutional Neural Networks\n(CNN) and Recurrent Neural Networks (RNN). Finally, some\nnew trends of DL-based machine health monitoring methods are\ndiscussed.\nIndex Terms —Deep learning, machine health monitoring, big\ndata\nI. I NTRODUCTION\nINDUSTRIAL Internet of Things (IoT) and data-driven\ntechniques have been revolutionizing manufacturing by\nenabling computer networks to gather the huge amount of\ndata from connected machines and turn the big machinery data\ninto actionable information [1], [2], [3]. As a key component\nin modern manufacturing system, machine health monitoring\nhas fully embraced the big data revolution. Compared to\ntop-down modeling provided by the traditional physics-based\nmodels [4], [5], [6] , data-driven machine health monitoring\nsystems offer a new parad",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "17": {
    "pdf_path": "data/pdfs\\machine learning_paper_114.pdf",
    "text_excerpt": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identiﬁer 10.1109/ACCESS.2017.DOI\nDiversity in Machine Learning\nZHIQIANG GONG1, PING ZHONG1, (SENIOR MEMBER, IEEE), AND WEIDONG HU1\n1National Key Laboratory of Science and Technology on ATR, College of Electronic Science and Technology, National University of Defense Technology,\nChangsha 410073, China (e-mail: gongzhiqiang13@nudt.edu.cn, zhongping@nudt.edu.cn, wdhuatr@icloud.com)\nCorresponding author: Ping Zhong (e-mail: zhongping@nudt.edu.cn).\nThis work was supported in part by the Natural Science Foundation of China under Grant 61671456 and 61271439, in part by the\nFoundation for the Author of National Excellent Doctoral Dissertation of China (FANEDD) under Grant 201243, and in part by the\nProgram for New Century Excellent Talents in University under Grant NECT-13-0164.\nABSTRACT Machine learning methods have achieved good performance and been widely applied\nin various real-world applications. They can learn the model adaptively and be better ﬁt for special\nrequirements of different tasks. Generally, a good machine learning system is composed of plentiful training\ndata, a good model training process, and an accurate inference. Many factors can affect the performance of\nthe machine learning process, among which the diversity of the machine learning process is an important\none. The diversity can help each procedure to guarantee a total good machine learning: diversity of the\ntraining data ensures that the training data can provide more discriminative information for the model,\ndiversity of the learned model (diversity in parameters of each model or diversity among different base\nmodels) makes each parameter/model capture unique or complement information and the diversity in\ninference can provide multiple choices each of which corresponds to a speciﬁc plausible local optimal result.\nEven though the diversity plays an important role in machine learning process, there is no systemat",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "18": {
    "pdf_path": "data/pdfs\\machine learning_paper_115.pdf",
    "text_excerpt": "arXiv:1904.00001v2  [cs.SE]  22 Aug 2019Engineering problems in machine learning systems\nHiroshi Kuwajima\nDENSO CORPORATION & Tokyo Institute of Technology\nhiroshi.kuwajima.j7d@jp.denso.com, kuwajima@ok.sc.e. titech.ac.jp\nHirotoshi Yasuoka\nDENSO CORPORATION\nhirotoshi.yasuoka.j2z@jp.denso.comToshihiro Nakae\nDENSO CORPORATION\ntoshihiro.nakae.j8z@jp.denso.com\nAbstract\nFatal accidents are a major issue hindering the wide accepta nce of safety-critical\nsystems that employ machine learning and deep learning mode ls, such as auto-\nmated driving vehicles. In order to use machine learning in a safety-critical sys-\ntem, it is necessary to demonstrate the safety and security o f the system through\nengineering processes. However, thus far, no such widely ac cepted engineering\nconcepts or frameworks have been established for these syst ems. The key to using\na machine learning model in a deductively engineered system is decomposing the\ndata-driven training of machine learning models into requi rement, design, and ver-\niﬁcation, particularly for machine learning models used in safety-critical systems.\nSimultaneously, open problems and relevant technical ﬁeld s are not organized in a\nmanner that enables researchers to select a theme and work on it. In this study, we\nidentify, classify, and explore the open problems in engine ering (safety-critical)\nmachine learning systems — that is, in terms of requirement, design, and veriﬁca-\ntion of machine learning models and systems — as well as discu ss related works\nand research directions, using automated driving vehicles as an example. Our\nresults show that machine learning models are characterize d by a lack of require-\nments speciﬁcation, lack of design speciﬁcation, lack of in terpretability, and lack\nof robustness. We also perform a gap analysis on a convention al system quality\nstandard SQuARE with the characteristics of machine learni ng models to study\nquality models for machine learning systems. We ﬁnd that a la ck of requirements",
    "title": "Applications of machine learning to machine fault diagnosis: A review and roadmap",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/7ae2783a9196fb4bc2a610ae812d19722daddce5",
    "published": "2020-04-01"
  },
  "19": {
    "pdf_path": "data/pdfs\\machine learning_paper_116.pdf",
    "text_excerpt": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nAutomated Graph Machine Learning:\nApproaches, Libraries,\nBenchmarks and Directions\nXin Wang, Member, IEEE, Ziwei Zhang, Member, IEEE, Haoyang Li, Member, IEEE,\nand Wenwu Zhu, Fellow, IEEE\nAbstract —Graph machine learning has been extensively studied in both academic and industry. However, as the literature on graph\nlearning booms with a vast number of emerging methods and techniques, it becomes increasingly difficult to manually design the\noptimal machine learning algorithm for different graph-related tasks. To tackle the challenge, automated graph machine learning, which\naims at discovering the best hyper-parameter and neural architecture configuration for different graph tasks/data without manual\ndesign, is gaining an increasing number of attentions from the research community. In this paper, we extensively discuss automated\ngraph machine learning approaches, covering hyper-parameter optimization (HPO) and neural architecture search (NAS) for graph\nmachine learning. We briefly overview existing libraries designed for either graph machine learning or automated machine learning\nrespectively, and further in depth introduce AutoGL, our dedicated and the world’s first open-source library for automated graph\nmachine learning. Also, we describe a tailored benchmark that supports unified, reproducible, and efficient evaluations. Last but not\nleast, we share our insights on future research directions for automated graph machine learning. This paper is the first systematic and\ncomprehensive discussion of approaches, libraries as well as directions for automated graph machine learning.\nIndex Terms —Graph Machine Learning, Graph Neural Network, Automated Machine Learning, AutoML, Neural Architecture Search,\nHyper-parameter Optimization\n✦\n1 I NTRODUCTION\nGRAPH data is ubiquitous in our daily life. We can use\ngraphs to model the complex relationships and dependencies\nbetween entities ranging from small molecules in pro",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "20": {
    "pdf_path": "data/pdfs\\machine learning_paper_117.pdf",
    "text_excerpt": "On the impact of measure pre-conditionings\non general parametric ML models and\ntransfer learning via domain adaptation\nSanchez Garcia Joaquin1,1*\nCorresponding author(s). E-mail(s): joaqsan@math.utoronto.com;\nAbstract\nWe study a new technique for understanding convergence of learn-\ning agents under small modifications of data. We show that such\nconvergence can be understood via an analogue of Fatou’s lemma\nwhich yields Γ-convergence. We show it’s relevance and applications in\ngeneral machine learning tasks and domain adaptation transfer learning.\nKeywords: measure pre-conditioning, recovery systems, stability\nContents\n1 Introduction 3\n1.1 Organization of this document . . . . . . . . . . . . . . . . . . 3\n1.2 Relation to literature . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.3 Necessity of non-parametric measure pre-conditioning techniques 4\n2 Measure pre-conditionings 4\n3 A mathematical framework admitting pre-conditioning 5\n3.1 Formulation of the problem . . . . . . . . . . . . . . . . . . . . 5\n3.2 Convergence of the learning problem . . . . . . . . . . . . . . . 6\n3.3 The main question . . . . . . . . . . . . . . . . . . . . . . . . . 6\n3.3.1 Main Theorem . . . . . . . . . . . . . . . . . . . . . . . 7\n3.4 A version of the envelope Theorem . . . . . . . . . . . . . . . . 7\n3.4.1 No Empirical Probability Measure can Converge in the\nTotal Variation Sense for all Distributions . . . . . . . . 12\n1arXiv:2403.02432v1  [stat.ML]  4 Mar 20242 CONTENTS\n3.4.2 Example: Linear regression . . . . . . . . . . . . . . . . 12\n3.5 Measure pre-conditioning approaches . . . . . . . . . . . . . . . 14\n3.6 Background and Notation . . . . . . . . . . . . . . . . . . . . . 14\n4 Empirical measures and non-parametric estimation 14\n4.1 Non-exhausting list of non-parametric estimation techniques . . 15\n4.1.1 Wasserstein 2-Barycenter . . . . . . . . . . . . . . . . . 15\n4.1.2 Uniform convex hull . . . . . . . . . . . . . . . . . . . . 16\n4.1.3 Entropically regularized barycenter .",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "21": {
    "pdf_path": "data/pdfs\\machine learning_paper_118.pdf",
    "text_excerpt": "arXiv:2202.13608v2  [stat.ML]  11 Mar 2022Semi-supervised Learning on Large Graphs:\nis Poisson Learning a Game-Changer?\nCanh Hao Nguyen\nBioinformatics Center, ICR, Kyoto University\nUji, Kyoto, 611-0011, Japan\nMarch 14, 2022\nAbstract\nWe explain Poisson learning [2] on graph-based semi-superv ised learning to see if it could\navoid the problem of global information loss problem as Lapl ace-based learning methods on large\ngraphs. From our analysis, Poisson learning is simply Lapla ce regularization with thresholding,\ncannot overcome the problem.\n1 Introduction\nGiven a graph G= (V,W ) withV={x0,···xn−1}as a set of training data and Wis an×nreal\nnonnegative weight matrix (weights of the edges of the graphs). S upposed that we are given m < n\nlabelsy0···ym−1(real or binary) of mnodes (x0···xm−1) , one wishes to infer the labels of the\nremaining nodes ( xm···xn−1):u(xi) :V→ R (binary class labels are usually of the form sign(u)).\nGraph-based models have been the main tools for semi-supervised le arning.\n1.1 Laplace Learning\nTraditional Laplace learning [6] would use the Laplacian regularization in this variational form:\nu= arg min uTLu|u(xi) =yi,0≤i < m (1)\nwithL=D−Wbeing the graph Laplacian with Dbeing the degree diagonal matrix ( D=diag(W×\n1n)). The solution of (1) satisﬁes\nu(xi) =yi|0≤i < m, (2)\nLu(xi) = 0|m≤i < n. (3)\n1.2 Laplace Regularization\nLaplace learning can be modiﬁed to introduce a loss-based data term [3] as follows:\nu= arg min( y−u)2+λuTLu, (4)\nwith the loss ( y−u)2only on labeled data points is added to the Laplace regularization term .\n11.3 Global Information Loss Problem\nThe problem with Laplace learning, or Laplace regularization is that in la rge graphs, the regularization\ntermuTLudoes transfer the label from labeled nodes to far away nodes in the graphs. Concretely,\nthe following problems are proven:\n1.udoes not carry label information for the whole dataset, becoming a non-informative function\n[3], unrelated to the data distribution [1]. This leads to t",
    "title": "Machine learning in automated text categorization",
    "abstract": "The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.",
    "link": "https://www.semanticscholar.org/paper/6b20af22b0734757d9ead382b201a65f9dd637cc",
    "published": "2001-10-26"
  },
  "22": {
    "pdf_path": "data/pdfs\\machine learning_paper_119.pdf",
    "text_excerpt": "arXiv:1711.01431v1  [cs.AI]  4 Nov 2017The Case for Meta-Cognitive Machine Learning:\nOn Model Entropy and Concept Formation in Deep Learning\nJohan Loeckx\nArtiﬁcial Intelligence Lab, Vrije Universiteit Brussel, P leinlaan 2, 1050 Brussel\njloeckx@ai.vub.ac.be\nAbstract\nMachine learning is usually deﬁned in behaviourist\nterms, where external validation is the primary\nmechanism of learning. In this paper, I argue for a\nmore holistic interpretation in which ﬁnding more\nprobable, efﬁcient and abstract representations is as\ncentral to learning as performance. In other words,\nmachine learning should be extended with strate-\ngies to reason over its own learning process, lead-\ning to so-called meta-cognitive machine learning.\nAs such, the de facto deﬁnition of machine learning\nshould be reformulated in these intrinsically multi-\nobjective terms, taking into account not only the\ntask performance but also internal learning objec-\ntives. To this end, we suggest a “model entropy\nfunction” to be deﬁned that quantiﬁes the efﬁciency\nof the internal learning processes. It is conjured\nthat the minimization of this model entropy leads to\nconcept formation. Besides philosophical aspects,\nsome initial illustrations are included to support the\nclaims.\n1 Introduction\nMachine learning is often approached from a behaviourist\nperspective, in which external feedback in the form of a re-\ninforcement signal is the major driving force of improve-\nment. Though this method has lead to many successes, it\nis confronted with interesting and unsolved challenges lik e\ntackling overﬁtting, providing comprehensibility, build ing\nreusable abstractions and concept formation, among many\nother [Kotsiantis et al. , 2007; Bengio, 2009]. The problem\nwith these behaviourist approaches is that they ignore the c en-\ntral importance of internal processes when considering lea rn-\ning. Model internals are often regarded just as a means to\nachieve higher performance. Analogous to studying human\nbehaviour, however, appre",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "23": {
    "pdf_path": "data/pdfs\\machine learning_paper_12.pdf",
    "text_excerpt": "MLBench: How Good Are Machine Learning Clouds for\nBinary Classification Tasks on Structured Data?\n[Experiments and Analysis]\nYu Liu\nDepartment of Computer Science\nETH Zurich\nyu.liu@inf.ethz.chHantian Zhang\nDepartment of Computer Science\nETH Zurich\nhantian.zhang@inf.ethz.chLuyuan Zeng\nDepartment of Computer Science\nETH Zurich\nzengl@student.ethz.ch\nWentao Wu\nMicrosoft Research, Redmond\nwentao.wu@microsoft.comCe Zhang\nDepartment of Computer Science\nETH Zurich\nce.zhang@inf.ethz.ch\nABSTRACT\nWe conduct an empirical study of machine learning functionali-\nties provided by major cloud service providers, which we call ma-\nchine learning clouds . Machine learning clouds hold the promise\nof hiding all the sophistication of running large-scale machine\nlearning: Instead of specifying how to run a machine learning\ntask, users only specify what machine learning task to run and\nthe cloud figures out the rest. Raising the level of abstraction,\nhowever, rarely comes free — a performance penalty is possible.\nHow good, then, are current machine learning clouds on real-world\nmachine learning workloads?\nWe study this question with a focus on binary classification\nproblems. We present mlbench , a novel benchmark constructed\nby harvesting datasets from Kaggle competitions. We then com-\npare the performance of the top winning code available from\nKaggle with that of running machine learning clouds from both\nAzure and Amazon on mlbench . Our comparative study reveals\nthe strength and weakness of existing machine learning clouds\nand points out potential future directions for improvement.\n1 INTRODUCTION\nIn spite of the recent advancement of machine learning research,\nmodern machine learning systems are still far from easy to use,\nat least from the perspective of business users or even scientists\nwithout a computer science background [ 30]. Recently, there\nis a trend toward pushing machine learning onto the cloud as\na “service,” a.k.a. machine learning clouds . By putting a set of\nmachine learnin",
    "title": "MLBench: How Good Are Machine Learning Clouds for Binary Classification\n  Tasks on Structured Data?",
    "abstract": "We conduct an empirical study of machine learning functionalities provided by\nmajor cloud service providers, which we call machine learning clouds. Machine\nlearning clouds hold the promise of hiding all the sophistication of running\nlarge-scale machine learning: Instead of specifying how to run a machine\nlearning task, users only specify what machine learning task to run and the\ncloud figures out the rest. Raising the level of abstraction, however, rarely\ncomes free - a performance penalty is possible. How good, then, are current\nmachine learning clouds on real-world machine learning workloads?\n  We study this question with a focus on binary classication problems. We\npresent mlbench, a novel benchmark constructed by harvesting datasets from\nKaggle competitions. We then compare the performance of the top winning code\navailable from Kaggle with that of running machine learning clouds from both\nAzure and Amazon on mlbench. Our comparative study reveals the strength and\nweakness of existing machine learning clouds and points out potential future\ndirections for improvement.",
    "link": "http://arxiv.org/abs/1707.09562v3",
    "published": "2017-07-29T21:59:18Z"
  },
  "24": {
    "pdf_path": "data/pdfs\\machine learning_paper_120.pdf",
    "text_excerpt": "arXiv:1907.07543v1  [cs.LG]  17 Jul 2019Low-Shot Classiﬁcation: A Comparison of Classical and Deep Transfer\nMachine Learning Approaches\nPeter Usherwood\npeter.usherwood@kantar.comSteven Smit\nsteven.smit@kantar.com\nJuly 18, 2019\nAbstract\nDespite the recent success of deep transfer learning approache s in NLP, there is a lack of quan-\ntitative studies demonstrating the gains these models oﬀer in low-sh ot text classiﬁcation tasks over\nexisting paradigms. Deep transfer learning approaches such as BE RT and ULMFiT demonstrate\nthat they can beat state-of-the-art results on larger dataset s, however when one has only 100-1000\nlabelled examples per class, the choice of approach is less clear, with c lassical machine learning and\ndeep transfer learning representing valid options. This paper comp ares the current best transfer\nlearning approach with top classical machine learning approaches on a trinary sentiment classiﬁca-\ntion task to assess the best paradigm. We ﬁnd that BERT, represe nting the best of deep transfer\nlearning, is the best performing approach, outperforming top clas sical machine learning algorithms\nby 9.7% on average when trained with 100 examples per class, narrowing t o 1.8% at 1000 labels per\nclass. We also show the robustness of deep transfer learning in mov ing across domains, where the\nmaximum loss in accuracy is only 0 .7% in similar domain tasks and 3 .2% cross domain, compared\nto classical machine learning which loses up to 20 .6%.\n11 Introduction\nTransfer learning in the Natural Language Pro-\ncessing (NLP) ﬁeld has advanced signiﬁcantly\nin the last two years, introducing ﬁne-tuning\napproaches akin to those seen in computer vi-\nsion some years earlier (Donahue et al., 2013).\nThisgrowth originated fromfeature-based trans-\nfer learning, which in the form of word embed-\ndings has been in usefor some years, particularly\ndriven by (Mikolov, Chen, Corrado, & Dean,\n2013). As part of this new wave, we have\nseen advancements in feature-based transfer\nlearni",
    "title": "Small data machine learning in materials science",
    "abstract": "This review discussed the dilemma of small data faced by materials machine learning. First, we analyzed the limitations brought by small data. Then, the workflow of materials machine learning has been introduced. Next, the methods of dealing with small data were introduced, including data extraction from publications, materials database construction, high-throughput computations and experiments from the data source level; modeling algorithms for small data and imbalanced learning from the algorithm level; active learning and transfer learning from the machine learning strategy level. Finally, the future directions for small data machine learning in materials science were proposed.",
    "link": "https://www.semanticscholar.org/paper/35b1d79993f0e4fbfcb3b86c5013c5e2a7e3117c",
    "published": "2023-03-25"
  },
  "25": {
    "pdf_path": "data/pdfs\\machine learning_paper_121.pdf",
    "text_excerpt": "Noname manuscript No.\n(will be inserted by the editor)\nMachine Learning with a Reject Option: A survey\nHendrickx Kilian* ·Perini Lorenzo* ·Van\nder Plas Dries ·Meert Wannes ·Davis Jesse\nReceived: 23 July 2021 / Revised: 27 July 2023 & 15 February 2024\nAbstract Machine learning models always make a prediction, even when it is\nlikely to be inaccurate. This behavior should be avoided in many decision support\napplications, where mistakes can have severe consequences. Albeit already studied\nin 1970, machine learning with rejection recently gained interest. This machine\nlearning subfield enables machine learning models to abstain from making a pre-\ndiction when likely to make a mistake.\nThis survey aims to provide an overview on machine learning with rejection.\nWe introduce the conditions leading to two types of rejection, ambiguity and nov-\nelty rejection, which we carefully formalize. Moreover, we review and categorize\nstrategies to evaluate a model’s predictive and rejective quality. Additionally, we\ndefine the existing architectures for models with rejection and describe the stan-\ndard techniques for learning such models. Finally, we provide examples of relevant\n* These authors contributed equally to this work.\n·K. Hendrickx\nSiemens Digital Industries Software, Leuven, Belgium\nKU Leuven, Leuven, Belgium\nE-mail: kilian.hendrickx@cs.kuleuven.be\nL. Perini\nKU Leuven, Leuven, Belgium\nE-mail: lorenzo.perini@cs.kuleuven.be\nD. Van der Plas\nOSG bv, Natus Medical, Kontich, Belgium\nKU Leuven, Leuven, Belgium\nUniversity of Antwerp, Antwerp, Belgium\nE-mail: dries.vanderplas@cs.kuleuven.be\nW. Meert\nKU Leuven; Leuven.AI, Leuven, Belgium\nE-mail: wannes.meert@cs.kuleuven.be\nJ. Davis\nKU Leuven; Leuven.AI, Leuven, Belgium\nE-mail: jesse.davis@cs.kuleuven.be\nCorresponding author: Kilian HendrickxarXiv:2107.11277v3  [cs.LG]  21 Feb 20242 Hendrickx K., Perini L., et al\napplication domains and show how machine learning with rejection relates to other\nmachine learning research areas.\nKeywords ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "26": {
    "pdf_path": "data/pdfs\\machine learning_paper_122.pdf",
    "text_excerpt": "PSL is Dead. Long Live PSL\nKevin Smith\nRice University\nHouston, U.S.A.\nkwsmith@rice.eduHai Lin\nPalo Alto Networks\nSanta Clara, U.S.A.\nhalin@paloaltonetworks.comPraveen Tiwari\nPalo Alto Networks\nSanta Clara, U.S.A.\nprtiwari@paloaltonetworks.com\nMarjorie Sayer\nPalo Alto Networks\nSanta Clara, U.S.A.\nmsayer@paloaltonetworks.comClaudionor Coelho\nPalo Alto Networks\nSanta Clara, U.S.A.\nccoelho@paloaltonetworks.com\nAbstract —Property Speciﬁcation Language (PSL) is a form of\ntemporal logic that has been mainly used in discrete domains\n(e.g. formal hardware veriﬁcation). In this paper, we show that\nby merging machine learning techniques with PSL monitors,\nwe can extend PSL to work on continuous domains. We apply\nthis technique in machine learning-based anomaly detection to\nanalyze scenarios of real-time streaming events from continuous\nvariables in order to detect abnormal behaviors of a system.\nBy using machine learning with formal models, we leverage the\nstrengths of both machine learning methods and formal semantics\nof time. On one hand, machine learning techniques can produce\ndistributions on continuous variables, where abnormalities can be\ncaptured as deviations from the distributions. On the other hand,\nformal methods can characterize discrete temporal behaviors\nand relations that cannot be easily learned by machine learning\ntechniques. Interestingly, the anomalies detected by machine\nlearning and the underlying time representation used are discrete\nevents. We implemented a temporal monitoring package (TEF)\nthat operates in conjunction with normal data science packages\nfor anomaly detection machine learning systems, and we show\nthat TEF can be used to perform accurate interpretation of\ntemporal correlation between events.\nIndex Terms —formal methods, PSL, anomaly detection\nI. I NTRODUCTION\nProperty Speciﬁcation Language (PSL) is a form of tem-\nporal logic that is designed to capture temporal relations\nbetween discrete variables over discrete time. Due to this\nnature, PS",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "27": {
    "pdf_path": "data/pdfs\\machine learning_paper_123.pdf",
    "text_excerpt": "A Declarative Query Language for Scientific Machine Learning\nHasan M. Jamil\nUniversity of Idaho, USA\njamil@uidaho.edu\nABSTRACT\nThe popularity of data science as a discipline and its importance\nin the emerging economy and industrial progress dictate that ma-\nchine learning be democratized for the masses. This also means that\nthe current practice of workforce training using machine learning\ntools, which requires low-level statistical and algorithmic details,\nis a barrier that needs to be addressed. Similar to data management\nlanguages such as SQL, machine learning needs to be practiced at\na conceptual level to help make it a staple tool for general users. In\nparticular, the technical sophistication demanded by existing ma-\nchine learning frameworks is prohibitive for many scientists who\nare not computationally savvy or well versed in machine learning\ntechniques. The learning curve to use the needed machine learning\ntools is also too high for them to take advantage of these powerful\nplatforms to rapidly advance science. In this paper, we introduce\na new declarative machine learning query language, called MQL ,\nfor naive users. We discuss its merit and possible ways of imple-\nmenting it over a traditional relational database system. We discuss\ntwo materials science experiments implemented using MQL on a\nmaterials science workflow system called MatFlow.\nCCS CONCEPTS\n•Computing methodologies →Machine learning ;Artificial in-\ntelligence ;•Information systems →Query languages ; Database\ndesign and models; •Human-centered computing ;\nKEYWORDS\nTranslational semantics, declarative query language.\nACM Reference Format:\nHasan M. Jamil. 2024. A Declarative Query Language for Scientific Machine\nLearning. In Proceedings of ACM Conference (Conference’17). ACM, New\nYork, NY, USA, Article 4, 12 pages. https://doi.org/10.1145/xxxxxxxxxx\n1 INTRODUCTION\nIn this paper, we ask the question, how difficult it is to design a\ndeclarative query language for machine learning (ML) analysis by\npoi",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "28": {
    "pdf_path": "data/pdfs\\machine learning_paper_124.pdf",
    "text_excerpt": "arXiv:1303.2104v1  [cs.LG]  8 Mar 2013TransferLearning for Voice Activity Detection: ADenoisin g Deep Neural\nNetwork Perspective\nXiao-Lei Zhang,Ji Wu\nMultimediaSignal andIntelligentInformationProcessing Laboratory,\nTsinghuaNationalLaboratoryforInformationScience and T echnology,\nDepartmentofElectronicEngineering,TsinghuaUniversit y,Beijing,China.\nhuoshan6@126.com, wu_ji@tsinghua.edu.cn\nAbstract\nMismatching problem between the source and target noisy cor -\npora severely hinder the practical use of the machine-learn ing-\nbasedvoiceactivitydetection(VAD).Inthispaper,wetryt oad-\ndressthisprobleminthetransferlearningprospective. Tr ansfer\nlearning tries toﬁnd a common learning machine or a common\nfeature subspace that is shared by both the source corpus and\nthe target corpus. The denoising deep neural network is used\nas the learning machine. Three transfer techniques, which a im\nto learn common feature representations, are used for analy sis.\nExperimental resultsdemonstrate the effectiveness ofthe trans-\nfer learningschemes onthe mismatch problem.\nIndex Terms : deep learning, domain adaptation, feature learn-\ning, transfer learning, voice activitydetection.\n1. Introduction\nVoice activity detectors (VADs) aim to discover speech from\nitsbackground noises. Theyare important frontends ofmode rn\nspeech recognition systems [1–3] and speech signal process ing\nsystems [4]. Recently, the machine-learning-based VADs[5 –9]\nhave received much attention in that they not only can be inte -\ngrated to the speech recognition systems naturally but also can\nfuse the advantages of multiple features [10–15] much bette r\nthan traditional VADs. However, the machine-learning-bas ed\nVAD is still far from its practical use. One signiﬁcant prob-\nlem is that we are not sure whether the VAD model trained in\na given source corpus is still powerful in a target corpus whi ch\nmight have a different distributionwiththe source corpus.\nIn this paper, we try to deal with the aforementioned prob-\nlem by a no",
    "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "link": "https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e",
    "published": "2014-06-03"
  },
  "29": {
    "pdf_path": "data/pdfs\\machine learning_paper_125.pdf",
    "text_excerpt": "arXiv:1807.10681v1  [cs.LG]  27 Jul 2018Learnable: Theory vs Applications\nMarina Sapir\nMarch 2018\nAbstract\nTwo diﬀerent views on machine learning problem: Applied lea rning\n(machine learning with business applications) and Agnosti c PAC learning\nare formalized and compared here. I show that, under some con ditions,\nthe theory of PAC Learnable provides a way to solve the Applie d learning\nproblem. However, the theory requires to have the training s ets so large,\nthat it would make the learning practically useless. I sugge st to shed\nsome theoretical misconceptions about learning to make the theory more\naligned with the needs and experience of practitioners.\n1 Introduction\nMachine learning includes a practical side as well as a theoretical side . Prac-\ntitioners solve real life problems, theoreticians study theory of lea rning. Prac-\ntitioners need help answering the questions the life poses. Theoret icians give\nthe answers. Unfortunately, they are, apparently, answering d iﬀerent questions,\naboutsome-whatdiﬀerentsubjects. Forexample,practitioners dealwithlimited\ndata and deadlines, while the theory talks about what happens when training\ndata increase indeﬁnitely. There appears to be some disconnect he re.\nMore the over, the practitioners often can not formulate their qu estions ex-\nactly, becausethelanguageoftheexistingtheorywasdevelopedb ytheoreticians\nto study diﬀerent issues and diﬀerent situations.\nHere I formulate the learning problem as it is encountered in practica l ap-\nplications, pose the related real life questions and show the answer s to these\nquestions which follow from PAC learnable theory. To do it, I express both\nquestions of practitioners and the results of the PAC learnable the ory in terms\nof problem solving.\n2 Practical point of view\nWhat is machine learning, practically speaking? Let us look on the typic al\nsituation when such a problem arises.\n12.1 Business learning scenario\nMachine learning is used when a business wants to model a relationship",
    "title": "Machine Learning for High-Speed Corner Detection",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/e0408181bccb7e3754dd5e6785ec47d8beb8b6bd",
    "published": "2006-05-07"
  },
  "30": {
    "pdf_path": "data/pdfs\\machine learning_paper_126.pdf",
    "text_excerpt": "Minimal Achievable Sufﬁcient Statistic Learning\nMilan Cvitkovic1G¨unther Koliander2\nAbstract\nWe introduce Minimal Achievable Sufﬁcient\nStatistic (MASS) Learning, a training method for\nmachine learning models that attempts to produce\nminimal sufﬁcient statistics with respect to a class\nof functions (e.g. deep networks) being optimized\nover. In deriving MASS Learning, we also intro-\nduce Conserved Differential Information (CDI),\nan information-theoretic quantity that — unlike\nstandard mutual information — can be usefully\napplied to deterministically-dependent continu-\nous random variables like the input and output\nof a deep network. In a series of experiments,\nwe show that deep networks trained with MASS\nLearning achieve competitive performance on su-\npervised learning and uncertainty quantiﬁcation\nbenchmarks.\n1. Introduction\nTherepresentation learning approach to machine learning\nfocuses on ﬁnding a representation Zof an input random\nvariableXthat is useful for predicting a random variable Y\n(Goodfellow et al., 2016).\nWhat makes a representation Z“useful” is much debated,\nbut a common assertion is that Zshould be a minimal sufﬁ-\ncient statistic ofXforY(Adragni, Koﬁ P. & Cook, R. Den-\nnis, 2009; Shamir et al., 2010; James et al., 2017; Achille &\nSoatto, 2018b). That is:\n1.Zshould be a statistic ofX. This means Z=f(X)\nfor some function f.\n2.Zshould be sufﬁcient forY. This means p(XjZ;Y) =\np(XjZ).\n3.Given thatZis a sufﬁcient statistic, it should be mini-\nmalwith respect to X. This means for any measurable,\n1Department of Computing and Mathematical Sciences, Califor-\nnia Institute of Technology, Pasadena, California, USA2Acoustics\nResearch Institute, Austrian Academy of Sciences, Vienna, Austria.\nCorrespondence to: Milan Cvitkovic <mcvitkov@caltech.edu >.\nProceedings of the 36thInternational Conference on Machine\nLearning , Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).non-invertible function g,g(Z)is no longer sufﬁcient\nforY.1\nIn other words: a ",
    "title": "Machine Learning for Fluid Mechanics",
    "abstract": "The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications.",
    "link": "https://www.semanticscholar.org/paper/4087e84fc695bb6433d0104ee94f9d7e9f4b7da5",
    "published": "2019-05-27"
  },
  "31": {
    "pdf_path": "data/pdfs\\machine learning_paper_127.pdf",
    "text_excerpt": "SUSTAINABLE FEDERATED LEARNING\nBa¸ sak Güler\u0003Aylin Yener\u0003\u0003\n\u0003University of California, Riverside\nDepartment of Electrical and Computer Engineering\nRiverside, CA 92521\nbguler@ece.ucr.edu\n\u0003\u0003The Ohio State University\nDepartment of Electrical and Computer Engineering\nColumbus, OH 43210\nyener@ece.osu.edu\nABSTRACT\nPotential environmental impact of machine learning by large-scale wireless networks is a major\nchallenge for the sustainability of future smart ecosystems. In this paper, we introduce sustainable\nmachine learning in federated learning settings, using rechargeable devices that can collect energy\nfrom the ambient environment. We propose a practical federated learning framework that leverages\nintermittent energy arrivals for training, with provable convergence guarantees. Our framework can\nbe applied to a wide range of machine learning settings in networked environments, including\ndistributed and federated learning in wireless and edge networks. Our experiments demonstrate\nthat the proposed framework can provide signiﬁcant performance improvement over the benchmark\nenergy-agnostic federated learning settings.\nIndex Terms\nSustainable machine learning, federated learning, green AI.\nI. Introduction\nThe environmental impact of machine learning matters. Modern machine learning systems consume massive amounts\nof energy. In fact, the computational resources needed to train a state-of-the-art deep learning model has increased\nby 300000x between 2012-2018 [1]. Today, it is estimated that training a single deep learning model can generate\nas muchCO2as the total lifetime of ﬁve cars [2]. This impact will worsen with the emergence of machine learning\nin distributed and federated learning settings, where billions of devices are expected to train machine learning\nmodels on a regular basis. In this paper, we provide a ﬁrst study for sustainable machine learning in the federated\nlearning setting, through the use of compute devices that can generate energy from renewable sources in ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "32": {
    "pdf_path": "data/pdfs\\machine learning_paper_128.pdf",
    "text_excerpt": "Received: Added at production Revised: Added at production Accepted: Added at production\nDOI: xxx/xxxx\nARTICLE TYPE\nThe ART of Transfer Learning: An Adaptive and Robust Pipeline\nBoxiang Wang1| Yunan Wu*2| Chenglong Ye3\n1Department of Statistics and Actuarial\nScience, University of Iowa, IA, USA\n2Department of Mathematical Sciences,\nUniversity of Texas at Dallas, TX, USA\n3Dr. Bing Zhang Department of\nStatistics, University of Kentucky, KY,\nUSA\nCorrespondence\n*Yunan Wu. Email:\nyunan.wu@utdallas.eduAbstract\nTransfer learning is an essential tool for improving the performance of primary tasks by\nleveraging information from auxiliary data resources. In this work, we propose Adaptive\nRobust Transfer Learning (ART), a ﬂexible pipeline of performing transfer learning with\ngeneric machine learning algorithms. We establish the non-asymptotic learning theory of\nART, providing a provable theoretical guarantee for achieving adaptive transfer while\npreventing negative transfer. Additionally, we introduce an ART-integrated-aggregating\nmachine that produces a single ﬁnal model when multiple candidate algorithms are con-\nsidered. We demonstrate the promising performance of ART through extensive empirical\nstudies on regression, classiﬁcation, and sparse learning. We further present a real-data\nanalysis for a mortality study.\nKEYWORDS:\nAuxiliary data, Transfer learning, Negative transfer, Model aggregation, Variable Impor-\ntance\n1INTRODUCTION\nThe age of rapid technological change is unfolding in real time, empowering the collection of massive amounts of data in a variety of /uniFB01elds. Despite\nthis, many /uniFB01elds still struggle with data acquisition with limited sample sizes, particularly in experiments that involve human or animal subjects and\ncan be prohibitively expensive. To improve the performance of the primary task on those occasions, transfer learning has been widely advocated\nas a means of leveraging knowledge from available auxiliary data that are di/uniFB00erent while",
    "title": "Classification Based on Decision Tree Algorithm for Machine Learning",
    "abstract": "Decision tree classifiers are regarded to be a standout of the most well-known methods to data classification representation of classifiers. Different researchers from various fields and backgrounds have considered the problem of extending a decision tree from available data, such as machine study, pattern recognition, and statistics. In various fields such as medical disease analysis, text classification, user smartphone classification, images, and many more the employment of Decision tree classifiers has been proposed in many ways. This paper provides a detailed approach to the decision trees. Furthermore, paper specifics, such as algorithms/approaches used, datasets, and outcomes achieved, are evaluated and outlined comprehensively. In addition, all of the approaches analyzed were discussed to illustrate the themes of the authors and identify the most accurate classifiers. As a result, the uses of different types of datasets are discussed and their findings are analyzed.",
    "link": "https://www.semanticscholar.org/paper/0d6ef817813d04a3b3ec6c3ce008e104fb3e587a",
    "published": "2021-03-24"
  },
  "33": {
    "pdf_path": "data/pdfs\\machine learning_paper_129.pdf",
    "text_excerpt": "arXiv:1404.6674v1  [cs.LG]  26 Apr 2014OAGM Workshop 2014 (arXiv:1404.3538) 1\nA Comparison of First-order Algorithms for\nMachine Learning\nWei Yu Thomas Pock\nComputer Graphics and Vision, Graz University of Technolog y, Austria\nAbstract. Using an optimization algorithm to solve a machine learning\nproblem is one of mainstreams in the ﬁeld of science. In this w ork, we\ndemonstrate a comprehensive comparison of some state-of-t he-art ﬁrst-\norder optimization algorithms for convex optimization pro blems in ma-\nchine learning. We concentrate on several smooth and non-sm ooth ma-\nchine learning problems with a loss function plus a regulari zer. The over-\nall experimental results show the superiority of primal-du al algorithms in\nsolving a machine learning problem from the perspectives of the ease to\nconstruct, running time and accuracy.\n1 Introduction\nOptimization is the key of machine learning. Most machine le arning problems\ncan be cast as optimization problems. Furthermore, practic al applications of\nmachine learning usually involve a massive and complex data set. Thus, eﬃ-\nciency, accuracy and generalization of the optimization al gorithm (solver) should\nbe regarded as a crucial issue [2]. Many papers present dedic ated optimization\nalgorithms for speciﬁc machine learning problems. However , little attention has\nbeen devoted to the ability of a solver for a speciﬁc class of m achine learning\nproblems. The most common structure of machine learning pro blems is a loss\nfunction plus a regularizer. The loss function calculates t he disparity between\nthe prediction of a solution and the ground truth. This term u sually involves the\ntraining data set. For example, the well known square loss is for the purpose of\nregression problems and hinge loss is for the purpose of maxi mum margin clas-\nsiﬁcation. The regularizer usually uses a norm function. Fo r example, group\nlasso is an extension of the lasso for feature selection. It c an lead to a sparse\nsolution within a group.\nIn ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "34": {
    "pdf_path": "data/pdfs\\machine learning_paper_13.pdf",
    "text_excerpt": "Noname manuscript No.\n(will be inserted by the editor)\nData Pricing in Machine Learning Pipelines\nZicun Cong \u0001Xuan Luo \u0001Pei Jian \u0001\nFeida Zhu \u0001Yong Zhang\nReceived: date / Accepted: date\nAbstract Machine learning is disruptive. At the same time, machine learning\ncan only succeed by collaboration among many parties in multiple steps natu-\nrally as pipelines in an eco-system, such as collecting data for possible machine\nlearning applications, collaboratively training models by multiple parties and\ndelivering machine learning services to end users. Data is critical and pene-\ntrating in the whole machine learning pipelines. As machine learning pipelines\ninvolve many parties and, in order to be successful, have to form a construc-\ntive and dynamic eco-system, marketplaces and data pricing are fundamental\nin connecting and facilitating those many parties. In this article, we survey\nthe principles and the latest research development of data pricing in machine\nlearning pipelines. We start with a brief review of data marketplaces and pric-\ning desiderata. Then, we focus on pricing in three important steps in machine\nlearning pipelines. To understand pricing in the step of training data collection,\nwe review pricing raw data sets and data labels. We also investigate pricing\nin the step of collaborative training of machine learning models, and overview\nZicun Cong\nSimon Fraser University, Burnaby, Canada\nE-mail: zicun cong@cs.sfu.ca\nXuan Luo\nSimon Fraser University, Burnaby, Canada\nE-mail: xuan luo@cs.sfu.ca\nJian Pei\nSimon Fraser University, Burnaby, Canada\nE-mail: jpei@cs.sfu.ca\nFeida Zhu\nSingapore Management University, Singapore\nE-mail: fdzhu@smu.edu.sg\nYong Zhang\nHuawei Technologies Canada, Burnaby, Canada\nE-mail: yong.zhang3@huawei.comarXiv:2108.07915v1  [cs.LG]  18 Aug 20212 Zicun Cong et al.\npricing machine learning models for end users in the step of machine learning\ndeployment. We also discuss a series of possible future directions.\nKeywords Data Assets\u0001Data Pricing\u0001Data",
    "title": "Data Pricing in Machine Learning Pipelines",
    "abstract": "Machine learning is disruptive. At the same time, machine learning can only\nsucceed by collaboration among many parties in multiple steps naturally as\npipelines in an eco-system, such as collecting data for possible machine\nlearning applications, collaboratively training models by multiple parties and\ndelivering machine learning services to end users. Data is critical and\npenetrating in the whole machine learning pipelines. As machine learning\npipelines involve many parties and, in order to be successful, have to form a\nconstructive and dynamic eco-system, marketplaces and data pricing are\nfundamental in connecting and facilitating those many parties. In this article,\nwe survey the principles and the latest research development of data pricing in\nmachine learning pipelines. We start with a brief review of data marketplaces\nand pricing desiderata. Then, we focus on pricing in three important steps in\nmachine learning pipelines. To understand pricing in the step of training data\ncollection, we review pricing raw data sets and data labels. We also\ninvestigate pricing in the step of collaborative training of machine learning\nmodels, and overview pricing machine learning models for end users in the step\nof machine learning deployment. We also discuss a series of possible future\ndirections.",
    "link": "http://arxiv.org/abs/2108.07915v1",
    "published": "2021-08-18T00:57:06Z"
  },
  "35": {
    "pdf_path": "data/pdfs\\machine learning_paper_130.pdf",
    "text_excerpt": "Meaningful Models: Utilizing Conceptual Structure to Improve Machine\nLearning Interpretability\nNick Condry NSCONDRY @GWMAIL .GWU .EDU\nThe George Washington University, 2121 I St. NW, Washington, DC 20052 USA\nAbstract\nThe last decade has seen huge progress in the de-\nvelopment of advanced machine learning mod-\nels; however, those models are powerless un-\nless human users can interpret them. Here we\nshow how the mind‘s construction of concepts\nand meaning can be used to create more inter-\npretable machine learning models. By proposing\na novel method of classifying concepts, in terms\nof ‘form’ and ‘function’, we elucidate the nature\nof meaning and offer proposals to improve model\nunderstandability. As machine learning begins\nto permeate daily life, interpretable models may\nserve as a bridge between domain-expert authors\nand non-expert users.\n1. Introduction\nIn the last decade, machine learning algorithms have made\nhuge strides, producing state-of-the-art results across a\nnumber of domains including image recognition, speech\nrecognition, and natural language processing. However,\nwhile such results are exciting, there currently exists a gap\nbetween data modeling and knowledge extraction (Vellido\net al., 2012). Machine learning models are rendered power-\nless unless they can be interpreted, thus in order for knowl-\nedge to be extracted from a model, we must account for the\nhuman cognitive factors involved in such a process. Inter-\npretation must therefore be accounted for in machine learn-\ning processes, as shown in Figure 1. In addition to promot-\ning more transparent results, interpretable models enable\nnon-experts to utilize machine learning tools. For exam-\nple, a business manager is more likely to accept a model‘s\nrecommendations if its results can be presented in busi-\nness terms (Bose & Mahapatra, 2001). As an ever-growing\nnumber of professionals come to rely on machine learning\ntools, the most successful models will provide an elegant\n2016 ICML Workshop on Human I",
    "title": "Machine learning and deep learning",
    "abstract": "Today, intelligent systems that offer artificial intelligence capabilities often rely on machine learning. Machine learning describes the capacity of systems to learn from problem-specific training data to automate the process of analytical model building and solve associated tasks. Deep learning is a machine learning concept based on artificial neural networks. For many applications, deep learning models outperform shallow machine learning models and traditional data analysis approaches. In this article, we summarize the fundamentals of machine learning and deep learning to generate a broader understanding of the methodical underpinning of current intelligent systems. In particular, we provide a conceptual distinction between relevant terms and concepts, explain the process of automated analytical model building through machine learning and deep learning, and discuss the challenges that arise when implementing such intelligent systems in the field of electronic markets and networked business. These naturally go beyond technological aspects and highlight issues in human-machine interaction and artificial intelligence servitization.",
    "link": "https://www.semanticscholar.org/paper/a0f303b6e22ef52943355993f57d65938997066a",
    "published": "2021-04-08"
  },
  "36": {
    "pdf_path": "data/pdfs\\machine learning_paper_131.pdf",
    "text_excerpt": "arXiv:1801.04016v1  [cs.LG]  11 Jan 2018Theoretical Impediments to Machine Learning\nWith Seven Sparks from the Causal Revolution\nJudea Pearl\nUniversity of California, Los Angeles\nComputer Science Department\nLos Angeles, CA, 90095-1596, USA\njudea@cs.ucla.edu\nJanuary 15, 2018\nAbstract\nCurrent machine learning systems operate, almost exclusiv ely, in a statistical, or model-free mode,\nwhich entails severe theoretical limits on their power and p erformance. Suchsystems cannot reason about\ninterventions and retrospection and, therefore, cannot se rve as the basis for strong AI. To achieve human\nlevel intelligence, learning machines need the guidance of a model of reality, similar to the ones used in\ncausal inference tasks. To demonstrate the essential role o f suchmodels, I will present asummary of seven\ntasks which are beyond reach of current machine learning sys tems and which have been accomplished\nusing the tools of causal modeling.\nScientiﬁc Background\nIf we examine the information that drives machine learning today, we ﬁnd that it is almost entirely\nstatistical. In other words, learning machines improve their perfor mance by optimizing parameters over a\nstream of sensory inputs received from the environment. It is a slo w process, analogous in many respects\nto the natural selection process that drives Darwinian evolution. I t explains how species like eagles\nand snakes have developed superb vision systems over millions of yea rs. It cannot explain however the\nsuper-evolutionary process that enabled humans to build eyeglass es and telescopes over barely one thousand\nyears. What humans possessed that other species lacked was a me ntal representation, a blue-print of\ntheir environment which they could manipulate at will to imagine alternative hypothetical environments\nfor planning and learning. Anthropologists like N. Harari, and S. Mithe n are in general agreement that\nthe decisive ingredient that gave our Homo sapiens ancestors the a bility to achieve global dominio",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "37": {
    "pdf_path": "data/pdfs\\machine learning_paper_132.pdf",
    "text_excerpt": "Optimizing for Generalization in Machine Learning\nwith Cross-Validation Gradients\nShane Barratt Rishi Sharma\nDepartment of Electrical Engineering\nStanford University\n{sbarratt,rsh}@stanford.edu\nAbstract\nCross-validation is the workhorse of modern applied statistics and machine learn-\ning, as it provides a principled framework for selecting the model that maximizes\ngeneralization performance. In this paper, we show that the cross-validation risk\nis differentiable with respect to the hyperparameters and training data for many\ncommon machine learning algorithms, including logistic regression, elastic-net\nregression, and support vector machines. Leveraging this property of differentiabil-\nity, we propose a cross-validation gradient method (CVGM) for hyperparameter\noptimization. Our method enables efﬁcient optimization in high-dimensional hy-\nperparameter spaces of the cross-validation risk, the best surrogate of the true\ngeneralization ability of our learning algorithm.\n1 Introduction\nThe ultimate aim of a supervised learning method is generalization , that is, achieving good prediction\nability on unseen test data given only a ﬁnite set of training data. The generalization capability\nof learning algorithms should be the primary criterion for model selection, yet an algorithm’s\ngeneralization capability is a somewhat elusive quantity that is challenging to optimize for. In this\npaper we introduce a method to optimize directly for the closest available proxy to generalization\nperformance: cross-validation loss.\nWe begin with a formal description of the overall goal in predictive learning, which also serves as an\nintroduction to notation used throughout the paper. The task of predictive learning involves deriving\na prediction function from a ﬁnite set of training data. More formally, suppose that (x;y)2X\u0002Y\nhave some joint probability distribution. We have access to a ﬁnite dataset of Ntraining examples\nzi2Z=X\u0002Y drawn i.i.d. from the joint distribution, denoted\nS=fz1;z2;:::",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "38": {
    "pdf_path": "data/pdfs\\machine learning_paper_133.pdf",
    "text_excerpt": "Algebraic Expression of Subjective Spatial and Temporal\nPatterns\u0003\nChuyu Xiong\nIndependent researcher, New York, USA\nEmail: chuyux99@gmail.com\nJune 1, 2018\nAbstract\nUniversal learning machine is a theory trying to study machine learning from mathematical\npoint of view. The outside world is re\rected inside an universal learning machine according to\npattern of incoming data. This is subjective pattern of learning machine. In [2, 4], we discussed\nsubjective spatial pattern, and established a powerful tool { X-form, which is an algebraic expres-\nsion for subjective spatial pattern. However, as the initial stage of study, there we only discussed\nspatial pattern. Here, we will discuss spatial and temporal patterns, and algebraic expression for\nthem.\nKeywords: Universal learning machine, spatial pattern, temporal pattern, objective\npattern, subjective pattern, sX-form, tX-form, X-form, algebraic expression, con-\nceiving space\n1 Introduction\nMachine learning, especially deep learning, currently is a very active area of research. However,\nunfortunately, a thorough mathematical foundation for deep learning is still missing. For example,\nwhat deep learning really is doing inside is still quite mystery. Universal learning machine is a\nmathematical theory to study machine learning by trying to understand patterns. By de\fnition, an\nuniversal learning machine is a machine can learn any pattern from data. Pattern can be spatial, and\ncan be temporal. In order to reduce the complexity, we focus on spatial pattern only at \frst. We\ndeveloped theory of universal learning machine for only spatial pattern [2, 4].\nIn the studies in [2, 4], we found that one very crucial step is to \fnd a way to express the subjective\nview of a learning machine to its outside world. Thus, we introduced one e\u000bective tool, which we\ncalled as X-form. X-form is an algebraic expression of subjective spatial pattern. By using X-form, we\nindeed get deeper understanding of learning machine. For example, we can de\fne ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "39": {
    "pdf_path": "data/pdfs\\machine learning_paper_134.pdf",
    "text_excerpt": " \nMachine Learning in Official Statistics  \n \nMartin Beck1, Florian Dumpert2, Joerg Feuerhake3 \n \nAs of 13 December 2018.  \n \n \n \nAbstract  \nIn the first half of 2018, the Federal Statistical Office of Germany (Destatis) carried out a \"Proof of \nConcept Machine Learning\" as part of its Digital Agenda. A major component of this was surveys \non the use of machine learning methods in official statistics, which were conducted at selected \nnational and international statistical in stitutions and among the divisions of Destatis. It was of \nparticular interest to find out in which statistical areas and for which tasks machine learning is \nused and which methods are applied. This paper is intended to make the results of the surveys \npubli cly accessible.  \n \nKeywords and phrases: Machine learning; official statistics; applications  \n                                                           \n1 Federal Statistical Office of Germany; corresponding author martin.beck@destatis.de  \n2 Department of Mathematics, University of Bayreuth, Germany  \n3 Federal Statistical Office of Germany  Martin Beck; Florian Dumpert; Joerg Feuerhake: Machine Learning in Official Statistics  \n \n2 \n 1 Introduction  \n1.1 Digital Agenda of  the Federal Statistical Office of Germany  \nOn 10 October 2017, the development of a Digital Agenda of  the Federal Statistical Office of \nGermany (Destatis) has started  (Statistisches  Bundesamt  2018) . One of many topics that were \nintensively discussed was Machine Learning. In a meeting at 13–15 November 2017, the office \nand department heads of Destatis evaluated and prioritised 59 measures of the Digital Agenda \naccording to their benefits and costs. A  “Proof of Concept Machine Learning ” was given high \npriority and classified as one of four lighthouse projects of the Digital Agenda. The content \nspecificat ion was “ Proof of Concept Machine Learning –  Set up Proof of Concept for Machine \nLearning, e.g. in business statistics, to perform automatic categori",
    "title": "A guide to machine learning for biologists",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/80d9f0eb47b712988d19cbe29a7bfa63f2a175d0",
    "published": "2021-09-13"
  },
  "40": {
    "pdf_path": "data/pdfs\\machine learning_paper_135.pdf",
    "text_excerpt": "Seven Myths in Machine Learning Research\nOscar Chang, Hod Lipson\nFebruary 2019\nAbstract\nWe present seven myths commonly believed to be true in machine\nlearning research, circa Feb 2019. This is an archival copy of the blog post\nat https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-\nlearning-research/\nMyth 1: TensorFlow is a Tensor manipulation library\nMyth 2: Image datasets are representative of real images found in the\nwild\nMyth 3: Machine Learning researchers do not use the test set for valida-\ntion\nMyth 4: Every datapoint is used in training a neural network\nMyth 5: We need (batch) normalization to train very deep residual net-\nworks\nMyth 6: Attention >Convolution\nMyth 7: Saliency maps are robust ways to interpret neural networks\nMyth 1: TensorFlow is a Tensor manipulation\nlibrary\nIt is actually a Matrix manipulation library, and this di\u000berence is signi\fcant.\nIn Laue et al. [2018], the authors demonstrate that their automatic di\u000beren-\ntiation library based on actual Tensor Calculus has signi\fcantly more compact\nexpression trees. This is because Tensor Calculus uses index notation, which\nresults in treating both the forward mode and the reverse mode in the same\nmanner.\nBy contrast, Matrix Calculus hides the indices for notational convenience,\nand this often results in overly complicated automatic di\u000berentiation expression\ntrees.\nConsider the matrix multiplication C=AB. We have _C=_AB+A_Bfor the\nforward mode and \u0016A=\u0016CBT;\u0016B=AT\u0016Cfor the reverse mode. To perform the\nmultiplications correctly, we have to be careful about the order of multiplication\nand the use of transposes. Notationally, this is a point of confusion for the\nmachine learning practitioner, but computationally, this is an overhead for the\nprogram.\n1arXiv:1902.06789v2  [cs.LG]  22 Feb 2019Here's another example, which is decidedly less trivial: c=det(A). We\nhave _ c=tr(inv(A)_A) for the forward mode, and \u0016A= \u0016ccinv (A)Tfor the reverse\nmode. In this case, it is clearly not possible to use ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "41": {
    "pdf_path": "data/pdfs\\machine learning_paper_136.pdf",
    "text_excerpt": "Optimal Algorithms for Ski Rental with Soft\nMachine-Learned Predictions\nRohan Kodialam\nMassachusetts Institute of Technology\nCambridge MA\nkodialam@mit.edu\nAbstract\nWe consider a variant of the classic Ski Rental online algorithm with applications to\nmachine learning. In our variant, we allow the skier access to a black-box machine\nlearning algorithm that provides an estimate of the probability that there will be\nless than a threshold number of ski-days. We derive a class of optimal randomized\nalgorithms to determine the strategy that minimizes the worst-case expected com-\npetitive ratio for the skier given a prediction from the machine learning algorithm,\nand analyze the performance and robustness of these algorithms.\n1 Introduction\nOnline decision-making problems fundamentally address the issue of dealing with the uncertainty\ninherently present in the future. In broad terms, these problems can be addressed in two ways. First,\na predictive approach like a machine learning algorithm can be used to guess at future events and\nto act accordingly. This method, while clearly powerful, has the drawback that it is very difﬁcult to\nmake any guarantees about the performance of the algorithm.\nAnother paradigm for solving these problems is to use competitive analysis to guarantee a bound on\nthe performance of a given algorithm. In this approach, we consider some cost function and note\nthe cost that would be incurred by an omniscient algorithm that could access future data. Then, we\ncompare this optimal omniscient cost to the worst-case cost of an online algorithm which cannot use\nfuture data. By bounding the ratio of these two costs, a guarantee can be made as to how well the\nonline algorithm will perform in terms of the performance of an omniscient algorithm.\nA classic problem in competitive analysis is the Ski Rental Problem . In this problem, a skier must\ndecide whether to rent skis at a rate of $1per day, or to buy skis at a price of $B. The uncertainty\nlies in the number o",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "42": {
    "pdf_path": "data/pdfs\\machine learning_paper_137.pdf",
    "text_excerpt": "Towards Quantiﬁcation of Bias in Machine Learning\nfor Healthcare: A Case Study of Renal Failure\nPrediction\nJosie V . Williams\njvw242@nyu.edu\nDepartments of Computer Science\nCourant Institute of Mathematical Sciences\nNew York UniversityNarges Razavian\nnarges.razavian@nyumc.org\nDepartments of Population Health and Radiology\nCenter for Data Science\nNew York University Langone Medical Center\nAbstract\nAs machine learning (ML) models, trained on real-world datasets, become com-\nmon practice, it is critical to measure and quantify their potential biases. In this\npaper, we focus on renal failure and compare a commonly used traditional risk\nscore, Tangri, with a more powerful machine learning model, which has access to\na larger variable set and trained on 1.6 million patients’ EHR data. We will com-\npare and discuss the generalization and applicability of these two models, in an\nattempt to quantify biases of status quo clinical practice, compared to ML-driven\nmodels.\n1 Introduction\nData-driven models have become more common in the U.S. healthcare ﬁeld as their use in clinical\noperations and diagnosing procedures have expanded exponentially. The ever-increasing processing\npower of machine-learning algorithms allows automatic analysis of huge quantities of data, theoret-\nically maximizing the efﬁciency and accuracy of the medical diagnosing process. Predictions from\nmachine-learning models already drive important healthcare decisions for over 70 million people\nacross the United States[7]. However, there is reason to be cautious about assuming that the gen-\neralization of these models can be applied universally to all populations. Data-driven algorithms\nused in the criminal sentencing process, for example, have already shown evidence of negatively\nand disproportionately targeting defendants from low-income black communities due to biases in-\nherent to the data used to train these predictive models[1]. Considering the fact that health care\ndelivery already varies considerably by",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "43": {
    "pdf_path": "data/pdfs\\machine learning_paper_138.pdf",
    "text_excerpt": "arXiv:1911.07749v1  [cs.LG]  15 Nov 2019On the computation of counterfactual\nexplanations - A survey\nAndr´ e Artelt∗and Barbara Hammer†\nCITEC - Cognitive Interaction Technology\nBielefeld University - Faculty of Technology\nInspiration 1, 33619 Bielefeld - Germany\nAbstract . Due to the increasing use of machine learning in practice it\nbecomes more and more important to be able to explain the pred iction\nand behavior of machine learning models. An instance of expl anations are\ncounterfactual explanations which provide an intuitive an d useful expla-\nnations of machine learning models.\nIn this survey we review model-speciﬁc methods for eﬃcientl y computing\ncounterfactual explanations ofmanydiﬀerentmachinelear ningmodels and\npropose methods for models that have not been considered in l iterature so\nfar.\n1 Introduction\nDue to recent advances in machine learning (ML), ML methods are inc reasingly\nuse in real world scenarios [1–4]. Especially, ML technology is nowaday s used in\ncritical situations like predictive policing [5] and loan approval [6]. In or der to\nincrease trust and acceptance of these kind of technology, it is imp ortant to be\nable to explain the behaviour and prediction of these models [7] - in pa rticular\nanswer questions like “Why did the model do that? And why not smth. else?”.\nThis becomes even more important in view to legal regulations like the E U\nregulation on GDPR [8], that grants the user a right to an explanation .\nA popular method for explaining models [7,9–11] are counterfactua l expla-\nnations (often just called counterfactuals) [12]. A counterfactu al explanation\nstates changes to some features that lead to a diﬀerent (speciﬁe d) behaviour or\nprediction of the model. Thus, counterfactual explanation can be interpreted\nas a recommendation what to do in order to achieve a requested goa l. This\nis why counterfactual explanations are that popular - they are int uitive and\nuser-friendly [7,12].\nCounterfactualexplanationsareaninstanceofmodel-agnosti",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "44": {
    "pdf_path": "data/pdfs\\machine learning_paper_139.pdf",
    "text_excerpt": "arXiv:1911.12593v1  [cs.CY]  28 Nov 2019Computer Systems Have 99 Problems, Let’s Not\nMake Machine Learning Another One\nDavid Mohaisen†and Songqing Chen‡\n†University of Central Florida‡George Mason University\nAbstract —Machine learning techniques are ﬁnding many ap-\nplications in computer systems, including many tasks that r e-\nquire decision making: network optimization, quality of se rvice\nassurance, and security. We believe machine learning syste ms are\nhere to stay, and to materialize on their potential we advoca te\na fresh look at various key issues that need further attentio n,\nincluding security as a requirement and system complexity, and\nhow machine learning systems affect them. We also discuss\nreproducibility as a key requirement for sustainable machi ne\nlearning systems, and leads to pursuing it.\nI. I NTRODUCTION\nOver the past decade, machine learning has grown as\nscientiﬁc and perhaps independent discipline for the study of\nstatistical models that can help in performing tasks, such a s\nclassiﬁcation, without relying on explicitly deﬁned rules , and\nrather on patterns and inferences made from data used for\nbuilding those “data models”. For example, samples of data\nare typically used in machine learning algorithms to build a\nmodel in a process called “model training”, which (the model )\nthen is used for making decisions concerning samples of the\nsame type as those used in building the model.\nThe applications of machine learning in computer and\nnetworked systems are ubiquitous, with hundreds of appli-\ncations and thousands of publications emerging every year.\nThose applications include areas of computer networks and\nsimulation [1], pattern recognition [2], general designs [ 3],\ndata quality assurance [4], attack detection and security e n-\nhancement [5], [6], [7], [8], network and computer systems\noptimization through forecasting and classiﬁcation [9], u ser\nbehavior analysis for prioritization and optimization [10 ], [11],\namong many others. Machine learning",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "45": {
    "pdf_path": "data/pdfs\\machine learning_paper_14.pdf",
    "text_excerpt": "Techniques for Automated Machine Learning\nYi-Wei Chen, Qingquan Song, Xia Hu\nDepartment of Computer Science and Engineering\nTexas A&M University\n{yiwei_chen, song_3134, xiahu}@tamu.edu\nABSTRACT\nAutomated machine learning (AutoML) aims to ﬁnd opti-\nmal machine learning solutions automatically given a ma-\nchine learning problem. It could release the burden of data\nscientists from the multifarious manual tuning process and\nenable the access of domain experts to the oﬀ-the-shelf ma-\nchine learning solutions without extensive experience. In\nthis paper, we review the current developments of AutoML\nin terms of three categories, automated feature engineer-\ning (AutoFE), automated model and hyperparameter learn-\ning (AutoMHL), and automated deep learning (AutoDL).\nState-of-the-art techniques adopted in the three categories\nare presented, including Bayesian optimization, reinforce-\nment learning, evolutionary algorithm, and gradient-based\napproaches. We summarize popular AutoML frameworks\nand conclude with current open challenges of AutoML.\n1. INTRODUCTION\nAutomated machine learning (AutoML) has emerged as\na prevailing research ﬁeld upon the ubiquitous adoption of\nmachine learning techniques. It aims at automatically de-\ntermininghigh-performancemachinelearningsolutionswith\na little workforce in reasonable time budget. For example,\nGoogle HyperTune, Amazon Model Tuning, and Microsoft\nAzureAutoMLallprovidecloudserviceswithAutoMLtools\nwhich cultivate oﬀ-the-shelf machine learning solutions for\nboth researchers and practitioners. Therefore, AutoML not\nonly liberates them from the time-consuming tuning process\nand tedious trial-and-error iterations but also facilitates the\ndevelopment of solving machine learning problems.\nAtraditionalmachinelearningpipelineisaniterativepro-\ncedure composed of feature engineering, model and hyper-\nparameter selection, and performance evaluation, as shown\nin Figure /one.oldstyle. Data scientists manually manipulate numerous\nfeatures, design models,",
    "title": "Techniques for Automated Machine Learning",
    "abstract": "Automated machine learning (AutoML) aims to find optimal machine learning\nsolutions automatically given a machine learning problem. It could release the\nburden of data scientists from the multifarious manual tuning process and\nenable the access of domain experts to the off-the-shelf machine learning\nsolutions without extensive experience. In this paper, we review the current\ndevelopments of AutoML in terms of three categories, automated feature\nengineering (AutoFE), automated model and hyperparameter learning (AutoMHL),\nand automated deep learning (AutoDL). State-of-the-art techniques adopted in\nthe three categories are presented, including Bayesian optimization,\nreinforcement learning, evolutionary algorithm, and gradient-based approaches.\nWe summarize popular AutoML frameworks and conclude with current open\nchallenges of AutoML.",
    "link": "http://arxiv.org/abs/1907.08908v1",
    "published": "2019-07-21T04:03:36Z"
  },
  "46": {
    "pdf_path": "data/pdfs\\machine learning_paper_140.pdf",
    "text_excerpt": "Sample title AIP/123-QED\nWhen Machine Learning Meets Multiscale Modeling in Chemical Reactions\nWuyue Yang,1,a)Liangrong Peng,2,a)Yi Zhu,1and Liu Hong1,b)\n1)Zhou Pei-Yuan Center for Applied Mathematics, Tsinghua, Beijing, 100084,\nP .R. China\n2)College of Mathematics and Data Science, Minjiang University, Fuzhou, 350108,\nP .R. China\n(Dated: 2 June 2020)\nDue to the intrinsic complexity and nonlinearity of chemical reactions, direct applications\nof traditional machine learning algorithms may face with many difﬁculties. In this study,\nthrough two concrete examples with biological background, we illustrate how the key ideas\nof multiscale modeling can help to reduce the computational cost of machine learning a\nlot, as well as how machine learning algorithms perform model reduction automatically\nin a time-scale separated system. Our study highlights the necessity and effectiveness of\nan integration of machine learning algorithms and multiscale modeling during the study of\nchemical reactions.\na)These authors have contributed equally to this work.\nb)Electronic mail: zcamhl@tsinghua.edu.cn\n1arXiv:2006.00700v1  [q-bio.MN]  1 Jun 2020Sample title\nI. INTRODUCTION\nAccompanied with the matter synthesis and decomposition, energy storage and release, bio-\nfunction activation and deactivation, chemical reactions play a fundamental role in multiple\ndisciplines1, including biology, chemical engineering, materials science and so on. They help\nto model complicated phenomena in nature by an explicit reaction network, to allow the inter-\npretation of observed data through quantitative mathematical equations, and to translate varied\nexperimental conditions into tunable reaction rates and reaction orders. Due to their high com-\nplexity and nonlinearity, the previous studies of chemical reactions heavily rely on sophisticated\nmathematical analysis and ﬁrst-principle calculations, like quantum chemistry2.\nThe ﬁrst mission of studies on chemical reactions is to obtain the proper mathematical mod",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "47": {
    "pdf_path": "data/pdfs\\machine learning_paper_141.pdf",
    "text_excerpt": "Efﬁcient Private Machine Learning by\nDifferentiable Random Transformations\n1stFei Zheng\nDept. Computer Science\nZhejiang University\nHangzhou, China\nzfscgy2@zju.edu.cn\nAbstract —With the increasing demands for privacy protec-\ntion, many privacy-preserving machine learning systems were\nproposed in recent years. However, most of them cannot be put\ninto production due to their slow training and inference speed\ncaused by the heavy cost of homomorphic encryption and secure\nmultiparty computation(MPC) methods. To circumvent this, I\nproposed a privacy deﬁnition which is suitable for large amount\nof data in machine learning tasks. Based on that, I showed\nthat random transformations like linear transformation and\nrandom permutation can well protect privacy. Merging random\ntransformations and arithmetic sharing together, I designed a\nframework for private machine learning with high efﬁciency and\nlow computation cost.\nIndex Terms —Secure Multiparty Computation, Machine\nLearning, Neural Network, Privacy Preserving\nI. I NTRODUCTION\nMachine learning has been widely used in many real-life\nscenarios in recent years. In most cases, training machine\nlearning models requires a large amount of data. For example,\ntraining a neural network to determine whether two pictures\nbelong to the same person may needs at least tens of thousands\nphotos, and training a model to predict the possibility of credit\ndefault of someone needs tens of thousands credit records\nof different people. Those data are always distributed among\ndifferent facilities. On the one hand, many governments have\npublished the laws against abuses of data in order to protect\npeople’s privacy and prevent those data from being stolen\nfor evil uses. On the other hand, the companies do not want\ntheir data being exposed to others. When they want to share\ndata with others, it’s always difﬁcult to ensure that the other\nparty will not store their data secretly for usages violates the\ncontract. So to make different data holders to share",
    "title": "Educational data mining: prediction of students' academic performance using machine learning algorithms",
    "abstract": "Educational data mining has become an effective tool for exploring the hidden relationships in educational data and predicting students' academic achievements. This study proposes a new model based on machine learning algorithms to predict the final exam grades of undergraduate students, taking their midterm exam grades as the source data. The performances of the random forests, nearest neighbour, support vector machines, logistic regression, Naïve Bayes, and k-nearest neighbour algorithms, which are among the machine learning algorithms, were calculated and compared to predict the final exam grades of the students. The dataset consisted of the academic achievement grades of 1854 students who took the Turkish Language-I course in a state University in Turkey during the fall semester of 2019–2020. The results show that the proposed model achieved a classification accuracy of 70–75%. The predictions were made using only three types of parameters; midterm exam grades, Department data and Faculty data. Such data-driven studies are very important in terms of establishing a learning analysis framework in higher education and contributing to the decision-making processes. Finally, this study presents a contribution to the early prediction of students at high risk of failure and determines the most effective machine learning methods.",
    "link": "https://www.semanticscholar.org/paper/0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f",
    "published": "2022-03-03"
  },
  "48": {
    "pdf_path": "data/pdfs\\machine learning_paper_142.pdf",
    "text_excerpt": "Distributed Double Machine Learning with a Serverless\nArchitecture\nMalte S. Kurz\nUniversity of Hamburg\nHamburg, Germany\nmalte.simon.kurz@uni-hamburg.de\nABSTRACT\nThis paper explores serverless cloud computing for double machine\nlearning. Being based on repeated cross-fitting, double machine\nlearning is particularly well suited to exploit the high level of par-\nallelism achievable with serverless computing. It allows to get fast\non-demand estimations without additional cloud maintenance ef-\nfort. We provide a prototype Python implementation DoubleML-\nServerless for the estimation of double machine learning models\nwith the serverless computing platform AWS Lambda and demon-\nstrate its utility with a case study analyzing estimation times and\ncosts.\nCCS CONCEPTS\n•Computer systems organization →Cloud computing ;•Com-\nputing methodologies →Machine learning .\nKEYWORDS\nMachine Learning; Serverless Computing; Function-as-a-Service\n(FaaS); Distributed Computing; AWS Lambda; Causal Machine\nLearning\n1 INTRODUCTION\nDouble machine learning (DML) models [ 19] are becoming increas-\ningly popular among statisticians, econometricians and data sci-\nentists with numerous methodological extensions [ 8,18,21,27–\n29,33,34,37] and applications in areas like finance [ 22], COVID-19\nresearch [ 20,39] or economics [ 30,38]. The DML models allow\nresearchers to exploit the excellent prediction power of machine\nlearning algorithms in a valid statistical framework for estimation\nand inference on causal parameters. Recently, the Python and R\npackages DoubleML with a flexible object-oriented structure for\nestimating double machine learning models have been published\n[6, 7].\nServerless cloud computing is predicted to be the dominating\nand default architecture of cloud computing in the coming decade\n(Berkley View on Serverless Computing [ 26]) and is becoming\nincreasingly adopted in the industry and by researchers. Its Func-\ntion as a Service (FaaS) paradigm lowers the entry bar to cloud\ncomputing tec",
    "title": "A Review of Feature Selection Methods for Machine Learning-Based Disease Risk Prediction",
    "abstract": "Machine learning has shown utility in detecting patterns within large, unstructured, and complex datasets. One of the promising applications of machine learning is in precision medicine, where disease risk is predicted using patient genetic data. However, creating an accurate prediction model based on genotype data remains challenging due to the so-called “curse of dimensionality” (i.e., extensively larger number of features compared to the number of samples). Therefore, the generalizability of machine learning models benefits from feature selection, which aims to extract only the most “informative” features and remove noisy “non-informative,” irrelevant and redundant features. In this article, we provide a general overview of the different feature selection methods, their advantages, disadvantages, and use cases, focusing on the detection of relevant features (i.e., SNPs) for disease risk prediction.",
    "link": "https://www.semanticscholar.org/paper/911fbaec109f72130815e05e2633ec879590382c",
    "published": "2022-06-27"
  },
  "49": {
    "pdf_path": "data/pdfs\\machine learning_paper_143.pdf",
    "text_excerpt": "Addressing Privacy Threats from Machine Learning\nMary Anne Smart\nDepartment of Computer Science & Engineering\nUniversity of California San Diego\nmsmart@ucsd.edu\nAbstract\nEvery year at NeurIPS, machine learning researchers gather and discuss exciting\napplications of machine learning in areas such as public health, disaster response,\nclimate change, education, and more1. However, many of these same researchers\nare expressing growing concern about applications of machine learning for surveil-\nlance (Nanayakkara et al., 2021). This paper presents a brief overview of strategies\nfor resisting these surveillance technologies and calls for greater collaboration\nbetween machine learning and human-computer interaction researchers to address\nthe threats that these technologies pose.\n1 Introduction\nA rapidly growing research area in computer science is that of privacy in machine learning (Papernot\net al., 2018; Mirshghallah et al., 2020; Liu et al., 2021). Differentially-private machine learning\nalgorithms and other privacy-preserving methods offer the opportunity to learn from sensitive datasets\nwhile limiting what can be revealed about any particular individual. However, some machine learning\nsystems violate privacy—not as some unintended side-effect, but by design. The tools from privacy-\npreserving machine learning are little use against machine learning models that are designed for\nbiometric or behavioral proﬁling (Katrina Ligett, 2020); different approaches are needed.\nAlthough a variety of deﬁnitions, frameworks, and taxonomies have been proposed, there is no\nsingle, universally agreed-upon deﬁnition of privacy (Solove, 2008; Nissenbaum, 2009; Arora, 2019;\nMcDonald and Forte, 2020). This paper focuses on the speciﬁc set of privacy-related harms that\nare perpetrated or exacerbated by machine learning systems. In an age where powerful algorithms\ncan process large datasets at high speeds, it becomes harder to ﬁnd privacy by hiding in a crowd.\nFacial recognition algorithms c",
    "title": "Human-in-the-loop machine learning: a state of the art",
    "abstract": "Researchers are defining new types of interactions between humans and machine learning algorithms generically called human-in-the-loop machine learning. Depending on who is in control of the learning process, we can identify: active learning, in which the system remains in control; interactive machine learning, in which there is a closer interaction between users and learning systems; and machine teaching, where human domain experts have control over the learning process. Aside from control, humans can also be involved in the learning process in other ways. In curriculum learning human domain experts try to impose some structure on the examples presented to improve the learning; in explainable AI the focus is on the ability of the model to explain to humans why a given solution was chosen. This collaboration between AI models and humans should not be limited only to the learning process; if we go further, we can see other terms that arise such as Usable and Useful AI. In this paper we review the state of the art of the techniques involved in the new forms of relationship between humans and ML algorithms. Our contribution is not merely listing the different approaches, but to provide definitions clarifying confusing, varied and sometimes contradictory terms; to elucidate and determine the boundaries between the different methods; and to correlate all the techniques searching for the connections and influences between them.",
    "link": "https://www.semanticscholar.org/paper/62cadbc4fcc73204a72847300cb2214f4401efad",
    "published": "2022-08-17"
  },
  "50": {
    "pdf_path": "data/pdfs\\machine learning_paper_144.pdf",
    "text_excerpt": "arXiv:2203.06430v2  [cs.LG]  9 May 2022Categories of Diﬀerentiable Polynomial Circuits\nfor Machine Learning\nPaul Wilson1,2[0000−0003−3575−135X]and Fabio Zanasi2[0000−0001−6457−1345]\n1University of Southampton\n2University College London\nAbstract. Reverse derivative categories (RDCs) have recently been sh own\nto be a suitable semantic framework for studying machine lea rning al-\ngorithms. Whereas emphasis has been put on training methodo logies,\nless attention has been devoted to particular model classes : the concrete\ncategories whose morphisms represent machine learning mod els. In this\npaper we study presentations by generators and equations of classes of\nRDCs. In particular, we propose polynomial circuits as a suitable ma-\nchine learning model. We give an axiomatisation for these ci rcuits and\nprove a functional completeness result. Finally, we discus s the use of\npolynomial circuits over speciﬁc semirings to perform mach ine learning\nwith discrete values.\n1 Introduction\nReverse Derivative Categories [11] have recently been intr oduced as a formal-\nism to study abstractly the concept of diﬀerentiable functi ons. As explored\nin [12], it turns out that this framework is suitable to give a categorical seman-\ntics for gradient-based learning. In this approach, models –as for instance neural\nnetworks–correspond to morphisms in some RDC. We think of th e particular\nRDC as a ‘model class’–the space of all possible deﬁnable mod els.\nHowever, much less attention has been directed to actually d eﬁning the RDCs\nin which models are speciﬁed: existing approaches assume th ere is some chosen\nRDC and morphism, treating both essentially as a black box. I n this paper,\nwe focus on classes of RDCs which we call ‘polynomial circuit s’, which may be\nthought of as a more expressive version of the boolean circui ts of Lafont [17],\nwith wires carrying values from an arbitrary semiring inste ad ofZ2. Because we\nensure polynomial circuits have RDC structure, they are sui table as machine\n",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "51": {
    "pdf_path": "data/pdfs\\machine learning_paper_145.pdf",
    "text_excerpt": "arXiv:2203.16797v1  [cs.LG]  31 Mar 2022WHEN PHYSICS MEETS MACHINE LEARNING : A S URVEY OF\nPHYSICS -INFORMED MACHINE LEARNING\nA P REPRINT\nChuizheng Meng, Sungyong Seo, Defu Cao, Sam Griesemer, Yan L iu\nDepartment of Computer Science\nUniversity of Southern California\nLos Angeles, CA 90089\n{chuizhem, sungyongs, defucao, samgriesemer, yanliu.cs} @usc.edu\nApril 1, 2022\nABSTRACT\nPhysics-informed machine learning (PIML), referring to th e combination of prior knowledge of\nphysics, which is the high level abstraction of natural phen omenons and human behaviours in the\nlong history, with data-driven machine learning models, ha s emerged as an effective way to mitigate\nthe shortage of training data, to increase models’ generali zability and to ensure the physical plausibil-\nity of results. In this paper, we survey an abundant number of recent works in PIML and summarize\nthem from three aspects: (1) motivations of PIML, (2) physic s knowledge in PIML, (3) methods\nof physics knowledge integration in PIML. We also discuss cu rrent challenges and corresponding\nresearch opportunities in PIML.\n1 Introduction\nMachine learning/deep learning models have already achiev ed tremendous success in a number of domains such as\ncomputer vision [ 1,2,3,4,5] and natural language processing [ 6,7,8,9,10,11,12,13,14], where large amounts\nof training data and highly expressive neural network archi tectures together give birth to solutions outperforming\npreviously dominating methods. As a consequence, research ers have also started exploring the possibility of applying\nmachine learning models to advance scientiﬁc discovery and to further improve traditional analytical modeling [ 15,\n16,17,18,19,20,21].\nWhile given a set of input and output pairs, deep neural netwo rks are able to extract the complicated relations between\nthe input and output via appropriate optimization over adeq uate large amount of data, prior knowledge still acts as an\nimportant role in ﬁnding the optimal solution. As the high ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "52": {
    "pdf_path": "data/pdfs\\machine learning_paper_146.pdf",
    "text_excerpt": "The Stata Journal ( yyyy ) vv, Number ii, pp. 1{20\npystacked: Stacking generalization and machine\nlearning in Stata\nAchim Ahrens\nETH Z urich\nachim.ahrens@gess.ethz.chChristian B. Hansen\nUniversity of Chicago\nchristian.hansen@chicagobooth.edu\nMark E. Scha\u000ber\nHeriot-Watt University\nEdinburgh, United Kingdom\nm.e.scha\u000ber@hw.ac.uk\nAbstract. pystacked implements stacked generalization (Wolpert, 1992) for\nregression and binary classi\fcation via Python's scikit-learn . Stacking combines\nmultiple supervised machine learners|the \\base\" or \\level-0\" learners|into a sin-\ngle learner. The currently supported base learners include regularized regression,\nrandom forest, gradient boosted trees, support vector machines, and feed-forward\nneural nets (multi-layer perceptron). pystacked can also be used as a `regular'\nmachine learning program to \ft a single base learner and, thus, provides an easy-\nto-use API for scikit-learn 's machine learning algorithms.\nKeywords: st0001, machine learning, stacked generalization, model averaging,\nStata, Python, sci-kit learn\n1 Introduction\nWhen faced with a new prediction or classi\fcation task, it is a priori rarely obvious\nwhich machine learning algorithm is best suited. A common approach is to evaluate\nthe performance of a set of machine learners on a hold-out partition of the data or\nvia cross-validation and then select the machine learner that minimizes a chosen loss\nmetric. However, this approach is incomplete as combining multiple learners into one\n\fnal prediction might lead to superior performance compared to each individual learner.\nThis possibility motivates stacked generalization, or simply stacking , due to Wolpert\n(1992) and Breiman (1996). Stacking is a form of model averaging. Theoretical results\nin van der Laan et al. (2007) support the use of stacking as it performs asymptotically\nat least as well as the best-performing individual learner as long as the number of base\nlearners is not too large.\nIn this article, we introduce pystacke",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "53": {
    "pdf_path": "data/pdfs\\machine learning_paper_147.pdf",
    "text_excerpt": "Nine tips for ecologists using machine learning\nMarine Desprez1, Vincent Miele2,*, and Olivier Gimenez1\n1CEFE, Univ Montpellier, CNRS, EPHE, IRD, Montpellier, France\n2Universit´ e de Lyon, F-69000 Lyon; Universit´ e Lyon 1; CNRS, UMR5558,\nLaboratoire de Biom´ etrie et Biologie ´Evolutive, F-69622 Villeurbanne, France\n*Corresponding author: Vincent.Miele@univ-lyon1.fr\nAbstract\nDue to their high predictive performance and flexibility, machine learning models are an\nappropriate and efficient tool for ecologists. However, implementing a machine learning model\nis not yet a trivial task and may seem intimidating to ecologists with no previous experience in\nthis area. Here we provide a series of tips to help ecologists in implementing machine learning\nmodels. We focus on classification problems as many ecological studies aim to assign data\ninto predefined classes such as ecological states or biological entities. Each of the nine tips\nidentifies a common error, trap or challenge in developing machine learning models and provides\nrecommendations to facilitate their use in ecological studies.\nIntroduction\nEcological datasets are generally characterised by complex interactions between variables, non-\nlinearity, missing values, dependence in the observations and/or a continuously expanding size [1 –3],\nespecially since the recent increase in the use of remote sensing and automatic recorders [4]. A\ngrowing number of those datasets cannot be effectively processed by humans anymore and require\nmethods that can deal with high number of variables and complex data structures [3,5,6]. Because\nof their ability to process large and complicated datasets, machine learning models are expected to\nbecome a standard framework in the analysis of ecological data [3, 7, 8]. Over the last few years,\nmachine learning algorithms have become increasingly popular due to their high performance and\nflexibility [8]. In ecology, they have been successfully applied to perform various tasks such as\nidentif",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "54": {
    "pdf_path": "data/pdfs\\machine learning_paper_148.pdf",
    "text_excerpt": "A Comparison of Machine Learning Methods for\nData with High-Cardinality Categorical\nVariables\nFabio Sigrist∗\nLucerne University of Applied Sciences and Arts\nJuly 6, 2023\nAbstract\nHigh-cardinality categorical variables are variables for which the number of different lev-\nels is large relative to the sample size of a data set, or in other words, there are few data\npoints per level. Machine learning methods can have difficulties with high-cardinality vari-\nables. In this article, we empirically compare several versions of two of the most successful\nmachine learning methods, tree-boosting and deep neural networks, and linear mixed effects\nmodels using multiple tabular data sets with high-cardinality categorical variables. We find\nthat, first, machine learning models with random effects have higher prediction accuracy\nthan their classical counterparts without random effects, and, second, tree-boosting with\nrandom effects outperforms deep neural networks with random effects.\nKeywords: Mixed effects machine learning, random effects, high-cardinality categorical variables,\ntree-boosting, deep neural networks\n1 Introduction\nHigh-cardinality categorical variables are variables for which the number of different levels is\nlarge relative to the sample size of a data set or, equivalently, there is little data per level. High-\ncardinality categorical variables can pose difficulties for machine learning methods such as deep\nneural networks and tree-based models.\nA simple strategy for dealing with categorical variables is to use one-hot encoding or dummy\nvariables. But this approach often does not work well for high-cardinality categorical variables\ndue to the reasons described below. For neural networks, a frequently adopted solution is to use\nentity embeddings [Guo and Berkhahn, 2016] that map every level of a categorical variable into a\nlow-dimensional Euclidean space. For tree-boosting, an alternative to one-hot encoding is to as-\nsign a number to every level of a categorical va",
    "title": "The Shapley Value in Machine Learning",
    "abstract": "Over the last few years, the Shapley value, a solution concept from cooperative game theory, has found numerous applications in machine learning. In this paper, we first discuss fundamental concepts of cooperative game theory and axiomatic properties of the Shapley value. Then we give an overview of the most important applications of the Shapley value in machine learning: feature selection, explainability, multi-agent reinforcement learning, ensemble pruning, and data valuation. We examine the most crucial limitations of the Shapley value and point out directions for future research.",
    "link": "https://www.semanticscholar.org/paper/09c72d9d46f6750e487afdb5f7cae7693ffccc10",
    "published": "2022-02-11"
  },
  "55": {
    "pdf_path": "data/pdfs\\machine learning_paper_149.pdf",
    "text_excerpt": "Machine learning for accuracy in density functional\napproximations\nJohannes Voss∗†\nNovember 2, 2023\nAbstract\nMachine learning techniques have found their way into computational chemistry\nas indispensable tools to accelerate atomistic simulations and materials design. In\naddition, machine learning approaches hold the potential to boost the predictive power\nof computationally efficient electronic structure methods, such as density functional\ntheory, to chemical accuracy and to correct for fundamental errors in density functional\napproaches. Here, recent progress in applying machine learning to improve the accuracy\nof density functional and related approximations is reviewed. Promises and challenges\nin devising machine learning models transferable between different chemistries and\nmaterials classes are discussed with the help of examples applying promising models\nto systems far outside their training sets.\nKeywords: Machine learning, density functional theory, materials prediction,\nexchange-correlation functional, self-interaction, electron delocalization\n∗SUNCAT Center for Interface Science and Catalysis, SLAC National Accelerator Laboratory, 2575 Sand\nHill Road, Menlo Park, CA 94025, USA\n†vossj@slac.stanford.edu\n1arXiv:2311.00196v1  [physics.chem-ph]  1 Nov 2023Machine learning techniques allow us, where benchmark data are available, to train elec-\ntronic structure models that substantially increase the predictive power of density functional\ntheory simulations of chemical reactions and structural and thermodynamic properties of\ngas, liquid, and solid phases. Not only can quantitative improvements be achieved, but also\nfundamental limitations of density functional approximations can be corrected for. Here,\ntechniques, benchmark data, and challenges for devising transferable electronic structure\nmachine learning models are reviewed.\n2Charge density functional-basedAtomic structure-based\nMachine learning approaches to improving electronic structure predictions\nNeural ne",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "56": {
    "pdf_path": "data/pdfs\\machine learning_paper_15.pdf",
    "text_excerpt": "The Landscape of Modern Machine Learning: A Review of\nMachine, Distributed and Federated Learning\nOmer Subasi1*, Oceane Bel1, Joseph Manzano1, Kevin Barker1\n1*High Performance Computing Group, Pacific Northwest National Laboratory, 902\nBattelle Blvd, Richland, 99354, WA, USA.\n*Corresponding author(s). E-mail(s): omer.subasi@pnnl.gov;\nContributing authors: obel@pnnl.gov; joseph.manzano@pnnl.gov; kevin.barker@pnnl.gov;\nAbstract\nWith the advance of the powerful heterogeneous, parallel and distributed computing systems and ever\nincreasing immense amount of data, machine learning has become an indispensable part of cutting-\nedge technology, scientific research and consumer products. In this study, we present a review of\nmodern machine and deep learning. We provide a high-level overview for the latest advanced machine\nlearning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed\nlearning, deep learning as well as federated learning. As a result, our work serves as an introductory\ntext to the vast field of modern machine learning.\nKeywords: Machine Learning, Distributed Machine Learning, Deep Learning, Federated Learning, Parallel\nand Distributed Computing.\n1 Introduction\nOver the last decade, Machine Learning (ML) has\nbeen applied to ever increasing immense amount\nof data that is becoming available as more people\nbecome daily users of internet, mobile and wireless\nnetworks. Coupled with the significant advances in\ndeep learning (DL), ML has found more complex\napplications: from medical to machine transla-\ntion and speech recognition, to intelligent object\nrecognition, and to smart cities [1, 2]. Modern\nparallel and heterogeneous computing systems\n[3, 4, 5] have enabled such applications by sup-\nporting highly parallel training. These large-scale\nand distributed systems therefore have become the\nbackbone of modern ML [6, 7, 8].\nFederated Learning (FL), as a sub-field of DL,\nhas emerged as a distributed learning solution toprovide data p",
    "title": "The Landscape of Modern Machine Learning: A Review of Machine,\n  Distributed and Federated Learning",
    "abstract": "With the advance of the powerful heterogeneous, parallel and distributed\ncomputing systems and ever increasing immense amount of data, machine learning\nhas become an indispensable part of cutting-edge technology, scientific\nresearch and consumer products. In this study, we present a review of modern\nmachine and deep learning. We provide a high-level overview for the latest\nadvanced machine learning algorithms, applications, and frameworks. Our\ndiscussion encompasses parallel distributed learning, deep learning as well as\nfederated learning. As a result, our work serves as an introductory text to the\nvast field of modern machine learning.",
    "link": "http://arxiv.org/abs/2312.03120v1",
    "published": "2023-12-05T20:40:05Z"
  },
  "57": {
    "pdf_path": "data/pdfs\\machine learning_paper_150.pdf",
    "text_excerpt": "Improving Radiography Machine Learning Workflows via Metadata\nManagement for Training Data Selection\nMirabel Reid\nmreid48@gatech.edu\nGeorgia Tech, LANL∗Christine Sweeney\ncahrens@lanl.gov\nLANLOleg Korobkin\nkorobkin@lanl.gov\nLANL\nAugust 26, 2024\nAbstract\nMost machine learning models require many iterations of hyper-parameter tuning, feature\nengineering, and debugging to produce effective results. As machine learning models become\nmore complicated, this pipeline becomes more difficult to manage effectively. In the physical\nsciences, there is an ever-increasing pool of metadata that is generated by the scientific research\ncycle. Tracking this metadata can reduce redundant work, improve reproducibility, and aid in\nthe feature and training dataset engineering process. In this case study, we present a tool for\nmachine learning metadata management in dynamic radiography. We evaluate the efficacy of\nthis tool against the initial research workflow and discuss extensions to general machine learning\npipelines in the physical sciences.1\n1 Introduction\nMachine learning (ML) is an increasingly integral part of scientific research. In the natural sci-\nences, ML can recognize patterns in observational data and inform the development of scientific\ntheories [Ros+20]. In applications such as materials science, an effective machine learning model\ncan dramatically reduce the need for costly experiments and long development cycles [Wei+19;\nCar+19]. It is undeniable that ML can be an effective method for interpreting large volumes of\nobserved or simulated data.\nHowever, the introduction of a complex method like ML comes with challenges. Outside of\nsimple models, most ML models require many iterations of hyper-parameter tuning, feature engi-\nneering, and debugging to produce effective results. Because of this, the workflow is often packaged\ninto a pipeline: an automated or semi-automated program which prepares the data, trains, and eval-\nuates the model in one swoop. As machine learning mod",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "58": {
    "pdf_path": "data/pdfs\\machine learning_paper_155.pdf",
    "text_excerpt": "A Survey on Bias and Fairness in Machine Learning\nNINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA,\nKRISTINA LERMAN, and ARAM GALSTYAN, USC-ISI\nWith the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting\nfor fairness has gained significant importance in designing and engineering of such systems. AI systems can be\nused in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure\nthat these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently\nsome work has been developed in traditional machine learning and deep learning that address such challenges in\ndifferent subdomains. With the commercialization of these systems, researchers are becoming more aware of\nthe biases that these applications can contain and are attempting to address them. In this survey we investigated\ndifferent real-world applications that have shown biases in various ways, and we listed different sources of\nbiases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning\nresearchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined\ndifferent domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes\nin the state-of-the-art methods and ways they have tried to address them. There are still many future directions\nand solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will\nmotivate researchers to tackle these issues in the near future by observing existing work in their respective fields.\nCCS Concepts: •Computing methodologies →Artificial intelligence ;Philosophical/theoretical founda-\ntions of artificial intelligence ;\nAdditional Key Words and Phrases: Fairness and Bias in Artificial Intelligence, Machine Learning, Deep\nLearning, Natural Language Processing, Representa",
    "title": "Empirical Asset Pricing Via Machine Learning",
    "abstract": "\n We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best-performing methods (trees and neural networks) and trace their predictive gains to allowing nonlinear predictor interactions missed by other methods. All methods agree on the same set of dominant predictive signals, a set that includes variations on momentum, liquidity, and volatility.\n Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.",
    "link": "https://www.semanticscholar.org/paper/caf9e0fa2c340fb07cef8d547ea8849508e5c358",
    "published": "2018-12-01"
  },
  "59": {
    "pdf_path": "data/pdfs\\machine learning_paper_157.pdf",
    "text_excerpt": "Membership Inference Attacks Against\nMachine Learning Models\nReza Shokri\nCornell Tech\nshokri@cornell.eduMarco Stronati\u0003\nINRIA\nmarco@stronati.orgCongzheng Song\nCornell\ncs2296@cornell.eduVitaly Shmatikov\nCornell Tech\nshmat@cs.cornell.edu\nAbstract —We quantitatively investigate how machine learning\nmodels leak information about the individual data records on\nwhich they were trained. We focus on the basic membership\ninference attack: given a data record and black-box access to\na model, determine if the record was in the model’s training\ndataset. To perform membership inference against a target model,\nwe make adversarial use of machine learning and train our own\ninference model to recognize differences in the target model’s\npredictions on the inputs that it trained on versus the inputs\nthat it did not train on.\nWe empirically evaluate our inference techniques on classi-\nﬁcation models trained by commercial “machine learning as a\nservice” providers such as Google and Amazon. Using realistic\ndatasets and classiﬁcation tasks, including a hospital discharge\ndataset whose membership is sensitive from the privacy perspec-\ntive, we show that these models can be vulnerable to membership\ninference attacks. We then investigate the factors that inﬂuence\nthis leakage and evaluate mitigation strategies.\nI. I NTRODUCTION\nMachine learning is the foundation of popular Internet\nservices such as image and speech recognition and natural lan-\nguage translation. Many companies also use machine learning\ninternally, to improve marketing and advertising, recommend\nproducts and services to users, or better understand the data\ngenerated by their operations. In all of these scenarios, ac-\ntivities of individual users—their purchases and preferences,\nhealth data, online and ofﬂine transactions, photos they take,\ncommands they speak into their mobile phones, locations they\ntravel to—are used as the training data.\nInternet giants such as Google and Amazon are already\noffering “machine learning as a s",
    "title": "Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods",
    "abstract": "The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.",
    "link": "https://www.semanticscholar.org/paper/b631ba962b4403a9c0fd9cce446ef3b1e21ea059",
    "published": "2019-10-21"
  },
  "60": {
    "pdf_path": "data/pdfs\\machine learning_paper_167.pdf",
    "text_excerpt": "Practical Black-Box Attacks against Machine Learning\nNicolas Papernot\nPennsylvania State University\nngp5056@cse.psu.eduPatrick McDaniel\nPennsylvania State University\nmcdaniel@cse.psu.eduIan Goodfellow\u0003\nOpenAI\nian@openai.com\nSomesh Jha\nUniversity of Wisconsin\njha@cs.wisc.eduZ. Berkay Celik\nPennsylvania State University\nzbc102@cse.psu.eduAnanthram Swami\nUS Army Research Laboratory\nananthram.swami.civ@mail.mil\nABSTRACT\nMachine learning (ML) models, e.g., deep neural networks\n(DNNs), are vulnerable to adversarial examples: malicious\ninputs modi\fed to yield erroneous model outputs, while ap-\npearing unmodi\fed to human observers. Potential attacks\ninclude having malicious content like malware identi\fed as\nlegitimate or controlling vehicle behavior. Yet, all existing\nadversarial example attacks require knowledge of either the\nmodel internals or its training data. We introduce the \frst\npractical demonstration of an attacker controlling a remotely\nhosted DNN with no such knowledge. Indeed, the only capa-\nbility of our black-box adversary is to observe labels given\nby the DNN to chosen inputs. Our attack strategy consists\nin training a local model to substitute for the target DNN,\nusing inputs synthetically generated by an adversary and\nlabeled by the target DNN. We use the local substitute to\ncraft adversarial examples, and \fnd that they are misclas-\nsi\fed by the targeted DNN. To perform a real-world and\nproperly-blinded evaluation, we attack a DNN hosted by\nMetaMind, an online deep learning API. We \fnd that their\nDNN misclassi\fes 84.24% of the adversarial examples crafted\nwith our substitute. We demonstrate the general applicabil-\nity of our strategy to many ML techniques by conducting the\nsame attack against models hosted by Amazon and Google,\nusing logistic regression substitutes. They yield adversarial\nexamples misclassi\fed by Amazon and Google at rates of\n96.19% and 88.94%. We also \fnd that this black-box attack\nstrategy is capable of evading defense strategies previous",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "61": {
    "pdf_path": "data/pdfs\\machine learning_paper_17.pdf",
    "text_excerpt": "ICML 2015 AutoML Workshop\nAutoCompete: A Framework for Machine Learning\nCompetitions\nAbhishek Thakur thakur@aisbi.de andArtus Krohn-Grimberghe artus@aisbi.de\nAISBI, University of Paderborn, Germany\nAbstract\nIn this paper, we propose AutoCompete, a highly automated machine learning framework\nfor tackling machine learning competitions. This framework has been learned by us, vali-\ndated and improved over a period of more than two years by participating in online machine\nlearning competitions. It aims at minimizing human interference required to build a \frst\nuseful predictive model and to assess the practical di\u000eculty of a given machine learning\nchallenge. The proposed system helps in identifying data types, choosing a machine learn-\ning model, tuning hyper-parameters, avoiding over-\ftting and optimization for a provided\nevaluation metric. We also observe that the proposed system produces better (or compa-\nrable) results with less runtime as compared to other approaches.\nKeywords: auto-machine learning, predictive modelling\n1. Introduction\nIn the industry, business analysts are usually not concerned with the algorithms, feature\nselection, feature engineering or selection of appropriate hyperparameters. All they want\nis a fast track to a highly accurate predictive model which they can apply with minimum\nknowledge and e\u000bort on their problems and datasets. To satisfy this need, many \"one-click\"\nmachine learning platforms have emerged that speci\fcally target those users. Platforms such\nas Google Predict and BigML take the dataset as input from the end user and provide them\nwith a predictive model for the dataset and a web service to consume it but that is beyond\nthe scope of this paper.\nIn machine learning research, this topic has arrived under the umbrella term AutoML\nthat subsumes and integrates disjunct areas of research such as identi\fcation of the problem\n(classi\fcation/regression, identifying the type of data, types of features and selection of\nfeatures). Besides conn",
    "title": "AutoCompete: A Framework for Machine Learning Competition",
    "abstract": "In this paper, we propose AutoCompete, a highly automated machine learning\nframework for tackling machine learning competitions. This framework has been\nlearned by us, validated and improved over a period of more than two years by\nparticipating in online machine learning competitions. It aims at minimizing\nhuman interference required to build a first useful predictive model and to\nassess the practical difficulty of a given machine learning challenge. The\nproposed system helps in identifying data types, choosing a machine learn- ing\nmodel, tuning hyper-parameters, avoiding over-fitting and optimization for a\nprovided evaluation metric. We also observe that the proposed system produces\nbetter (or comparable) results with less runtime as compared to other\napproaches.",
    "link": "http://arxiv.org/abs/1507.02188v1",
    "published": "2015-07-08T15:07:39Z"
  },
  "62": {
    "pdf_path": "data/pdfs\\machine learning_paper_171.pdf",
    "text_excerpt": " \n POST NOTE  \nThe Parliamentary Office of Science and Technology, Westminster, London SW1A 0AA  \n02072192840 post@parliament.uk  post.parliament.uk  @POST_UK  Number 633 October 2020  \nInterpretable machine learning  \n \nThis POSTnote gives a n overview of  machine \nlearning  (ML) and its role  in decision -making . It \nexamines the challenges  of understanding how \na complex ML system has reached its output , \nand some of the  technical approaches to  \nmaking ML easier to interpret . It gives a  brief \noverview of  some of the proposed tools for \nmaking  ML systems more accountable , such as \nalgorithm audit and impact assessments.   Overview   \n◼ Machine learning (ML) is being used to \nsupport decision -making in applications such \nas recruitment and medical dia gnoses.  \n◼ Concerns have been raised about some \ncomplex types of ML , where  it is difficult  to \nunderstand how a decision has been made . \n◼ A further risk is the potential for ML system s \nto introduce or perpetuate biases.   \n◼ Approaches to improving the interpretabi lity \nof ML include designing systems using \nsimpler methods and using tools to gain an \ninsight  into how complex systems function.  \n◼ Interpretable ML can improve user trust and  \nML performance, however there are \nchallenges such as  commercial sensitivity . \n◼ Proposed  ways to improve ML accountability \ninclude auditing and impact assessments.  \n \nBackground  \nMachine learning  (ML), a type of artificial intelligence (AI, Box \n1), is increasingly being used for a variety of applications from \nverifying a person’s identity based on their voice to diagnosing \ndisease. ML has the potential to bring many social and \neconomic benefits , including increased labour produc tivity and \nimprove d services across a wide range of sectors.1–3  \nHowever, there are concerns that decisions that are made or \ninformed by the outputs of ML can lack transparency  and \naccountability. This can be a particular  issue for certain  types of \nML (such as ",
    "title": "Machine learning and the physical sciences",
    "abstract": "Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.",
    "link": "https://www.semanticscholar.org/paper/a9cbbef8f4426329d0687025b34287c35bdd8b38",
    "published": "2019-03-25"
  },
  "63": {
    "pdf_path": "data/pdfs\\machine learning_paper_172.pdf",
    "text_excerpt": "Optimization Methods for Large-Scale Machine Learning\nL\u0013 eon Bottou\u0003Frank E. CurtisyJorge Nocedalz\nFebruary 12, 2018\nAbstract\nThis paper provides a review and commentary on the past, present, and future of numerical\noptimization algorithms in the context of machine learning applications. Through case studies\non text classi\fcation and the training of deep neural networks, we discuss how optimization\nproblems arise in machine learning and what makes them challenging. A major theme of our\nstudy is that large-scale machine learning represents a distinctive setting in which the stochastic\ngradient (SG) method has traditionally played a central role while conventional gradient-based\nnonlinear optimization techniques typically falter. Based on this viewpoint, we present a com-\nprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior,\nand highlight opportunities for designing algorithms with improved performance. This leads to\na discussion about the next generation of optimization methods for large-scale machine learning,\nincluding an investigation of two main streams of research on techniques that diminish noise in\nthe stochastic directions and methods that make use of second-order derivative approximations.\nContents\n1 Introduction 3\n2 Machine Learning Case Studies 4\n2.1 Text Classi\fcation via Convex Optimization . . . . . . . . . . . . . . . . . . . . . . . 4\n2.2 Perceptual Tasks via Deep Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 6\n2.3 Formal Machine Learning Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3 Overview of Optimization Methods 13\n3.1 Formal Optimization Problem Statements . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.2 Stochastic vs. Batch Optimization Methods . . . . . . . . . . . . . . . . . . . . . . . 15\n3.3 Motivation for Stochastic Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.4 Beyond SG: Noise Reduction and Second-Order Methods . . . . . . . ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "64": {
    "pdf_path": "data/pdfs\\machine learning_paper_177.pdf",
    "text_excerpt": "On Hyperparameter Optimization of Machine Learning\nAlgorithms: Theory and Practice\nLi Yang and Abdallah Shami\nDepartment of Electrical and Computer Engineering, University of Western Ontario,\n1151 Richmond St, London, Ontario, Canada N6A 3K7\nAbstract\nMachine learning algorithms have been used widely in various applications\nand areas. To \ft a machine learning model into di\u000berent problems, its hyper-\nparameters must be tuned. Selecting the best hyper-parameter con\fguration\nfor machine learning models has a direct impact on the model's performance.\nIt often requires deep knowledge of machine learning algorithms and appro-\npriate hyper-parameter optimization techniques. Although several automatic\noptimization techniques exist, they have di\u000berent strengths and drawbacks\nwhen applied to di\u000berent types of problems. In this paper, optimizing the\nhyper-parameters of common machine learning models is studied. We in-\ntroduce several state-of-the-art optimization techniques and discuss how to\napply them to machine learning algorithms. Many available libraries and\nframeworks developed for hyper-parameter optimization problems are pro-\nvided, and some open challenges of hyper-parameter optimization research\nare also discussed in this paper. Moreover, experiments are conducted on\nbenchmark datasets to compare the performance of di\u000berent optimization\nmethods and provide practical examples of hyper-parameter optimization.\nThis survey paper will help industrial users, data analysts, and researchers\nto better develop machine learning models by identifying the proper hyper-\nparameter con\fgurations e\u000bectively.1\nKeywords: Hyper-parameter optimization, machine learning, Bayesian\nEmail address: {lyang339, abdallah.shami}@uwo.ca (Li Yang and Abdallah\nShami)\n1General Hyperparameter Optimization Code and Tutorials: https://github.com/\nLiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms\nPreprint submitted to Neurocomputing October 6, 2022arXiv:2007.15745v3  [cs.LG]  5 Oct 202",
    "title": "Perspectives in machine learning for wildlife conservation",
    "abstract": "Inexpensive and accessible sensors are accelerating data acquisition in animal ecology. These technologies hold great potential for large-scale ecological understanding, but are limited by current processing approaches which inefficiently distill data into relevant information. We argue that animal ecologists can capitalize on large datasets generated by modern sensors by combining machine learning approaches with domain knowledge. Incorporating machine learning into ecological workflows could improve inputs for ecological models and lead to integrated hybrid modeling tools. This approach will require close interdisciplinary collaboration to ensure the quality of novel approaches and train a new generation of data scientists in ecology and conservation. Animal ecologists are increasingly limited by constraints in data processing. Here, Tuia and colleagues discuss how collaboration between ecologists and data scientists can harness machine learning to capitalize on the data generated from technological advances and lead to novel modeling approaches.",
    "link": "https://www.semanticscholar.org/paper/d9b34c6b616f75485856794478bfbeab1ea93b81",
    "published": "2021-10-25"
  },
  "65": {
    "pdf_path": "data/pdfs\\machine learning_paper_178.pdf",
    "text_excerpt": "ilastik: interactive machine learning for\n(bio)image analysis\nStuart Berg1, Dominik Kutra2,3, Thorben Kroeger2, Christoph N. Straehle2, Bernhard X. Kausler2, Carsten Haubold2, Martin\nSchiegg2, Janez Ales2, Thorsten Beier2, Markus Rudy2, Kemal Eren2, Jaime I Cervantes2, Buote Xu2, Fynn Beuttenmueller2,3,\nAdrian Wolny2, Chong Zhang2, Ullrich Koethe2, Fred A. Hamprecht2, \u0000, and Anna Kreshuk2,3, \u0000\n1HHMI Janelia Research Campus, Ashburn, Virginia, USA\n2HCI/IWR, Heidelberg University, Heidelberg, Germany\n3European Molecular Biology Laboratory, Heidelberg, Germany\nWe present ilastik, an easy-to-use interactive tool that brings\nmachine-learning-based (bio)image analysis to end users with-\nout substantial computational expertise. It contains pre-deﬁned\nworkﬂows for image segmentation, object classiﬁcation, count-\ning and tracking. Users adapt the workﬂows to the problem at\nhand by interactively providing sparse training annotations for\na nonlinear classiﬁer. ilastik can process data in up to ﬁve di-\nmensions (3D, time and number of channels). Its computational\nback end runs operations on-demand wherever possible, allow-\ning for interactive prediction on data larger than RAM. Once\nthe classiﬁers are trained, ilastik workﬂows can be applied to\nnew data from the command line without further user interac-\ntion. We describe all ilastik workﬂows in detail, including three\ncase studies and a discussion on the expected performance.\nmachine learning | image analysis | software\nCorrespondence:\nfred.hamprecht@iwr.uni-heidelberg.de\nanna.kreshuk@embl.de\nMain\nRapid development of imaging technology is bringing more\nand more life scientists to experimental pipelines where the\nsuccess of the entire undertaking hinges on the analysis of\nimages. Image segmentation, object tracking and counting\nare time consuming, tedious and error-prone processes when\nperformed manually. Besides, manual annotation is hard to\nscale for biological images, since expert annotators are typ-\nically required for cor",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "66": {
    "pdf_path": "data/pdfs\\machine learning_paper_18.pdf",
    "text_excerpt": "arXiv:1212.2686v1  [stat.ML]  12 Dec 2012Joint Training of Deep Boltzmann Machines for Classiﬁcatio n\nIan J. Goodfellow Aaron Courville Yoshua Bengio\nUniversit´ e de Montr´ eal\nAbstract\nWe introduce a new method for training deep\nBoltzmann machines jointly. Prior methods\nrequireaninitial learningpassthat trains the\ndeep Boltzmann machine greedily, one layer\nat a time, or do not perform well on classiﬁ-\ncation tasks.\n1 Deep Boltzmann machines\nA deep Boltzmann machine\n(Salakhutdinov and Hinton, 2009) is a probabilistic\nmodel consisting of many layers of random variables,\nmost of which are latent. Typically, a DBM contains\na set ofDinput features vthat are called the visible\nunitsbecause they are always observed during both\ntraining and evaluation. The DBM is usually applied\nto classiﬁcation problems and thus often represents\nthe class label with a one-of- kcode in the form of\na discrete-valued label unit y.yis observed (on\nexamples for which it is available) during training.\nThe DBM also contains several hidden units, which\nare usually organized into Llayersh(i)of size\nNi,i= 1,...,L,with each unit in a layer conditionally\nindependent of the other units in the layer given the\nneighboring layers. These conditional independence\nproperties allow fast Gibbs sampling because an entire\nlayer of units can be sampled at a time. Likewise,\nmean ﬁeld inference with ﬁxed point equations is fast\nbecause each ﬁxed point equation gives a solution to\nan entire layer of variational parameters.\nA DBM deﬁnes a probability distribution by exponen-\ntiating and normalizing an energy function\nP(v,h,y) =1\nZexp(−E(v,h,y))\nPreliminary work presented to Bruno Olshausen’s lab and\nGoogle Brain, December 2012.where\nZ=/summationdisplay\nv′,h′,y′exp(−E(v′,h′,y′)).\nZ, the partition function, is intractable, due to the\nsummation over all possible states. Maximum like-\nlihood learning requires computing the gradient of\nlogZ. Fortunately, the gradient can be estimated us-\ning an MCMC procedure (Younes, 19",
    "title": "Joint Training of Deep Boltzmann Machines",
    "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior\nmethods require an initial learning pass that trains the deep Boltzmann machine\ngreedily, one layer at a time, or do not perform well on classifi- cation\ntasks.",
    "link": "http://arxiv.org/abs/1212.2686v1",
    "published": "2012-12-12T01:59:27Z"
  },
  "67": {
    "pdf_path": "data/pdfs\\machine learning_paper_180.pdf",
    "text_excerpt": "RESEA RCH ARTICL E\nSoilGrids250m: Global gridded soil\ninformation based onmachine learning\nTomislav Hengl1*,Jorge Mendes deJesus1,Gerard B.M.Heuvelink1,Maria Ruiperez\nGonzalez1,Milan Kilibarda2,Aleksandar Blagotić3,Wei Shangguan4,Marvin N.Wright5,\nXiaoyuan Geng6,Bernhard Bauer-Marschallinger7,Mario Antonio Guevara8,\nRodrigo Vargas8,Robert A.MacMillan9,Niels H.Batjes1,Johan G.B.Leenaars1,\nEloi Ribeiro1,Ichsani Wheeler10,Stephan Mantel1,Bas Kempen1\n1ISRIC ÐWorld SoilInformation, Wageningen, theNetherlands ,2Faculty ofCivil Engineeri ng,University of\nBelgrade, Belgrade, Serbia, 3GILab Ltd,Belgrade, Serbia, 4School ofAtmospher icSciences, Sun Yat-sen\nUniversity ,Guangzhou, China, 5Institut fuÈrMedizinisc heBiometrie undStatistik, LuÈbeck, Germa ny,\n6Agriculture andAgri-Food Canada, Ottawa (Ontario), Canada, 7Departm entofGeodesy and\nGeoinform ation, Vienna University ofTechnology ,Vienna, Austria, 8University ofDelawa re,Newar k(DE),\nUnited States ofAmerica, 9LandMapper Environm ental Solutions Inc., Edmonto n(Alberta) ,Canada,\n10Envirome trixInc., Wagenin gen, theNetherlan ds\n*tom.hengl @isric.org\nAbstract\nThis paper describes thetechnical development and accuracy assessment ofthemost\nrecent and improved version oftheSoilGrids system at250m resolution (June 2016\nupdate). SoilGrids provides global predictions forstandard numeric soilproperties\n(organic carbon, bulk density, Cation Exchange Capacity (CEC), pH,soiltexture fractions\nand coarse fragments) atseven standard depths (0,5,15,30,60,100 and 200 cm), in\naddition topredictions ofdepth tobedrock and distribution ofsoilclasses based onthe\nWorld Reference Base (WRB) and USDA classification systems (ca. 280 raster layers in\ntotal). Predictions were based onca.150,000 soilprofiles used fortraining and astack of\n158 remote sensing-ba sedsoilcovariates (primarily derived from MODIS land products,\nSRTM DEM derivatives, climatic images and global landform and lithology maps), which\nwere used tofitanensemble ofmachine learnin",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "68": {
    "pdf_path": "data/pdfs\\machine learning_paper_185.pdf",
    "text_excerpt": "1\nMultimodal Machine Learning:\nA Survey and Taxonomy\nTadas Baltru ˇsaitis, Chaitanya Ahuja, and Louis-Philippe Morency\nAbstract —Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste ﬂavors.\nModality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when\nit includes multiple such modalities. In order for Artiﬁcial Intelligence to make progress in understanding the world around us, it needs\nto be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate\ninformation from multiple modalities. It is a vibrant multi-disciplinary ﬁeld of increasing importance and with extraordinary potential.\nInstead of focusing on speciﬁc multimodal applications, this paper surveys the recent advances in multimodal machine learning itself\nand presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader\nchallenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This\nnew taxonomy will enable researchers to better understand the state of the ﬁeld and identify directions for future research.\nIndex Terms —Multimodal, machine learning, introductory, survey.\nF\n1 I NTRODUCTION\nTHEworld surrounding us involves multiple modalities\n— we see objects, hear sounds, feel texture, smell odors,\nand so on. In general terms, a modality refers to the way in\nwhich something happens or is experienced. Most people\nassociate the word modality with the sensory modalities\nwhich represent our primary channels of communication\nand sensation, such as vision or touch. A research problem\nor dataset is therefore characterized as multimodal when it\nincludes multiple such modalities. In this paper we focus\nprimarily, but not exclusively, on three modalities: natural\nlanguage which can be both written or",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "69": {
    "pdf_path": "data/pdfs\\machine learning_paper_188.pdf",
    "text_excerpt": "Large-Scale Machine Learning\nwith Stochastic Gradient Descent\nL\u0013 eon Bottou\nNEC Labs America, Princeton NJ 08542, USA\nleon@bottou.org\nAbstract. During the last decade, the data sizes have grown faster than the speed\nof processors. In this context, the capabilities of statistical machine learning meth-\nods is limited by the computing time rather than the sample size. A more pre-\ncise analysis uncovers qualitatively di\u000berent tradeo\u000bs for the case of small-scale\nand large-scale learning problems. The large-scale case involves the computational\ncomplexity of the underlying optimization algorithm in non-trivial ways. Unlikely\noptimization algorithms such as stochastic gradient descent show amazing perfor-\nmance for large-scale problems. In particular, second order stochastic gradient and\naveraged stochastic gradient are asymptotically e\u000ecient after a single pass on the\ntraining set.\nKeywords: Stochastic gradient descent, Online learning, E\u000eciency\n1 Introduction\nThe computational complexity of learning algorithm becomes the critical\nlimiting factor when one envisions very large datasets. This contribution ad-\nvocates stochastic gradient algorithms for large scale machine learning prob-\nlems. The \frst section describes the stochastic gradient algorithm. The sec-\nond section presents an analysis that explains why stochastic gradient algo-\nrithms are attractive when the data is abundant. The third section discusses\nthe asymptotical e\u000eciency of estimates obtained after a single pass over the\ntraining set. The last section presents empirical evidence.\n2 Learning with gradient descent\nLet us \frst consider a simple supervised learning setup. Each example z\nis a pair (x;y) composed of an arbitrary input xand a scalar output y. We\nconsider a loss function `(^y;y) that measures the cost of predicting ^ ywhen the\nactual answer is y, and we choose a family Fof functions fw(x) parametrized\nby a weight vector w. We seek the function f2F that minimizes the loss\nQ(z;w) =`(fw(x);y) averag",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "70": {
    "pdf_path": "data/pdfs\\machine learning_paper_190.pdf",
    "text_excerpt": "Page 1 \n Applications of machine learning to machine fault diagnosis: A 1 \nreview and roadmap 2 \nYaguo Lei a,∗, Bin Yang a, Xinwei Jiang a, Feng Jia a, Naipeng Li a, Asoke K. Nandi b 3 \na Key Laboratory of Education Ministry for Modern Design and Rotor -Bearing System, Xi ’an Jiaotong 4 \nUniversity, Xi’ an 710049, China  5 \nb Department of Electronic and Computer Engineering, Brunel University London, Uxbridge UB8 3PH, 6 \nUnited Kingdom  7 \nAbstract 8 \nIntelligent fault diagnosis (IFD) refers to applications of machine learning theories to machine fault 9 \ndiagnosis. This is a promising way to release the contribution from human labor and automatically recognize 10 \nthe health states of machines, thus it has attracte d much attention in the last two or three decades. Although 11 \nIFD has achieved a considerable number of successes, a review still leaves a blank space to systematically 12 \ncover the development of IFD from the cradle to the bloom, and rarely provides potential guidelines for the 13 \nfuture development. To bridge the gap, this paper presents a review and roadmap to systematically cover the 14 \ndevelopment of IFD following the progress of machine learning theories and offer a future perspective. In the 15 \npast, traditional m achine learning theories began to weak the contribution of human labor and brought the era 16 \nof artificial intelligence to machine fault diagnosis. Over the recent years, the advent of deep learning theories 17 \nhas reformed IFD in further releasing the artifici al assistance since the 2010s, which encourages to construct 18 \nan end -to-end diagnosis process. It means to directly bridge the relationship between the increasingly -grown 19 \nmonitoring data and the health states of machines. In the future, transfer learning theories attempt to use the 20 \ndiagnosis knowledge from one or multiple diagnosis tasks to other related ones, which prospectively 21 \novercomes the obstacles in applications of IFD to engineering scenarios. Fi",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "71": {
    "pdf_path": "data/pdfs\\machine learning_paper_193.pdf",
    "text_excerpt": "arXiv:cs/0110053v1  [cs.IR]  26 Oct 2001Machine Learning in Automated Text Categorization\nFabrizio Sebastiani\nConsiglio Nazionale delle Ricerche, Italy\nThe automated categorization (or classiﬁcation) of texts i nto predeﬁned categories has witnessed a\nbooming interest in the last ten years, due to the increased a vailability of documents in digital form\nand the ensuing need to organize them. In the research commun ity the dominant approach to this\nproblem is based on machine learning techniques: a general i nductive process automatically builds\na classiﬁer by learning, from a set of preclassiﬁed document s, the characteristics of the categories.\nThe advantages of this approach over the knowledge engineer ing approach (consisting in the\nmanual deﬁnition of a classiﬁer by domain experts) are a very good eﬀectiveness, considerable\nsavings in terms of expert manpower, and straightforward po rtability to diﬀerent domains. This\nsurvey discusses the main approaches to text categorizatio n that fall within the machine learning\nparadigm. We will discuss in detail issues pertaining to thr ee diﬀerent problems, namely document\nrepresentation, classiﬁer construction, and classiﬁer ev aluation.\nCategories and Subject Descriptors: H.3.1 [ Information storage and retrieval ]: Content anal-\nysis and indexing— Indexing methods ; H.3.3 [ Information storage and retrieval ]: Informa-\ntion search and retrieval— Information ﬁltering ; H.3.3 [ Information storage and retrieval ]:\nSystems and software— Performance evaluation (eﬃciency and eﬀectiveness) ; I.2.3 [ Artiﬁcial\nIntelligence ]: Learning— Induction\nGeneral Terms: Algorithms, Experimentation, Theory\nAdditional Key Words and Phrases: Machine learning, text ca tegorization, text classiﬁcation\n1. INTRODUCTION\nIn the last ten years content-based document management tas ks (collectively known\nasinformation retrieval – IR) have gained a prominent status in the information\nsystems ﬁeld, due to the increased availability of document s in ",
    "title": "An open source machine learning framework for efficient and transparent systematic reviews",
    "abstract": "To help researchers conduct a systematic review or meta-analysis as efficiently and transparently as possible, we designed a tool to accelerate the step of screening titles and abstracts. For many tasks—including but not limited to systematic reviews and meta-analyses—the scientific literature needs to be checked systematically. Scholars and practitioners currently screen thousands of studies by hand to determine which studies to include in their review or meta-analysis. This is error prone and inefficient because of extremely imbalanced data: only a fraction of the screened studies is relevant. The future of systematic reviewing will be an interaction with machine learning algorithms to deal with the enormous increase of available text. We therefore developed an open source machine learning-aided pipeline applying active learning: ASReview. We demonstrate by means of simulation studies that active learning can yield far more efficient reviewing than manual reviewing while providing high quality. Furthermore, we describe the options of the free and open source research software and present the results from user experience tests. We invite the community to contribute to open source projects such as our own that provide measurable and reproducible improvements over current practice. It is a challenging task for any research field to screen the literature and determine what needs to be included in a systematic review in a transparent way. A new open source machine learning framework called ASReview, which employs active learning and offers a range of machine learning models, can check the literature efficiently and systemically.",
    "link": "https://www.semanticscholar.org/paper/223846b7b56e250e1b6f521997b4c1b809cc0da7",
    "published": "2020-06-22"
  },
  "72": {
    "pdf_path": "data/pdfs\\machine learning_paper_196.pdf",
    "text_excerpt": "REVIEW ARTICLE OPEN\nSmall data machine learning in materials science\nPengcheng Xu1, Xiaobo Ji2, Minjie Li2✉and Wencong Lu1,2,3✉\nThis review discussed the dilemma of small data faced by materials machine learning. First, we analyzed the limitations brought by\nsmall data. Then, the work ﬂow of materials machine learning has been introduced. Next, the methods of dealing with small data\nwere introduced, including data extraction from publications, materials database construction, high-throughput computations and\nexperiments from the data source level; modeling algorithms for small data and imbalanced learning from the algorithm level;\nactive learning and transfer learning from the machine learning strategy level. Finally, the future directions for small data machinelearning in materials science were proposed.\nnpj Computational Materials            (2023) 9:42 ; https://doi.org/10.1038/s41524-023-01000-z\nINTRODUCTION\nAs an interdisciplinary subject covering computer science,\nmathematics, statistics and engineering, machine learning is\ndedicated to optimizing the performance of computer programs\nby using data or previous experience, which is also one of theimportant directions of arti ﬁcial intelligence development\n1,2.I n\nrecent years, machine learning has been widely used in manyﬁelds such as ﬁnance, medical care, industry, and biology\n3–10.I n\n2011, the concept of material genome initiative (MGI) wasproposed to shorten the material development cycle through\ncomputational tools, experimental facilities and digital data. Under\nthe leadership of the MGI, machine learning has also become oneof the important means for materials design and discovery\n11,12.\nThe core of machine learning-assisted materials design anddiscovery lies in the construction of machine learning modelswith good performance through algorithms and materials data toachieve the accurate prediction of target properties for undeter-\nmined samples\n13. The constructed model could be further used to\ndiscover and",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "73": {
    "pdf_path": "data/pdfs\\machine learning_paper_199.pdf",
    "text_excerpt": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1724–1734,\nOctober 25-29, 2014, Doha, Qatar. c\r2014 Association for Computational Linguistics\nLearning Phrase Representations using RNN Encoder–Decoder\nfor Statistical Machine Translation\nKyunghyun Cho\nBart van Merri ¨enboer Caglar Gulcehre\nUniversit ´e de Montr ´eal\nfirstname.lastname@umontreal.caDzmitry Bahdanau\nJacobs University, Germany\nd.bahdanau@jacobs-university.de\nFethi Bougares Holger Schwenk\nUniversit ´e du Maine, France\nfirstname.lastname@lium.univ-lemans.frYoshua Bengio\nUniversit ´e de Montr ´eal, CIFAR Senior Fellow\nfind.me@on.the.web\nAbstract\nIn this paper, we propose a novel neu-\nral network model called RNN Encoder–\nDecoder that consists of two recurrent\nneural networks (RNN). One RNN en-\ncodes a sequence of symbols into a ﬁxed-\nlength vector representation, and the other\ndecodes the representation into another se-\nquence of symbols. The encoder and de-\ncoder of the proposed model are jointly\ntrained to maximize the conditional prob-\nability of a target sequence given a source\nsequence. The performance of a statisti-\ncal machine translation system is empiri-\ncally found to improve by using the con-\nditional probabilities of phrase pairs com-\nputed by the RNN Encoder–Decoder as an\nadditional feature in the existing log-linear\nmodel. Qualitatively, we show that the\nproposed model learns a semantically and\nsyntactically meaningful representation of\nlinguistic phrases.\n1 Introduction\nDeep neural networks have shown great success in\nvarious applications such as objection recognition\n(see, e.g., (Krizhevsky et al., 2012)) and speech\nrecognition (see, e.g., (Dahl et al., 2012)). Fur-\nthermore, many recent works showed that neu-\nral networks can be successfully used in a num-\nber of tasks in natural language processing (NLP).\nThese include, but are not limited to, language\nmodeling (Bengio et al., 2003), paraphrase detec-\ntion (Socher et al., 2011) and word",
    "title": "Mathematics for Machine Learning",
    "abstract": "Machine learning is a way to study the algorithm and statistical model that is used by computer to perform a specific task through pattern and deduction [1]. It builds a mathematical model from a sample data which may come under either supervised or unsupervised learning. It is closely\n related to computational statistics which is an interface between statistics and computer science. Also, linear algebra and probability theory are two tools of mathematics which form the basis of machine learning. In general, statistics is a science concerned with collecting, analysing, interpreting\n the data. Data are the facts and figure that can be classified as either quantitative or qualitative. From the given set of data, we can predict the expected observation, difference between the outcome of two observations and how data look like which can help in better decision making process\n [2]. Descriptive and inferential statistics are the two methods of data analysis. Descriptive statistics summarize the raw data into information through which common expectation and variation of data can be taken. It also provides graphical methods that can be used to visualize the sample\n of data and qualitative understanding of observation whereas inferential statistics refers to drawing conclusions from data. Inferences are made under the framework of probability theory. So, understanding of data and interpretation of result are two important aspects of machine learning.\n In this paper, we have reviewed the different methods of ML, mathematics behind ML, its application in day to day life and future aspects.",
    "link": "https://www.semanticscholar.org/paper/4f97e87512eb8bf48ce695443e958725c54908b6",
    "published": "2020-02-01"
  },
  "74": {
    "pdf_path": "data/pdfs\\machine learning_paper_2.pdf",
    "text_excerpt": "arXiv:1811.04422v1  [cs.LG]  11 Nov 2018An Optimal Control View of Adversarial Machine Learning\nXiaojin Zhu\nDepartment of Computer Sciences, University of Wisconsin-Madiso n\nAbstract\nI describe an optimal control view of adversarial machine le arning, where the dynamical system is the\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\ngoals to do harm and be hard to detect. This view encompasses m any types of adversarial machine\nlearning, including test-item attacks, training-data poi soning, and adversarial reward shaping. The view\nencourages adversarial machinelearningresearcher touti lize advancesincontroltheoryandreinforcement\nlearning.\n1 Adversarial Machine Learning is not Machine Learning\nMachine learning has its mathematical foundation in concentration in equalities. This is a consequence of\nthe independent and identically-distributed (i.i.d.) data assumption. I n contrast, I suggest that adversarial\nmachine learning may adopt optimal control as its mathematical fou ndation [3,25]. There are telltale signs:\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. st ructures – as control input might be.\n2 Optimal Control\nI will focus on deterministic discrete-time optimal control because it matches many existing adversarial\nattacks. Extensions to stochastic and continuous control are r elevant to adversarial machine learning, too.\nThe system to be controlled is called the plant, which is deﬁned by the s ystem dynamics:\nxt+1=f(xt,ut) (1)\nwherext∈Xtis the state of the system, ut∈Utis the control input, and Utis the control constraint\nset. The function fdeﬁnes the evolution of state under external control. The time ind extranges from 0\ntoT−1, and the time horizon Tcan be ﬁnite or inﬁnite. The quality of control is speciﬁed by the runn ing\ncost:\ngt(xt,ut) (2)\nwhich deﬁnes the step-by-step control cost, and the terminal c ost for ﬁnite horizon:\ngT(xT) (3)\nwhich deﬁnes the quality of the ﬁnal state. Th",
    "title": "An Optimal Control View of Adversarial Machine Learning",
    "abstract": "I describe an optimal control view of adversarial machine learning, where the\ndynamical system is the machine learner, the input are adversarial actions, and\nthe control costs are defined by the adversary's goals to do harm and be hard\nto detect. This view encompasses many types of adversarial machine learning,\nincluding test-item attacks, training-data poisoning, and adversarial reward\nshaping. The view encourages adversarial machine learning researcher to utilize\nadvances in control theory and reinforcement learning.",
    "link": "http://arxiv.org/abs/1811.04422v1",
    "published": "2018-11-11T14:28:34Z"
  },
  "75": {
    "pdf_path": "data/pdfs\\machine learning_paper_20.pdf",
    "text_excerpt": "Mathematical Perspective of Machine Learning\nYarema Boryshchak, PhD\nJuly 6, 2020\nAbstract\nWe take a closer look at some theoretical challenges of Machine Learning as a func-\ntion approximation, gradient descent as the default optimization algorithm, limitations of\nﬁxed length and width networks and a diﬀerent approach to RNNs from a mathematical\nperspective.\n1 Introduction\nIn the nutshell the idea of training a neural network ( NN) is equivalent to the problem\napproximation of a given function, f, with the domain, D, and codomain, C,\nf:D!C (1)\nwhich depends on some data of size N2Nofk-dimensional input vectors, ~ xj2D\u0012 Rk\nandl-dimensional output (label) vectors, ~ yj2C\u0012 Rl, by a composition of functions of the\nform\n~Pi(~ zi\u00001\u0001~ wi) = (Pi(~ zi\u00001\u0001~ wi);:::;Pi(~ zi\u00001\u0001~ wi)); (2)\nwhere Piis called an activation function of layer i,~ zi\u00001is an output vector of the layer\ni\u00001, and ~ wiis called the weight vector of layer i. Once the size of each layer and the\nchoice of each activation function is made, one usually uses, so called, back propagation\nalgorithm, adjusting the values of each weight vector according to some type of gradient\ndescent rule. In other words, one is trying to solve an optimization problem, minimizing\nthe \"diﬀerence norm\"\nC:=\r\r\rf\u0000~f\r\r\r\nd;where ~f=~Pr\u0010\n~Pr\u00001(:::~P1)\u0011\n(3)\nandr2Nis the number of layers of the neural network.\nSo apriori, we are making a choice of the function ~fof weights~ w1;:::;~ wr. Note that\nthe dimension of each vector ~ wiis the size of the layer i. To simplify the problem we may\n1arXiv:2007.01503v1  [cs.LG]  3 Jul 2020always ﬁnd the maximum, m, of the size layers, and assume that each ~ wi2Rm. We are\nimplicitly assuming that for each i= 1;:::;r,~Pi(~0) =~0.1\n2 Existence of function fand the toll of cost function C\nFirst thing to consider, given a labeled data set f(~ xj;~ yj) :j= 1;:::;Ng, if there is a\nrepresentation function fsuch that f(~ xj) =~ yjfor allj= 1;:::;N. This important step is\noften overlooked in practice. In theory, ",
    "title": "Mathematical Perspective of Machine Learning",
    "abstract": "We take a closer look at some theoretical challenges of Machine Learning as a\nfunction approximation, gradient descent as the default optimization algorithm,\nlimitations of fixed length and width networks and a different approach to RNNs\nfrom a mathematical perspective.",
    "link": "http://arxiv.org/abs/2007.01503v1",
    "published": "2020-07-03T05:26:02Z"
  },
  "76": {
    "pdf_path": "data/pdfs\\machine learning_paper_201.pdf",
    "text_excerpt": "Machine Learning for High-Speed\nCorner Detection\nEdward Rosten and Tom Drummond\nDepartment of Engineering,\nCambridge University, UK\n{er258, twd20 }@cam.ac.uk\nAbstract. Where feature points are used in real-time frame-rate appli-\ncations, a high-speed feature detector is necessary. Feature detectors such\nas SIFT (DoG), Harris and SUSAN are good methods which yield high\nquality features, however they are too computationally intensive for use\nin real-time applications of any complexity. Here we show that machine\nlearning can be used to derive a feature detector which can fully process\nlive PAL video using less than 7% of the available processing time. By\ncomparison neither the Harris detector (120%) nor the detection stage\nof SIFT (300%) can operate at full frame rate.\nClearly a high-speed detector is of limited use if the features produced\nare unsuitable for downstream processing. In particular, the same scene\nviewed from two diﬀerent positions should yield features which corre-\nspond to the same real-world 3D locations[1]. Hence the second contri-\nbution of this paper is a comparison corner detectors based on this crite-\nrion applied to 3D scenes. This comparison supports a number of claims\nmade elsewhere concerning existing corner detectors. Further, contrary\nto our initial expectations, we show that despite being principally con-\nstructed for speed, our detector signiﬁ cantly outperforms existing feature\ndetectors according to this criterion.\n1 Introduction\nCorner detection is used as the ﬁrst step of many vision tasks such as tracking,\nSLAM (simultaneous localisation and mapping), localisation, image matching\nand recognition. Hence, a large number of corner detectors e xist in the litera-\nture. With so many already available i t may appear unnecessary to present yet\nanother detector to the community; however, we have a strong interest in real-\ntime frame rate applications such as SLAM in which computational resources\nare at a premium. In particular, it is still tr",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "77": {
    "pdf_path": "data/pdfs\\machine learning_paper_203.pdf",
    "text_excerpt": "Machine Learning for Fluid\nMechanics\nSteven L. Brunton,1Bernd R. Noack2 3and\nPetros Koumoutsakos4\n1Mechanical Engineering, University of Washington, Seattle, WA, USA, 98195\n2LIMSI, CNRS, Universit\u0013 e Paris-Saclay, F-91403 Orsay, France\n3Institut f ur Str omungsmechanik und Technische Akustik, TU Berlin, D-10634,\nGermany\n4Professorship for Computational Science, ETH Zurich, CH-8092, Switzerland;\nemail: petros@ethz.ch\nXxxx. Xxx. Xxx. Xxx. YYYY. AA:1{32\nhttps://doi.org/10.1146/((please add\narticle doi))\nCopyright c\rYYYY by Annual Reviews.\nAll rights reservedKeywords\nmachine learning, data-driven modeling, optimization, control\nAbstract\nThe \feld of \ruid mechanics is rapidly advancing, driven by unprece-\ndented volumes of data from \feld measurements, experiments and large-\nscale simulations at multiple spatiotemporal scales. Machine learning\no\u000bers a wealth of techniques to extract information from data that\ncould be translated into knowledge about the underlying \ruid me-\nchanics. Moreover, machine learning algorithms can augment domain\nknowledge and automate tasks related to \row control and optimiza-\ntion. This article presents an overview of past history, current devel-\nopments, and emerging opportunities of machine learning for \ruid me-\nchanics. It outlines fundamental machine learning methodologies and\ndiscusses their uses for understanding, modeling, optimizing, and con-\ntrolling \ruid \rows. The strengths and limitations of these methods are\naddressed from the perspective of scienti\fc inquiry that considers data\nas an inherent part of modeling, experimentation, and simulation. Ma-\nchine learning provides a powerful information processing framework\nthat can enrich, and possibly even transform, current lines of \ruid me-\nchanics research and industrial applications.\n1arXiv:1905.11075v3  [physics.flu-dyn]  4 Jan 20201. INTRODUCTION\nFluid mechanics has traditionally dealt with massive amounts of data from experiments,\n\feld measurements, and large-scale numerical simulati",
    "title": "Some Studies in Machine Learning Using the Game of Checkers",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772",
    "published": "1995-10-31"
  },
  "78": {
    "pdf_path": "data/pdfs\\machine learning_paper_204.pdf",
    "text_excerpt": " \n \n Vol. 02, No. 01, pp. 20 – 28 (2021 ) \nISSN: 2708 -0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS  \n \nwww.jastt.org   \n \n20 \ndoi: 10.38094/jastt20 165       \n \nClassification Based on Decision Tree Algorithm for \nMachine Learning  \n \nBahzad Taha Jijo1*, Adnan Mohsin Abdulazeez2 \n1IT Department , Technical College of Informatics Akre, Duhok Polytechnic University, Duhok , Kurdistan Region, Iraq, \nbahzad .taha@dpu.edu.krd   \n2Presidency of Duhok Polytechnic University, Duhok, Kurdistan Region, Iraq, adnan.mohsin@dpu.edu.krd  \n*Correspondence : bahzad .taha@dpu.edu.krd   \n \n \nAbstract  \nDecision tree classifiers are regarded to be a standout of the most well -known methods to data classification representation of classifiers. \nDifferent researchers from various fields and backgrounds have considered the problem of extending a decision tree from avail able \ndata, such as m achine study, pattern recognition, and statistics. In various fields such as medical disease analysis, text classification, \nuser smartphone classification, images, and many more the employment of Decision tree classifiers has been proposed in many w ays. \nThis paper provides a detailed approach to the decision trees. Furthermore, paper specifics, such as algorithms/approaches used , \ndatasets, and outcomes achieved, are evaluated and outlined comprehensively. In addition, all of the approaches analyzed were  \ndiscussed to illustrate the themes of the authors and identify the most accurate classifiers. As a result, the uses of different  types of \ndatasets are discussed and their findings are analyzed.  \n \nKeywords:  Machine Learning, Supervised, Classification, Decision Tree.  \nReceived: January  11th, 2021  / Accepted: March 15th, 2021  / Online: March 24th, 2021  \n \nI. INTRODUCTION  \nNowadays, technology has developed a lot, especially in \nthe field of Machine Learning (ML), which is useful for \nreducing human work. In the field of artificial intelligence, ML \nintegrates statistics an",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "79": {
    "pdf_path": "data/pdfs\\machine learning_paper_207.pdf",
    "text_excerpt": "FUNDAMENTALS\nMachine learning and deep learning\nChristian Janiesch1&Patrick Zschech2&Kai Heinrich3\nReceived: 7 October 2020 / Accepted: 19 March 2021\n#The Author(s) 2021, corrected publication 2021\nAbstract\nToday, intelligent systems that offer artificial intelligence capabilities often rely on machine learning. Machine learning describes\nthe capacity of systems to learn from problem-specific training data to automate the process of analytical model building and\nsolve associated tasks. Deep learning is a machine learning concept based on artificial neural networks. For many applications,\ndeep learning models outperform shallow machine learning models and traditional data analysis approaches. In this article, we\nsummarize the fundamentals of machine learning and deep learning to generate a broader understanding of the methodical\nunderpinning of current intelligent systems. In particular, we provide a conceptual distinction between relevant terms and\nconcepts, explain the process of automated analytical model building through machine learning and deep learning, and discuss\nthe challenges that arise when implementing such intelligent systems in the field of electronic markets and networked business.\nThese naturally go beyond technological aspects and highlight issues in human-machine interaction and artificial intelligence\nservitization.\nKeywords Machine learning .Deep learning .Artificial intelligence .Artificial neural networks .Analytical model building\nJEL classification C6.C8.M15 .O3\nIntroduction\nIt is considered easier to explain to a child the nature of what\nconstitutes a sports car as opposed to a normal car by showing\nhim or her examples, rather than trying to formulate explicit\nrules that define a sports car.\nSimilarly, instead of codifying knowledge into computers,\nmachine learning (ML) seeks to automatically learnmeaningful relationships and patterns from examples and ob-\nservations (Bishop 2006 ). Advances in ML have enabled the\nrecent rise of intelligent ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "80": {
    "pdf_path": "data/pdfs\\machine learning_paper_21.pdf",
    "text_excerpt": "PRIVATE MACHINE LEARNING VIA RANDOMISED RE-\nSPONSE\nDavid Barber\nDepartment of Computer Science\nUniversity College London, UK\nABSTRACT\nWe introduce a general learning framework for private machine learning based on\nrandomised response. Our assumption is that all actors are potentially adversarial\nand as such we trust only to release a single noisy version of an individual’s\ndatapoint. Our approach forms a consistent way to estimate the true underlying\nmachine learning model and we demonstrate this in the case of logistic regression.\n1 P RIVATE MACHINE LEARNING\nOur desire is to develop a strategy for machine learning driven by the requirement that private data\nshould be shared as little as possible and that no-one can be trusted with an individual’s data, neither\na data collector/aggregator, nor the machine learner that tries to ﬁt a model.\nRandomised Response, see for example Warner (1965), is relevant in this context in which a datapoint\nxnis replaced with a randomised ‘noisy’ version ~xn. A classical example is voting in an election in\nwhich an individual voter votes for one of two candidates AorBand is asked to lie (with probability\np) about whom they voted for . This results in noisy data and estimating the fraction fAof voters that\nvoted for candidate Abased on this noisy data\n~fA=1\nNNX\nn=1I(~xn=A) (1)\ncan give a potentially signiﬁcantly incorrect estimate. As Warner (1965) showed, since we know\nthe probabilistic mechanism that generated the noisy data, a better estimate of the fraction of voters\nvoting for candidate Ais given by\nfA=~fA+p\n1\u00002p(2)\nIn a machine learning context, the kind of scenario we envisage is that users may have labelled face\nimages as “happy\" or “sad\" on their mobile phones and the company MugTome wishes to train a\n“happy/sad\" face classiﬁer; however, users do not wish to send the raw face images to MugTome and\nalso wish to be able to plausibly deny which label they gave any training image. To preserve privacy,\neach user will send to MugTome",
    "title": "Private Machine Learning via Randomised Response",
    "abstract": "We introduce a general learning framework for private machine learning based\non randomised response. Our assumption is that all actors are potentially\nadversarial and as such we trust only to release a single noisy version of an\nindividual's datapoint. We discuss a general approach that forms a consistent\nway to estimate the true underlying machine learning model and demonstrate this\nin the case of logistic regression.",
    "link": "http://arxiv.org/abs/2001.04942v2",
    "published": "2020-01-14T17:56:16Z"
  },
  "81": {
    "pdf_path": "data/pdfs\\machine learning_paper_210.pdf",
    "text_excerpt": " \n1 A guide to machine learning for biologists  \n \nJoe G. Greener*  \nDept of Computer Science, University College London, Gower Street, London WC1E 6BT, \nUnited Kingdom  \nj.greener@ucl.ac.uk  \n \nShaun M. Kandathil*  \nDept of Computer Science, University College London, Gower Street, London WC1E 6BT, \nUnited Kingdom  \ns.kandathil@ucl.ac.uk  \n \nLewis Moffat  \nDept of Computer Science, University College London, Gower Street, London WC1E 6BT, \nUnited Kingdom  \nlewis.moffat@cs.ucl.ac.uk  \n \nDavid T. Jones † \nDept of Computer Science , University College London, Gower Street, London WC1E 6BT, \nUnited Kingdom  \nd.t.jones@ucl.ac.uk  \n \n* These authors contributed equally  \n† Correspondence should be addressed to: d.t.jones@ucl.ac.uk  \n \n \n \nThe expanding scale and inherent complexity of biological data have encouraged a growing \nuse of machine learning in biology to build informative and predictive models of the \nunderlying biological processes. All machine learning techniques fit models to dat a, however, \nthe specific methodologies are quite varied and can at first glance seem bewildering. In this \narticle, we aim to provide readers with a gentle introduction to a few key machine learning \ntechniques, including those based on the most recently dev eloped and widely used techniques \ninvolving deep neural networks. We describe how different techniques may be suited to \nspecific types of biological data, and also discuss some best practices and points to consider \nwhen embarking on experiments involving m achine learning. Some emerging directions in \nmachine learning methodology will also be discussed.  \n   \n2 Introduction  \n \nHumans make sense of the world around them by observing it, and  learning to predict what might \nhappen next. Consider a child learning to catch a ball: the child (usually) knows nothing about the \nphysical laws that govern the motion of a thrown ball, yet, by a process of observation, trial and \nerror, the child adjusts their understanding of the bal",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "82": {
    "pdf_path": "data/pdfs\\machine learning_paper_217.pdf",
    "text_excerpt": "A Review of Feature Selection\nMethods for Machine Learning-Based\nDisease Risk Prediction\nNicholas Pudjihartono1, Tayaza Fadason1,2, Andreas W. Kempa-Liehr3* and\nJustin M. O ’Sullivan1,2,4,5,6*\n1Liggins Institute, University of Auckland, Auckland, New Zealand,2Maurice Wilkins Centre for Molecular Biodiscovery, Auckland,\nNew Zealand,3Department of Engineering Science, The University of Auckland, Auckland, New Zealand,4MRC Lifecourse\nEpidemiology Unit, University of Southampton, Southampton, United Kingdom,5Singapore Institute for Clinical Sciences, Agency\nfor Science, Technology and Research (A*STAR), Singapore, Singapore,6Australian Parkinson ’s Mission, Garvan Institute of\nMedical Research, Sydney, NSW, Australia\nMachine learning has shown utility in detecting patterns within large, unstructured, and\ncomplex datasets. One of the promising applications of machine learning is in precisionmedicine, where disease risk is predicted using patient genetic data. However, creating anaccurate prediction model based on genotype data remains challenging due to the so-called “curse of dimensionality ”(i.e., extensively larger number of features compared to the\nnumber of samples). Therefore, the generalizability of machine learning models bene ﬁts\nfrom feature selection, which aims to extract only the most “informative ”features and\nremove noisy “non-informative, ”irrelevant and redundant features. In this article, we\nprovide a general overview of the different feature selection methods, their advantages,disadvantages, and use cases, focusing on the detection of relevant features (i.e., SNPs)for disease risk prediction.\nKeywords: machine learing, feature selection (FS), risk prediction, disease risk prediction, statistical approaches\n1 INTRODUCTION\n1.1 Precision Medicine and Complex Disease Risk Prediction\nThe advancement of genetic sequencing technology over the last decade has re-ignited interest in\nprecision medicine and the goal of providing healthcare based on a patient ’s i",
    "title": "A Review on Linear Regression Comprehensive in Machine Learning",
    "abstract": "Perhaps one of the most common and comprehensive statistical and machine learning algorithms are linear regression. Linear regression is used to find a linear relationship between one or more predictors. The linear regression has two types: simple regression and multiple regression (MLR). This paper discusses various works by different researchers on linear regression and polynomial regression and compares their performance using the best approach to optimize prediction and precision. Almost all of the articles analyzed in this review is focused on datasets; in order to determine a model's efficiency, it must be correlated with the actual values obtained for the explanatory variables.",
    "link": "https://www.semanticscholar.org/paper/99afa67e28780754907b19b688bf2b35eb22e578",
    "published": null
  },
  "83": {
    "pdf_path": "data/pdfs\\machine learning_paper_218.pdf",
    "text_excerpt": "Educational data mining: prediction \nof students’ academic performance using \nmachine learning algorithms\nMustafa Yağcı*  \nIntroduction\nThe application of data mining methods in the field of education has attracted great \nattention in recent years. Data Mining (DM) is the discovery of data. It is the field of \ndiscovering new and potentially useful information or meaningful results from big data \n(Witten et al., 2011). It also aims to obtain new trends and new patterns from large data -\nsets by using different classification algorithms (Baker & Inventado, 2014).\nEducational data mining (EDM) is the use of traditional DM methods to solve prob -\nlems related to education (Baker & Yacef, 2009; cited in Fernandes et al., 2019). EDM \nis the use of DM methods on educational data such as student information, edu -\ncational records, exam results, student participation in class, and the frequency of Abstract \nEducational data mining has become an effective tool for exploring the hidden rela-\ntionships in educational data and predicting students’ academic achievements. This \nstudy proposes a new model based on machine learning algorithms to predict the \nfinal exam grades of undergraduate students, taking their midterm exam grades as \nthe source data. The performances of the random forests, nearest neighbour, support \nvector machines, logistic regression, Naïve Bayes, and k-nearest neighbour algorithms, \nwhich are among the machine learning algorithms, were calculated and compared to \npredict the final exam grades of the students. The dataset consisted of the academic \nachievement grades of 1854 students who took the Turkish Language-I course in a \nstate University in Turkey during the fall semester of 2019–2020. The results show that \nthe proposed model achieved a classification accuracy of 70–75%. The predictions \nwere made using only three types of parameters; midterm exam grades, Depart -\nment data and Faculty data. Such data-driven studies are very important in terms of \n",
    "title": "A Survey of Human-in-the-loop for Machine Learning",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/102ebe229df18c8733ea1b8def56cd79996e2178",
    "published": "2021-08-02"
  },
  "84": {
    "pdf_path": "data/pdfs\\machine learning_paper_219.pdf",
    "text_excerpt": "Vol.:(0123456789)Artificial Intelligence Review (2023) 56:3005–3054\nhttps://doi.org/10.1007/s10462-022-10246-w\n1 3\nHuman‑in‑the‑loop machine learning: a state of the art\nEduardo Mosqueira‑Rey1  · Elena Hernández‑Pereira1 · David Alonso‑Ríos1 · \nJosé Bobes‑Bascarán1 · Ángel Fernández‑Leal1\nPublished online: 17 August 2022 \n© The Author(s) 2022\nAbstract\nResearchers are defining new types of interactions between humans and machine learn-\ning algorithms generically called human-in-the-loop machine learning. Depending on who \nis in control of the learning process, we can identify: active learning, in which the sys-\ntem remains in control; interactive machine learning, in which there is a closer interaction \nbetween users and learning systems; and machine teaching, where human domain experts \nhave control over the learning process. Aside from control, humans can also be involved \nin the learning process in other ways. In curriculum learning human domain experts try to \nimpose some structure on the examples presented to improve the learning; in explainable \nAI the focus is on the ability of the model to explain to humans why a given solution was \nchosen. This collaboration between AI models and humans should not be limited only to \nthe learning process; if we go further, we can see other terms that arise such as Usable and \nUseful AI. In this paper we review the state of the art of the techniques involved in the new \nforms of relationship between humans and ML algorithms. Our contribution is not merely \nlisting the different approaches, but to provide definitions clarifying confusing, varied and \nsometimes contradictory terms; to elucidate and determine the boundaries between the dif-\nferent methods; and to correlate all the techniques searching for the connections and influ-\nences between them.\nKeywords Human-in-the-loop machine learning · Active learning · Interactive machine \nlearning · Machine teaching · Curriculum learning · Explainable AI\n * Eduardo Mosqueira-Rey \n e",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "85": {
    "pdf_path": "data/pdfs\\machine learning_paper_22.pdf",
    "text_excerpt": "arXiv:1906.06821v2  [cs.LG]  23 Oct 20191\nA Survey of Optimization Methods from\na Machine Learning Perspective\nShiliang Sun, Zehui Cao, Han Zhu, and Jing Zhao\nAbstract —Machine learning develops rapidly, which has made\nmany theoretical breakthroughs and is widely applied in var ious\nﬁelds. Optimization, as an important part of machine learni ng,\nhas attracted much attention of researchers. With the expon ential\ngrowth of data amount and the increase of model complexity,\noptimization methods in machine learning face more and more\nchallenges. A lot of work on solving optimization problems o r\nimproving optimization methods in machine learning has bee n\nproposed successively. The systematic retrospect and summ ary\nof the optimization methods from the perspective of machine\nlearning are of great signiﬁcance, which can offer guidance\nfor both developments of optimization and machine learning\nresearch. In this paper, we ﬁrst describe the optimization\nproblems in machine learning. Then, we introduce the princi ples\nand progresses of commonly used optimization methods. Next ,\nwe summarize the applications and developments of optimiza tion\nmethods in some popular machine learning ﬁelds. Finally, we\nexplore and give some challenges and open problems for the\noptimization in machine learning.\nIndex Terms —Machine learning, optimization method, deep\nneural network, reinforcement learning, approximate Baye sian\ninference.\nI. I NTRODUCTION\nRECENTLY, machine learning has grown at a remarkable\nrate, attracting a great number of researchers and\npractitioners. It has become one of the most popular researc h\ndirections and plays a signiﬁcant role in many ﬁelds, such\nas machine translation, speech recognition, image recogni tion,\nrecommendation system, etc. Optimization is one of the core\ncomponents of machine learning. The essence of most machine\nlearning algorithms is to build an optimization model and le arn\nthe parameters in the objective function from the given data .\nIn the era of",
    "title": "A Survey of Optimization Methods from a Machine Learning Perspective",
    "abstract": "Machine learning develops rapidly, which has made many theoretical\nbreakthroughs and is widely applied in various fields. Optimization, as an\nimportant part of machine learning, has attracted much attention of\nresearchers. With the exponential growth of data amount and the increase of\nmodel complexity, optimization methods in machine learning face more and more\nchallenges. A lot of work on solving optimization problems or improving\noptimization methods in machine learning has been proposed successively. The\nsystematic retrospect and summary of the optimization methods from the\nperspective of machine learning are of great significance, which can offer\nguidance for both developments of optimization and machine learning research.\nIn this paper, we first describe the optimization problems in machine learning.\nThen, we introduce the principles and progresses of commonly used optimization\nmethods. Next, we summarize the applications and developments of optimization\nmethods in some popular machine learning fields. Finally, we explore and give\nsome challenges and open problems for the optimization in machine learning.",
    "link": "http://arxiv.org/abs/1906.06821v2",
    "published": "2019-06-17T02:54:51Z"
  },
  "86": {
    "pdf_path": "data/pdfs\\machine learning_paper_222.pdf",
    "text_excerpt": "The Shapley Value in Machine Learning\nBenedek Rozemberczki1,Lauren Watson2,P´eter Bayer3,Hao-Tsung Yang2,\nOliv ´er Kiss4,Sebastian Nilsson1and Rik Sarkar2\n1Research Data & Analytics, Research & Development IT, AstraZeneca\n2The University of Edinburgh, School of Informatics\n3Toulouse School of Economics & Institute for Advanced Study in Toulouse\n4Central European University, Department of Economics and Business\nbenedek.rozemberczki@astrazeneca.com\nAbstract\nOver the last few years, the Shapley value, a solution\nconcept from cooperative game theory, has found nu-\nmerous applications in machine learning. In this paper,\nwe ﬁrst discuss fundamental concepts of cooperative\ngame theory and axiomatic properties of the Shapley\nvalue. Then, we give an overview of the most important\napplications of the Shapley value in machine learning:\nfeature selection, explainability, multi-agent reinforce-\nment learning, ensemble pruning, and data valuation.\nWe examine the most crucial limitations of the Shapley\nvalue and point out directions for future research.\n1 Introduction\nMeasuring importance and the attribution of various gains is a\ncentral problem in many practical aspects of machine learning\nsuch as explainability [Lundberg et al., 2017 ], feature selection\n[Cohen et al., 2007 ], data valuation [Ghorbani et al. , 2019 ],\nensemble pruning [Rozemberczki et al., 2021 ]and federated\nlearning [Wang et al., 2020; Fan et al., 2021 ]. For example, one\nmight ask: What is the importance of a feature in the decisions of\na machine learning model? How much is an individual data point\nworth? Which models are the most valuable in an ensemble?\nThese questions have been addressed in different domains using\nspeciﬁc approaches. Interestingly, there is also a general and\nuniﬁed approach to these questions as a solution to a transferable\nutility (TU) cooperative game. In contrast with other approaches,\nsolution concepts of TU games are theoretically motivated with\naxiomatic properties. The best known s",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "87": {
    "pdf_path": "data/pdfs\\machine learning_paper_228.pdf",
    "text_excerpt": "Nature | Vol 594 | 10 June 2021 | 265\nArticleSwarm Learning for decentralized and \nconfidential clinical machine learning\nStefanie Warnat-Herresthal1,2,127, Hartmut Schultze3,127, Krishnaprasad Lingadahalli Shastry3,127, \nSathyanarayanan Manamohan3,127, Saikat Mukherjee3,127, Vishesh Garg3,4,127, \nRavi Sarveswara3,127, Kristian Händler1,5,127, Peter Pickkers6,127, N. Ahmad Aziz7,8,127, \nSofia Ktena9,127, Florian Tran10,11, Michael Bitzer12, Stephan Ossowski13,14, Nicolas Casadei13,14, \nChristian Herr15, Daniel Petersheim16, Uta Behrends17, Fabian Kern18, Tobias Fehlmann18, \nPhilipp Schommers19, Clara Lehmann19,20,21, Max Augustin19,20,21, Jan Rybniker19,20,21, \nJanine Altmüller22, Neha Mishra11, Joana P. Bernardes11, Benjamin Krämer23, \nLorenzo Bonaguro1,2, Jonas Schulte-Schrepping1,2, Elena De Domenico1,5, Christian Siever3, \nMichael Kraut1,5, Milind Desai3, Bruno Monnet3, Maria Saridaki9, Charles Martin Siegel3, \nAnna Drews1,5, Melanie Nuesch-Germano1,2, Heidi Theis1,5, Jan Heyckendorf23, \nStefan Schreiber10, Sarah Kim-Hellmuth16, COVID-19 Aachen Study (COVAS)*, \nJacob Nattermann24,25, Dirk Skowasch26, Ingo Kurth27, Andreas Keller18,28, Robert Bals15, \nPeter Nürnberg22, Olaf Rieß13,14, Philip Rosenstiel11, Mihai G. Netea29,30, Fabian Theis31, \nSach Mukherjee32, Michael Backes33, Anna C. Aschenbrenner1,2,5,29, Thomas Ulas1,2,  \nDeutsche COVID-19 Omics Initiative (DeCOI)*, Monique M. B. Breteler7,34 ,128,  \nEvangelos J. Giamarellos-Bourboulis9,128, Matthijs Kox6,128, Matthias Becker1,5,128, \nSorin Cheran3,128, Michael S. Woodacre3,128, Eng Lim Goh3,128 & Joachim L. Schultze1,2,5 ,128 ✉\nFast and reliable detection of patients with severe and heterogeneous illnesses is a \nmajor goal of precision medicine1,2. Patients with leukaemia can be identified using \nmachine learning on the basis of their blood transcriptomes3. However, there is an \nincreasing divide between what is technically possible and what is allowed, because of \nprivacy legislation4,5. Here, to facilitate",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "88": {
    "pdf_path": "data/pdfs\\machine learning_paper_229.pdf",
    "text_excerpt": "Predicting the Future — Big Data, Machine Learning, and \nClinical Medicine\nZiad Obermeyer, M.D., M.Phil.  and\nDepartment of Emergency Medicine, Harvard Medical School and Brigham and Women’s \nHospital, and the Department of Health Care Policy, Harvard Medical School, Boston\nEzekiel J. Emanuel, M.D., Ph.D.\nDepartment of Medical Ethics and Health Policy, Perelman School of Medicine, and the \nDepartment of Health Care Management, the Wharton School, University of Pennsylvania, \nPhiladelphia\nBy now, it’s almost old news: big data will transform medicine. It’s essential to remember, \nhowever, that data by themselves are useless. To be useful, data must be analyzed, \ninterpreted, and acted on. Thus it is algorithms — not data sets — that will prove \ntransformative. We believe attention therefore has to shift to new statistical tools from the \nfield of machine learning that will be critical for anyone practicing medicine in the 21st \ncentury.\nFirst, it’s important to understand what machine learning is not. Most computer-based \nalgorithms in medicine are “expert systems” — rule sets encoding knowledge on a given \ntopic, which are applied to draw conclusions about specific clinical scenarios, such as \ndetecting drug interactions or judging the appropriateness of obtaining radiologic imaging. \nExpert systems work the way an ideal medical student would: they take general principles \nabout medicine and apply them to new patients.\nMachine learning, conversely, approaches problems as a doctor progressing through \nresidency might: by learning rules from data. Starting with patient-level observations, \nalgorithms sift through vast numbers of variables, looking for combinations that reliably \npredict outcomes. In one sense, this process is similar to that of traditional regression \nmodels: there is an outcome, covariates, and a statistical function linking the two. But where \nmachine learning shines is in handling enormous numbers of predictors — sometimes, \nremarkably, more predic",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "89": {
    "pdf_path": "data/pdfs\\machine learning_paper_23.pdf",
    "text_excerpt": "Ten-year Survival Prediction\nfor Breast Cancer Patients\nChangmao Li, Han He, Yunze Hao, Caleb Ziems\nEmory University\nfchangmao.li, han.he, yunze.hao, cziems g@emory.edu\n1 Introduction\nDifferent stages of breast cancer require different treat-\nments. Understanding the current stage of a patient’s breast\ncancer is crucial then for applying the best treatment. For\nthis purpose, machine learning models may be used to\nlearn and predict patient survival or other clinical out-\ncomes from professionally-labeled features or large-scale\ngenomic proﬁles [2, 7].\nIn the present study, we train and tune models to predict\nthe 10-year survival of breast cancer patients using the\nMETABRIC (Molecular Taxonomy of Breast Cancer Inter-\nnational Consortium) dataset. This dataset includes both\nhand-labeled clinical data and high-dimensional genomic\ndata from over 2,000 patients. We trained our ﬁrst set of\nmodels on the clinical data and our second set of models\non genomic data. Our goal was to construct a model from\nthe latter set which could outperform our best model from\nthe former.\nWe deﬁned our learning objective as a classiﬁcation\nproblem with binary class labels. Class 1 was assigned to\npatients who had died of the disease within 10 years (120\nmonths) of prognosis. Class 2 was assigned to patients\nwho survived longer than 10 years.\nThe nature of the METABRIC data makes our learning\nproblem a challenge. We recognize that the clinical fea-\ntures were hand-selected by experts and reﬁned through\ndecades of medical research. Thus, we might reasonably\nexpect many of these features to be strong predictors of\npatient outcomes. Genomic data, on the other hand, is\nquite noisy, and may lead our models to overﬁt because the\nnumber of genomic features far outweighs the number of\nsamples. For this reason, we implemented four advancedlearning algorithms which we selected to overcome this\nchallenge. These include semi-supervised learning, L1-\nregularized logistic regression, a multi-layer perceptro",
    "title": "Ten-year Survival Prediction for Breast Cancer Patients",
    "abstract": "This report assesses different machine learning approaches to 10-year\nsurvival prediction of breast cancer patients.",
    "link": "http://arxiv.org/abs/1911.00776v1",
    "published": "2019-11-02T19:53:32Z"
  },
  "90": {
    "pdf_path": "data/pdfs\\machine learning_paper_230.pdf",
    "text_excerpt": "NBER WORKING PAPER SERIES\nEMPIRICAL ASSET PRICING VIA MACHINE LEARNING\nShihao Gu\nBryan Kelly\nDacheng Xiu\nWorking Paper 25398\nhttp://www.nber.org/papers/w25398\nNATIONAL BUREAU OF ECONOMIC RESEARCH\n1050 Massachusetts Avenue\nCambridge, MA 02134\nDecember 2018, Revised September 2019\nWe benefitted from discussions with Joseph Babcock, Si Chen (Discussant), Rob Engle, Andrea \nFrazzini, Amit Goyal (Discussant), Lasse Pedersen, Lin Peng (Discussant), Alberto Rossi \n(Discussant), Guofu Zhou (Discussant), and seminar and conference participants at Erasmus \nSchool of Economics, NYU, Northwestern, Imperial College, National University of Singapore, \nUIBE, Nanjing University, Tsinghua PBC School of Finance, Fannie Mae, U.S. Securities and \nExchange Commission, City University of Hong Kong, Shenzhen Finance Institute at CUHK, \nNBER Summer Institute, New Methods for the Cross Section of Returns Conference, Chicago \nQuantitative Alliance Conference, Norwegian Financial  Research Conference, EFA, China \nInternational Conference in Finance, 10th World Congress of the Bachelier Finance Society, \nFinancial Engineering and Risk Management International Symposium, Toulouse Financial \nEconometrics Conference, Chicago Conference on New Aspects of Statistics, Financial  \nEconometrics, and Data Science, Tsinghua Workshop on Big Data and Internet Economics, Q \ngroup, IQ-KAP Research Prize Symposium, Wolfe Re- search, INQUIRE UK, Australasian \nFinance and Banking Conference, Goldman Sachs Global Alternative Risk Premia Conference, \nAFA, and Swiss Finance Institute. We gratefully acknowledge the computing support from the \nResearch Computing  Center at the University of Chicago. The views expressed herein are those \nof the authors and do not necessarily reflect the views of the National Bureau of Economic \nResearch.\nAt least one co-author has disclosed a financial relationship of potential relevance for this \nresearch. Further information is available online at http://www.nber.org/papers/w25398",
    "title": "What Role Does Hydrological Science Play in the Age of Machine Learning?",
    "abstract": "This paper is derived from a keynote talk given at the Google's 2020 Flood Forecasting Meets Machine Learning Workshop. Recent experiments applying deep learning to rainfall‐runoff simulation indicate that there is significantly more information in large‐scale hydrological data sets than hydrologists have been able to translate into theory or models. While there is a growing interest in machine learning in the hydrological sciences community, in many ways, our community still holds deeply subjective and nonevidence‐based preferences for models based on a certain type of “process understanding” that has historically not translated into accurate theory, models, or predictions. This commentary is a call to action for the hydrology community to focus on developing a quantitative understanding of where and when hydrological process understanding is valuable in a modeling discipline increasingly dominated by machine learning. We offer some potential perspectives and preliminary examples about how this might be accomplished.",
    "link": "https://www.semanticscholar.org/paper/62d4aaaf562df94c4bdb116ee1e5cc2843c88bec",
    "published": "2020-02-11"
  },
  "91": {
    "pdf_path": "data/pdfs\\machine learning_paper_233.pdf",
    "text_excerpt": "Vol.:(0123456789)Machine Learning (2021) 110:457–506\nhttps://doi.org/10.1007/s10994-021-05946-3\n1 3\nAleatoric and epistemic uncertainty in machine learning: \nan introduction to concepts and methods\nEyke Hüllermeier1  · Willem Waegeman2\nReceived: 26 March 2020 / Revised: 29 September 2020 / Accepted: 2 January 2021 / \nPublished online: 8 March 2021 \n© The Author(s) 2021\nAbstract\nThe notion of uncertainty is of major importance in machine learning and constitutes a key \nelement of machine learning methodology. In line with the statistical tradition, uncertainty \nhas long been perceived as almost synonymous with standard probability and probabilistic \npredictions. Yet, due to the steadily increasing relevance of machine learning for practical \napplications and related issues such as safety requirements, new problems and challenges \nhave recently been identified by machine learning scholars, and these problems may call \nfor new methodological developments. In particular, this includes the importance of dis-\ntinguishing between (at least) two different types of uncertainty, often referred to as alea-\ntoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty \nin machine learning as well as an overview of attempts so far at handling uncertainty in \ngeneral and formalizing this distinction in particular.\nKeywords Uncertainty · Probability · Epistemic uncertainty · Version space learning · \nBayesian inference · Calibration · Ensembles · Gaussian processes · Deep neural networks · \nLikelihood-based methods · Credal sets and classifiers · Conformal prediction · Set-valued \nprediction · Generative models\n1 Introduction\nMachine learning is essentially concerned with extracting models from data, often (though \nnot exclusively) using them for the purpose of prediction. As such, it is inseparably con-\nnected with uncertainty. Indeed, learning in the sense of generalizing beyond the data seen \nEditor: Peter Flach.\n * Eyke Hüllermeier \n eyke@upb.de\n W",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "92": {
    "pdf_path": "data/pdfs\\machine learning_paper_234.pdf",
    "text_excerpt": "ARTICLE\nPower of data in quantum machine learning\nHsin-Yuan Huang1,2,3, Michael Broughton1, Masoud Mohseni1, Ryan Babbush1, Sergio Boixo1,\nHartmut Neven1& Jarrod R. McClean1✉\nThe use of quantum computing for machine learning is among the most exciting prospective\napplications of quantum technologies. However, machine learning tasks where data is pro-vided can be considerably different than commonly studied computational tasks. In this work,we show that some problems that are classically hard to compute can be easily predicted byclassical machines learning from data. Using rigorous prediction error bounds as a founda-tion, we develop a methodology for assessing potential quantum advantage in learning tasks.The bounds are tight asymptotically and empirically predictive for a wide range of learningmodels. These constructions explain numerical results showing that with the help of data,classical machine learning models can be competitive with quantum models even if they aretailored to quantum problems. We then propose a projected quantum model that provides asimple and rigorous quantum speed-up for a learning problem in the fault-tolerant regime. Fornear-term implementations, we demonstrate a signi ﬁcant prediction advantage over some\nclassical models on engineered data sets designed to demonstrate a maximal quantum\nadvantage in one of the largest numerical tests for gate-based quantum machine learning todate, up to 30 qubits.https://doi.org/10.1038/s41467-021-22539-9 OPEN\n1Google Quantum AI, Venice, CA, USA.2Institute for Quantum Information and Matter, Caltech, Pasadena, CA, USA.3Department of Computing and\nMathematical Sciences, Caltech, Pasadena, CA, USA.✉email: jmcclean@google.com\nNATURE COMMUNICATIONS |         (2021) 12:2631 | https://doi.org/10.1038/s41467-021-22539-9 | www.nature.com/naturecommunications 11234567890():,;As quantum technologies continue to rapidly advance, it\nbecomes increasingly important to understand which\napplications can bene ﬁt from the po",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "93": {
    "pdf_path": "data/pdfs\\machine learning_paper_238.pdf",
    "text_excerpt": "Universal Differential Equations for Scienti\u0000c\nMachine Learning\nChristopher Rackauckas  (  crackauc@mit.edu )\nMassachusetts Institute of Technology https://orcid.org/0000-0001-5850-0663\nYingbo Ma \nJulia Computing\nJulius Martensen \nUniversity of Bremen https://orcid.org/0000-0003-4143-3040\nCollin Warner \nMassachusetts Institute of Technology\nKirill Zubov \nSaint Petersburg State University https://orcid.org/0000-0003-0441-449X\nRohit Supekar \nMassachusetts Institute of Technology\nDominic Skinner \nMassachusetts Institute of Technology https://orcid.org/0000-0002-2698-041X\nAli Ramadhan \nMassachusetts Institute of Technology\nAlan Edelman \nMassachusetts Institute of Technology\nArticle\nKeywords:  machine learning, generalized approaches, modeling  methodology\nPosted Date:  August 31st, 2020\nDOI:  https://doi.org/10.21203/rs.3.rs-55125/v1\nLicense:    This work is licensed under a Creative Commons Att ribution 4.0 International License.  \nRead Full LicenseUniversal Diﬀerential Equations for Scientiﬁc\nMachine Learning\nChristopher Rackauckasa,b, Yingbo Mac, Julius Martensend, Collin\nWarnera, Kirill Zubove, Rohit Supekara, Dominic Skinnera, Ali\nRamadhana, and Alan Edelmana\naMassachusetts Institute of Technology\nbUniversity of Maryland, Baltimore\ncJulia Computing\ndUniversity of Bremen\neSaint Petersburg State University\nAugust 26, 2020\nAbstract 1\nIn the context of science, the well-known adage “a picture is w orth a 2\nthousand words” might well be “a model is worth a thousand datas ets.” 3\nScientiﬁc models, such as Newtonian physics or biological g ene regula- 4\ntory networks, are human-driven simpliﬁcations of complex phen omena 5\nthat serve as surrogates for the countless experiments that valid ated the 6\nmodels. Recently, machine learning has been able to overcome t he in- 7\naccuracies of approximate modeling by directly learning the ent ire set of 8\nnonlinear interactions from data. However, without any predeterm ined 9\nstructure from the scientiﬁc basis behind the problem,",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "94": {
    "pdf_path": "data/pdfs\\machine learning_paper_24.pdf",
    "text_excerpt": " \nTuning Learning Rates with the Cumulative-Learning Constant \n By Nathan Faraj   \nAbstract This paper introduces a novel method for optimizing learning rates in machine learning. A previously unrecognized proportionality between learning rates and dataset sizes is discovered, providing valuable insights into how dataset scale inﬂuences training dynamics. Additionally, a cumulative learning constant is identiﬁed, oﬀering a framework for designing and optimizing advanced learning rate schedules. These ﬁndings have the potential to enhance training eﬃciency and performance across a \nwide\n \nrange\n \nof\n \nmachine\n \nlearning\n \napplications. \n1. Introduction \nThe learning rate is a deep learning hyperparameter that is very important for training models eﬃciently. There has been a lot of research done on optimizing learning rates including sophisticated methods for adaptive learning rates as well as alternating learning rates (Wu et al., 2019). However, the learning rate selection is arbitrary - An optimal learning rate is found without understanding due to the belief that the learning rate is too sensitive to changes in other hyperparameters within the model. However, this paper will present that the learning rate follows some strict proportionality laws. Previous work has been done to compare proportionality of learning rates in reference to batch size (Diego et al., 2020). Therefore, this paper will not be exploring how batch size aﬀects the optimal learning rate. \n1.1 Main Contributions \nIn this paper, we demonstrate that the optimal learning rate is inversely proportional to the size of the dataset. Notably, we show that this proportionality holds regardless of whether the dataset is expanded with new data or repeated data; for instance, doubling the number of epochs and doubling the dataset size are equivalent in this context. Additionally, we introduce the concept of a \"cumulative learning constant,\" a value speciﬁc to a given model architecture. This constant can be",
    "title": "Tuning Learning Rates with the Cumulative-Learning Constant",
    "abstract": "This paper introduces a novel method for optimizing learning rates in machine\nlearning. A previously unrecognized proportionality between learning rates and\ndataset sizes is discovered, providing valuable insights into how dataset scale\ninfluences training dynamics. Additionally, a cumulative learning constant is\nidentified, offering a framework for designing and optimizing advanced learning\nrate schedules. These findings have the potential to enhance training\nefficiency and performance across a wide range of machine learning\napplications.",
    "link": "http://arxiv.org/abs/2505.13457v1",
    "published": "2025-04-30T00:07:48Z"
  },
  "95": {
    "pdf_path": "data/pdfs\\machine learning_paper_249.pdf",
    "text_excerpt": "Machine learning and the physical sciences\nGiuseppe Carleo\nCenter for Computational Quantum Physics,\nFlatiron Institute,\n162 5th Avenue,\nNew York, NY 10010,\nUSA\u0003\nIgnacio Cirac\nMax-Planck-Institut fur Quantenoptik,\nHans-Kopfermann-Straße 1,\nD-85748 Garching,\nGermany\nKyle Cranmer\nCenter for Cosmology and Particle Physics,\nCenter of Data Science,\nNew York University, 726 Broadway,\nNew York, NY 10003,\nUSA\nLaurent Daudet\nLightOn, 2 rue de la Bourse, F-75002 Paris,\nFrance\nMaria Schuld\nUniversity of KwaZulu-Natal,\nDurban 4000, South Africa\nNational Institute for Theoretical Physics,\nKwaZulu-Natal, Durban 4000, South Africa,\nand Xanadu Quantum Computing,\n777 Bay Street, M5B 2H7 Toronto,\nCanada\nNaftali Tishby\nThe Hebrew University of Jerusalem,\nEdmond Safra Campus,\nJerusalem 91904,\nIsrael\nLeslie Vogt-Maranto\nDepartment of Chemistry,\nNew York University,\nNew York, NY 10003,\nUSA\nLenka Zdeborová\nInstitut de physique théorique,\nUniversité Paris Saclay, CNRS, CEA,\nF-91191 Gif-sur-Yvette,\nFrancey\nMachine learning encompasses a broad range of algorithms and modeling tools used for\na vast array of data processing tasks, which has entered most scientiﬁc disciplines in\nrecent years. We review in a selective way the recent research on the interface between\nmachine learning and physical sciences. This includes conceptual developments in ma-\nchine learning (ML) motivated by physical insights, applications of machine learning\ntechniques to several domains in physics, and cross-fertilization between the two ﬁelds.\nAfter giving basic notion of machine learning methods and principles, we describe ex-\namples of how statistical physics is used to understand methods in ML. We then movearXiv:1903.10563v2  [physics.comp-ph]  6 Dec 20192\nto describe applications of ML methods in particle physics and cosmology, quantum\nmany body physics, quantum computing, and chemical and material physics. We also\nhighlight research and development into novel computing architectures aimed at acceler-\nating ML. In ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "96": {
    "pdf_path": "data/pdfs\\machine learning_paper_25.pdf",
    "text_excerpt": "When Machine Learning Meets Privacy: A Survey and\nOutlook\nBO LIU∗,University of Technology Sydney, Australia\nMING DING, Data61, CSIRO, Australia\nSINA SHAHAM, The University of Sydney, Australia\nWENNY RAHAYU, La Trobe University, Australia\nFARHAD FAROKHI, The University of Melbourne, Australia\nZIHUAI LIN, The University of Sydney, Australia\nThe newly emerged machine learning (e.g. deep learning) methods have become a strong driving force to\nrevolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance\nsystems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence\nera. It is important to note that the problem of privacy preservation in the context of machine learning is\nquite different from that in traditional data privacy protection, as machine learning can act as both friend and\nfoe. Currently, the work on the preservation of privacy and machine learning (ML) is still in an infancy stage,\nas most existing solutions only focus on privacy problems during the machine learning process. Therefore,\na comprehensive study on the privacy preservation problems and machine learning is required. This paper\nsurveys the state of the art in privacy issues and solutions for machine learning. The survey covers three\ncategories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine\nlearning aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection\nschemes. The current research progress in each category is reviewed and the key challenges are identified.\nFinally, based on our in-depth analysis of the area of privacy and machine learning, we point out future\nresearch directions in this field.\nCCS Concepts: •Security and privacy →Privacy protections ;Social network security and privacy .\nAdditional Key Words and Phrases: machine learning, privacy, deep learning, differential privacy\nACM Reference Format:\nBo Li",
    "title": "When Machine Learning Meets Privacy: A Survey and Outlook",
    "abstract": "The newly emerged machine learning (e.g. deep learning) methods have become a\nstrong driving force to revolutionize a wide range of industries, such as smart\nhealthcare, financial technology, and surveillance systems. Meanwhile, privacy\nhas emerged as a big concern in this machine learning-based artificial\nintelligence era. It is important to note that the problem of privacy\npreservation in the context of machine learning is quite different from that in\ntraditional data privacy protection, as machine learning can act as both friend\nand foe. Currently, the work on the preservation of privacy and machine\nlearning (ML) is still in an infancy stage, as most existing solutions only\nfocus on privacy problems during the machine learning process. Therefore, a\ncomprehensive study on the privacy preservation problems and machine learning\nis required. This paper surveys the state of the art in privacy issues and\nsolutions for machine learning. The survey covers three categories of\ninteractions between privacy and machine learning: (i) private machine\nlearning, (ii) machine learning aided privacy protection, and (iii) machine\nlearning-based privacy attack and corresponding protection schemes. The current\nresearch progress in each category is reviewed and the key challenges are\nidentified. Finally, based on our in-depth analysis of the area of privacy and\nmachine learning, we point out future research directions in this field.",
    "link": "http://arxiv.org/abs/2011.11819v1",
    "published": "2020-11-24T00:52:49Z"
  },
  "97": {
    "pdf_path": "data/pdfs\\machine learning_paper_257.pdf",
    "text_excerpt": "Machine Learning, 9, 5-7 (1992)\n© 1992 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nMachine Learning: A Maturing Field\nWith this volume I complete my four-year term as executive editor of Machine Learning,\nand Tom Dietterich, who has been co-executive editor with me recently, takes over the\nhelm—or starts serving his sentence, depending upon one's point of view. Let me take this\nopportunity to make a few reflections about the state of the field; past, present and future,\nbased on personal observations.\nA decade ago machine learning was regrouping from the rather uneventful 1970s. The\nfirst machine learning workshop was held in 1980 at Carnegie Mellon University with some\ntwo dozen participants and photocopied preprints. Shortly thereafter we started preparing\nthe first machine learning book, and I was in charge of finding a publication venue. However\nthe title \"Machine Learning\" raised skeptical eyebrows in publishers. By \"machine learn-\ning\" did we not really mean learning about machines rather than learning by machines?\nCouldn't we think of something more scientific-sounding to call the book? And anyway\nhadn't Minsky and Papert debunked this learning nonsense? Since it proved difficult to\nexplain the difference between linear perceptrons and symbolic learning to those publishers\n(who shall go unnamed, to protect the guilty), we approached Nils Nilsson and his Tioga\nPress. Nils embraced the project with foresight and enthusiasm, and the rest, as they say\nin the tired cliche, is history.\nThe uneventful '70s, and the struggling turn of the decade, were followed by the boom-\ning '80s. Machine learning became respectable: some would argue (this writer included)\nit even became scientifically sound. In the process, it also became much more popular.\nMachine learning conferences in the late '80s drew ten times as many participants as diat\nfirst CMU workshop: UCAI and AAAI have very well represented machine learning tracks;\nthis journal was establish",
    "title": "Quantum Machine Learning in Feature Hilbert Spaces.",
    "abstract": "A basic idea of quantum computing is surprisingly similar to that of kernel methods in machine learning, namely, to efficiently perform computations in an intractably large Hilbert space. In this Letter we explore some theoretical foundations of this link and show how it opens up a new avenue for the design of quantum machine learning algorithms. We interpret the process of encoding inputs in a quantum state as a nonlinear feature map that maps data to quantum Hilbert space. A quantum computer can now analyze the input data in this feature space. Based on this link, we discuss two approaches for building a quantum model for classification. In the first approach, the quantum device estimates inner products of quantum states to compute a classically intractable kernel. The kernel can be fed into any classical kernel method such as a support vector machine. In the second approach, we use a variational quantum circuit as a linear model that classifies data explicitly in Hilbert space. We illustrate these ideas with a feature map based on squeezing in a continuous-variable system, and visualize the working principle with two-dimensional minibenchmark datasets.",
    "link": "https://www.semanticscholar.org/paper/7e7eb0f93c9550d7336f4bbfad5fe89604295705",
    "published": "2018-03-19"
  },
  "98": {
    "pdf_path": "data/pdfs\\machine learning_paper_258.pdf",
    "text_excerpt": "Machine Learning 2: 5-8, 1987\n© 1987 Kluwer Academic Publishers, Boston - Manufactured in The Netherlands\nEDITORIAL\nMachine Learning and Grammar Induction\nLanguage and the Acquisition of Syntax\nLanguage is a major component of cognition, and as such, its acquisition\nhas been a central concern of machine learning researchers. Some of the\nearliest AI learning work focused on this topic, and interest has continued\nto the present. However, progress in this area has been much slower than\nin most other learning tasks, undoubtedly due to the inherent complexity\nof natural language.\nDespite its complexity, the task of natural language processing can be\ndivided into a number of well-defined subtasks, and one of these centers on\nsyntax. This aspect of language has been studied in detail by linguists, and\ndevelopmental studies have provided a variety of empirical generalizations\nabout the stages that children traverse in their acquisition of grammar.\nBecause our knowledge of syntax is more complete than that for other\ncomponents of language, the vast majority of language-related research in\nmachine learning has focused on the task of grammar induction.\nTwo Views of Grammar Induction\nWithin this effort, two different paradigms have emerged for describ-\ning the grammar induction task. The first approach was formulated by\nSolomonoff (1959) and others in the early days of AI. It assumed only a\nset of legal sentences as input, from which the learner induced a grammar\nthat would parse those sentences. This approach was quite popular during\nthe 1960's, during which Gold (1967) and others formulated a number of\nformal results about the task. This paradigm has sometimes been called\ngrammatical inference.\nThe second approach did not appear until the late 1960's, when some\nresearchers noted that in natural languages, grammars were used for more\nthan simply parsing sentences - they also mapped sentences onto meaning\nstructures. This led to an alternative view of the grammar induction task",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "99": {
    "pdf_path": "data/pdfs\\machine learning_paper_259.pdf",
    "text_excerpt": "Machine Learning 1: 243-248, 1986\n© 1986 Kluwer Academic Publishers, Boston - Manufactured in The Netherlands\nEditorial: Human and Machine Learning\nThe goals of machine learning\nOne can identify a number of different themes within the machine learning\ncommunity, each corresponding to central goals of its parent field, artificial\nintelligence. For instance, many AI researchers are concerned with implementing\nknowledge-intensive systems, but these often take man-years to construct. Machine\nlearning may provide methods for automating this process, promising consider-\nable savings in time and effort. Similarly, many AI researchers view artificial\nintelligence as a scientific discipline rather than an engineering one, and hope to\nformulate general principles of intelligent behavior that hold across a variety of\ndomains. Since machine learning focuses on the acquisition of domain-specific\nknowledge rather than the knowledge itself, it holds considerable potential for such\ngeneral principles.\nStill other AI researchers are concerned with explaining human behavior,\nviewing AI techniques as an ideal tool for stating complex theories of the human\ninformation processing system. Since learning is a central phenomenon in human\ncognition, these researchers evaluate machine learning methods in terms of their\nability to explain human learning. Although this is a minority opinion within\nmachine learning, below we present some reasons why more members of the field\nshould take this approach seriously.\nScience as search\nOne of the central insights of AI is that intelligence involves search, and that\neffective search is constrained by domain-specific knowledge. This framework can\nbe applied to problem solving, language understanding, and learning from\nexperience. One can even apply this search metaphor to machine learning as a field\nof scientific study. In this framework, machine learning researchers are exploring a\nvast space of possible learning methods, searching for techniques with ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "100": {
    "pdf_path": "data/pdfs\\machine learning_paper_26.pdf",
    "text_excerpt": "Probabilistic Machine\nLearning for Healthcare\nIrene Y. Chen,1;\u0003Shalmali Joshi,2;\u0003Marzyeh\nGhassemi,2;3and Rajesh Ranganath4;5;6\n1Electrical Engineering and Computer Science, Massachusetts Institute of\nTechnology, Cambridge, MA 02139, USA; email: iychen@mit.edu\n2Vector Institute, Toronto, Ontario, Canada; email: shalmali@vectorinstitute.ai\n3Department of Computer Science, University of Toronto, Toronto, Ontario,\nCanada\n4Department of Computer Science, Courant Institute, New York University, New\nYork, New York 10012, USA\n5Center for Data Science, New York University, New York, New York 10012, USA\n6Department of Population Health, New York University Grossman School of\nMedicine, New York, New York 10012, USA\nXxxx. Xxx. Xxx. Xxx. 2021. AA:1{25\nhttps://doi.org/10.1146/((please add\narticle doi))\nCopyright c\r2021 by Annual Reviews.\nAll rights reservedKeywords\nprobabilistic modeling, health, electronic health records\nAbstract\nMachine learning can be used to make sense of healthcare data. Proba-\nbilistic machine learning models help provide a complete picture of ob-\nserved data in healthcare. In this review, we examine how probabilistic\nmachine learning can advance healthcare. We consider challenges in\nthe predictive model building pipeline where probabilistic models can\nbe bene\fcial including calibration and missing data. Beyond predictive\nmodels, we also investigate the utility of probabilistic machine learning\nmodels in phenotyping, in generative models for clinical use cases, and\nin reinforcement learning.\n1arXiv:2009.11087v1  [stat.ML]  23 Sep 2020Contents\n1. INTRODUCTION ............................................................................................ 2\n2. PROBABILISTIC MODELS ................................................................................. 3\n3. CHALLENGES IN BUILDING PREDICTIVE MODELS FOR MEDICINE .................................. 4\n3.1. Missing Values .........................................................................................",
    "title": "A method to benchmark high-dimensional process drift detection",
    "abstract": "Process curves are multivariate finite time series data coming from\nmanufacturing processes. This paper studies machine learning that detect drifts\nin process curve datasets. A theoretic framework to synthetically generate\nprocess curves in a controlled way is introduced in order to benchmark machine\nlearning algorithms for process drift detection. An evaluation score, called\nthe temporal area under the curve, is introduced, which allows to quantify how\nwell machine learning models unveil curves belonging to drift segments.\nFinally, a benchmark study comparing popular machine learning approaches on\nsynthetic data generated with the introduced framework is presented that shows\nthat existing algorithms often struggle with datasets containing multiple drift\nsegments.",
    "link": "http://arxiv.org/abs/2409.03669v2",
    "published": "2024-09-05T16:23:07Z"
  },
  "101": {
    "pdf_path": "data/pdfs\\machine learning_paper_260.pdf",
    "text_excerpt": "Machine Learning 2: 195 198, 1987\n© 1987 Kluwer Academic- Publishers, Boston Manufactured in The Netherlands\nEDITORIAL\nResearch Papers in Machine Learning\nA prototype for machine learning papers\nScientists do not work in isolation. Communication is central to the\nscientific enterprise, and much of the exchange occurs through research\npapers. Naturally, the content of these papers will vary with the field of\nstudy, and in this essay I discuss the information that machine learning\nresearchers should attempt to communicate in their articles. Readers may\ninterpret these comments as reflecting the current editorial policy of the\njournal Machine Learning, though this policy will undoubtedly evolve over\ntime, as even editors and editorial boards learn from experience.\nDespite the advantages of standardization, it can lead a new discipline\nto crystallize into a rigid pattern before its time. Rather than define an\ninflexible format that all papers in machine learning must follow, I provide\nbelow a more flexible, prototypical outline. Specific papers will diverge\nfrom this prototype, but it should provide a useful target, particularly\nfor authors new to the field. The outline assumes a paper that describes\na single, running machine learning system. Most papers that appear in\nMachine Learning will take this form, but later I will briefly consider some\nother possibilities.\n1. Goals of the research. Machine learning researchers have many different\nreasons for carrying out their work. Some are interested in general\nprinciples of intelligent behavior, others are concerned with modeling\nhuman learning, and still others are oriented towards applications. If\nan author hopes to communicate with his audience, it is essential that\nhe state his goals early in the paper, so that readers can decide for\nthemselves whether the work has made progress towards those goals.\n2. Description of the task. Learning always occurs in the context of some\nperformance task - such as classification, planni",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "102": {
    "pdf_path": "data/pdfs\\machine learning_paper_261.pdf",
    "text_excerpt": "Machin e Learning , 14, 305-31 2 (1994 )\n© 199 4 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands .\nExtended  Abstract\nMachin e Learnin g and Qualitativ e Reasonin g\nIVAN BRATK O\nLjubljana  University,  Faculty  of Electrical  Engineering  and  Computer  Science  and  J. Stefan  Institute,  Ljubl-\njana,  Slovenia\nEditor : D. Sleema n\nDate Submitted : 17 Novembe r 1992\n1. Qualitativ e representation s and ILP\nQualitativ e modelin g and reasonin g is a mos t interestin g area for applyin g and experi -\nmentin g with machin e learnin g techniques . Qualitativ e reasonin g task s of interes t wher e\nmachin e learnin g can be applie d includ e modeling , diagnosis , control , discovery , design ,\nand knowledg e compilation . This pape r review s example s of recen t researc h into som e\nof thes e application s areas . In particular , the example s give n illustrat e how Inductiv e\nLogic Programmin g (ILP ; Muggleto n 1990 , 1992 ) applie s naturall y to thes e task s whe n\nqualitativ e representation s are used .\nLet us first conside r the natur e of description s that we typicall y encounte r in qualitativ e\nreasoning . Conside r the proces s of fillin g a containe r with water . Tabl e 1 show s som e\nexample s of quantitativ e description s and their correspondin g qualitativ e abstractions . In\nrow (a) of the table , t1 and t2 denot e som e time point s that we kno w exist , but the\nexact time s are not given , zero  and top correspon d to two levels , 0 and the top of the\ncontainer , wher e we kno w that zero  and top, but the exac t valu e of top is not know n\nor given . In row (b) of the table , the qualitativ e descriptio n is read as \"Amount  is\nmonotonically  increasing  functio n of Level.\"\nTable  I. Quantitativ e and qualitativ e description s\n(a)\n(b)Quantitativ e\nTime\n3 sec.\n4 sec.\n5 sec.Level\n0.01 m\n0.23 m\n0.31 m\nAmount  = 2.5 * Level  + 0.7 * Level 2Qualitativ e\nLevel(between(t 1,t2)) =\n(between(zero,  top),  increasing)\nAmount  = M+",
    "title": "Machine Learning with a Reject Option: A survey",
    "abstract": "Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake. This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model's predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machine learning research areas.",
    "link": "https://www.semanticscholar.org/paper/24864a7f899718477c04ede9c0bea906c5dc2667",
    "published": "2021-07-23"
  },
  "103": {
    "pdf_path": "data/pdfs\\machine learning_paper_262.pdf",
    "text_excerpt": "Machine Learning 1: 141-144, 1986\n© 1986 Kluwer Academic Publishers, Boston - Manufactured in The Netherlands\nEditorial: The Terminology of Machine Learning\nScience is a communal endeavor, and communication is essential to its effective\noperation. Journals play an important role in this process, and the primary goal of\nMachine Learning is to improve communication between researchers in our emerging\nfield. One way to maximize this communication is to encourage clear and consistent\nterminology, and this is especially important in new fields like machine learning. An\nauthor's choice of words can have a major impact on his paper's ability to com-\nmunicate, and this makes terminology an important concern for both the authors and\nthe editors of any journal.\nThe fields of artificial intelligence and cognitive science have a long history of ter-\nminological disputes. Researchers in both fields have been criticized (both from\nwithin and without) for using semantically loaded terms. The pages of journals and\nproceedings abound with statements that an AI system understands natural\nlanguage, that it reasons about some problem, or even that it learns new concepts\nor discovers scientific laws.\nHowever, we should not forget that AI and cognitive science are unique fields.\nUnlike physics and chemistry, they study processes that can be observed through in-\ntrospection (at least to some extent) by all humans. As a result, our everyday\nlanguages already contain terms for most of the mechanisms that we are interested\nin explaining. Thus, it is natural for us to describe our systems as \"understanding\"\nor \"learning\", even when we realize that our current models of these processes only\nshadow the human versions. In contrast, \"hard\" scientists are free to invent entirely\nnew names for concepts, or to use existing terms (such as charm and color) in new\nsenses without fear of confusion.\nBut despite the criticisms, I would argue that AI and related fields are in little\ndanger from their use o",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "104": {
    "pdf_path": "data/pdfs\\machine learning_paper_263.pdf",
    "text_excerpt": "Machine Learning 5, 233-237 (1990)\n© 1990 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nEditorial\nAdvice to Machine Learning Authors\nThis editorial contains suggestions to authors of papers in the area of machine learning,\nalthough much of it applies to the broader field of artificial intelligence. I have distilled\nthese comments from my five-year experience as an editor of Machine Learning, focusing\non problems that tended to recur in different papers. Many comments are slanted toward\npapers that describe running systems, but others will be useful for different types of papers.\nAuthors should focus on those suggestions relevant to their own research emphasis.\nI have divided the suggestions into a number of categories, which should be self-explanatory.\nI expect most readers will agree with many of the points, but undoubtedly some will be\nmore controversial. Despite this, I believe that listing them explicitly in this manner will\nat least encourage authors to think about the issues before drafting their papers, and thus\nreduce the need for revisions at later dates.\nContent\n• State the goals of your research and the criteria by which readers should evaluate your\napproach. Categorize the paper in terms of some familiar class; e.g., a theoretical analysis,\na comparative experimental study, a description of some new learning algorithm, or a\ncomputational model of human learning.\n• Specify the performance and learning tasks that are the focus of your research, clearly\ndistinguishing between the two aspects. If there is no performance system, propose some\nother means of evaluating the learning behavior.\n• Describe the representation and organization of your system's knowledge, along with\nthe representation of training data. Include examples of each in the paper.\n• Explain both the performance and learning components of your system in enough detail\nthat readers can reimplement them. If possible, include both a pseudocode description\nand an extended ex",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "105": {
    "pdf_path": "data/pdfs\\machine learning_paper_264.pdf",
    "text_excerpt": "Machin e Learnin g 2: 99-102 , 1987\n© 1987 Kluwe r Academi c Publishers , Bosto n - Manufacture d in The Netherland s\nEDITORIA L\nMachin e Learnin g and Concep t Formatio n\nThe task of concep t formatio n\nThe vast majorit y of researc h in machin e learnin g has focuse d on the\nacquisitio n of concepts.  Ye t despit e this emphasis , littl e of the wor k has\nany direc t relevanc e to modelin g huma n concep t learning . For a variet y\nof reasons , machin e learnin g researcher s have tende d to mak e assumption s\nthat violat e our knowledg e abou t the representation , use, and acquisitio n\nof huma n concepts .\nFor instance , mos t AI researcher s have attempte d to describ e concept s\nin logica l terms , as the conjunctio n or disjunctio n of features . However ,\nwe know that man y huma n concept s canno t be describe d in such a logica l\nformalism . Som e instance s of a concep t are mor e typica l than others ; e.g.,\nrobin s are mor e typica l bird s than penguins . This woul d seem to requir e a\nmore flexibl e representation , combine d with some form of partia l matching .\nAnothe r commo n assumptio n is that concep t learnin g is supervised;  i.e.,\na benevolen t teache r is presen t to direc t the learnin g process . However , we\nknow that muc h huma n concep t learnin g is unsupervised.  Childre n form\nusefu l concept s long befor e they acquir e enoug h languag e to benefi t from a\ntutor' s advice , and they seem to accomplis h this throug h direc t interactio n\nwith thei r environment .\nResearcher s have also tende d to focu s on learnin g one or a few concept s\nat a singl e leve l of abstraction . However , we know that human s organize\ntheir concept s into larg e hierarchie s that describ e categorie s at varyin g\nlevels of specificity . Thus , model s of concep t learnin g shoul d addres s the\nformatio n of conceptua l hierarchie s as well as the formatio n of individua l\ncategories .\nFinally , muc h of the work on concep t acquisitio n has used nonincrem",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "106": {
    "pdf_path": "data/pdfs\\machine learning_paper_267.pdf",
    "text_excerpt": "Machin e Learnin g 2: 281-284 , 1988\n© 198 8 Kluwe r Academi c Publishers , Bosto n - Manufacture d in The Netherland s\nGUES T EDITORIA L\nNew Theoretica l Direction s in Machin e Learnin g\nLookin g back over the last decad e of work in machin e learning , one thin g\nis apparent : ther e is an art to designin g efficien t and effectiv e learnin g\nalgorithms . Too muc h of an art. Man y researcher s have argue d that if the\nfield is ever to fully mature , we must find a soun d and adequat e theoretica l\nbasis on whic h to buil d a scienc e of machin e learning .\nA learnin g algorith m is an algorithm . Althoug h it woul d be unwis e to\nsubsum e machin e learnin g entirel y unde r the genera l theor y of algorithm s\nand computationa l complexity , the technique s and perspective s from this\nfield are pertinen t to developin g a theoretica l basi s for machin e learning .\nThis specia l issu e of Machine  Learning  present s som e recen t progres s in\napplyin g thes e technique s and perspective s to the computationa l problem s\ninheren t in learning .\nEarly effort s alon g thes e line s were base d primaril y on the framewor k\nintroduce d by Gol d (1967) . Thi s earl y wor k is reviewe d in the excellen t\nsurve y by Anglui n and Smit h (1983) . Mor e recently , Valian t (1984 ) has\nintroduce d a probabilisti c framewor k for the stud y of learnin g algorithms .\nKearns , Li, Pitt , and Valian t (1987 ) and Haussle r (1987 ) hav e surveye d\napplication s of this framewor k to AI concept-learnin g problems ; Rives t\n(1987 ) has also carrie d out recen t wor k in this paradigm .\nThe Valian t framewor k has been mor e successfu l than the Gol d frame -\nwork in addressin g som e of the requirement s that are typicall y place d on a\nlearnin g algorith m in practice . To allow greate r emphasi s on computationa l\nefficiency , it require s only that a good approximatio n to the targe t concep t\nbe foun d with high probability , rathe r than requirin g exac t identificatio n\nof t",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "107": {
    "pdf_path": "data/pdfs\\machine learning_paper_268.pdf",
    "text_excerpt": "Machin e Learnin g 3: 5-8, 198 8\n© 1988 Kluwe r Academi c Publishers , Bosto n - Manufacture d in The Netherland s\nEDITORIA L\nMachin e Learnin g as an Experimenta l Scienc e\nThe role of experiment s in machin e learnin g\nMachin e learnin g is a scientifi c disciplin e and, like the field s of AI and com -\nputer science , has both theoretica l and empirica l aspects . Althoug h recen t\nprogres s has occurre d on the theoretica l fron t (see Machine  Learning,  volum e\n2, numbe r 4), mos t learnin g algorithm s are too comple x for forma l analysis .\nThus , the field promise s to hav e a significan t empirica l componen t for the\nforeseeabl e future . An d unlik e some empirica l sciences , machin e learnin g is\nfortunat e enoug h to have experimenta l contro l over a wide rang e of factors ,\nmakin g it mor e akin to physic s and chemistr y than astronom y or sociology .\nIn any science , the goal of experimentatio n is to bette r understan d a class of\nbehavior s and the condition s unde r whic h they occur . Ideally , this will lead to\nempirica l laws that can aid the proces s of theor y formation . In our field , the\ncentra l behavio r is learning , and the condition s involv e the algorith m employed ,\nthe domai n knowledge , and the environmen t in whic h learnin g occurs . An\nimplemente d learnin g algorith m is necessar y but not sufficient ; one shoul d\nalso attemp t to specify  whe n it operate s well and the reason s for that behavior.\nLackin g theoretica l evidence , experimentatio n is the natura l alternative .\nAs normall y defined , an experiment  involve s systematicall y varyin g one or\nmore independent  variable s and examinin g their effec t on some dependent  vari -\nables . Thus , a machin e learnin g experimen t require s mor e than a singl e learn -\ning run; it require s a numbe r of runs carrie d out unde r differen t conditions . In\neach case , one mus t measur e som e aspec t of the system' s behavio r for compar -\nison acros s the differen t co",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "108": {
    "pdf_path": "data/pdfs\\machine learning_paper_27.pdf",
    "text_excerpt": "Accepted as a paper at ICLR 2023 Workshop on Machine Learning for Remote Sensing\nEVALUATION CHALLENGES FOR GEOSPATIAL ML\nEsther Rolf\nHarvard University\nABSTRACT\nAs geospatial machine learning models and maps derived from their predictions\nare increasingly used for downstream analyses in science and policy, it is im-\nperative to evaluate their accuracy and applicability. Geospatial machine learn-\ning has key distinctions from other learning paradigms, and as such, the correct\nway to measure performance of spatial machine learning outputs has been a topic\nof debate. In this paper, I delineate unique challenges of model evaluation for\ngeospatial machine learning with global or remotely sensed datasets, culminating\nin concrete takeaways to improve evaluations of geospatial model performance.\n1 M OTIVATION\nGeospatial machine learning (ML), for example with remotely sensed data, is being used across\nconsequential domains, including public health (Nilsen et al., 2021; Draidi Areed et al., 2022) con-\nservation (Sofaer et al., 2019), food security (Nakalembe, 2018), and wealth estimation (Jean et al.,\n2016; Chi et al., 2022). By both their use and their very nature, geospatial predictions have a purpose\nbeyond model benchmarking; mapped data are to be read, scrutinized, and acted upon. Thus, it is\ncritical to rigorously and comprehensively evaluate how well a predicted map represents the state of\nthe world it is meant to reﬂect, or how well a spatial ML model performs across the many conditions\nin which it might be used.\nUnique structures in remotely sensed and geospatial data complicate or even invalidate use of tradi-\ntional ML evaluation procedures. Partially as a result of misunderstandings of these complications,\nthe stated performance of several geospatial models and predictive maps has come into question\n(Fourcade et al., 2018; Ploton et al., 2020). This in turn has sparked disagreement on what the\n“right” evaluation procedure is. With respect to a certain set of spat",
    "title": "Inverse Problems and Data Assimilation: A Machine Learning Approach",
    "abstract": "The aim of these notes is to demonstrate the potential for ideas in machine\nlearning to impact on the fields of inverse problems and data assimilation. The\nperspective is one that is primarily aimed at researchers from inverse problems\nand/or data assimilation who wish to see a mathematical presentation of machine\nlearning as it pertains to their fields. As a by-product, we include a succinct\nmathematical treatment of various topics in machine learning.",
    "link": "http://arxiv.org/abs/2410.10523v1",
    "published": "2024-10-14T14:01:35Z"
  },
  "109": {
    "pdf_path": "data/pdfs\\machine learning_paper_272.pdf",
    "text_excerpt": "Machin e Learnin g 3: 253-259 , 1989\n© 1989 Kluwe r Academi c Publisher s - Manufacture d in The Netherland s\nEDITORIA L\nTowar d a Unifie d Scienc e of Machin e Learnin g\nDiversificatio n and unificatio n\nMachin e learnin g is a divers e disciplin e that acts as host to a variet y of re-\nsearc h goals , learnin g techniques , and methodologica l approaches . Researcher s\nare makin g continua l progres s on all of these front s - tacklin g new problems ,\nformulatin g innovativ e solution s to thos e problems , and devisin g new ways to\nevaluat e their solutions . Suc h variet y is the sign of a health y and growin g field .\nHowever , diversificatio n also has its dangers . Subdiscipline s can emerg e\nthat focu s on one goal or evaluatio n schem e to the exclusio n of others , and\nsimilaritie s amon g method s can be obscure d by differen t notation s and termi -\nnology . Thus , it is equall y importan t to searc h for basic principle s that unif y\nthe differen t paradigm s withi n a field . Just as the twin force s of gravit y and\npressur e hold a star in dynami c equilibriu m whil e generatin g energy , so the\njoint processe s of diversificatio n and unificatio n can hold a scienc e togethe r\nwhile fosterin g progress .\nIn this editorial , I examin e seve n dichotomie s that have emerge d in recen t\nyears to partitio n the field of machin e learning . I begi n with thre e issue s\nrelate d to researc h goal s and evaluatio n methodologies , then turn to four mor e\nsubstantiv e issue s abou t learnin g method s themselves . In each case , I argu e\nthat long-ter m progres s will occu r only if we can find way s to unif y thes e\napparentl y competin g view s into a coheren t whole .\nAccurac y and efficienc y\nLearnin g involve s some chang e in performance, 1 and one of the main goal s\nof machin e learnin g is to develo p algorithm s that improv e their performanc e\nover time . However , ther e are man y differen t aspect s of performance . For\ninstance , early work on e",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "110": {
    "pdf_path": "data/pdfs\\machine learning_paper_276.pdf",
    "text_excerpt": "Machine Learning 1: 363-366, 1986\n© 1986 Kluwer Academic Publishers, Boston - Manufactured in The Netherlands\nEditorial: Machine Learning and Discovery\nDiscovery as learning\nIn everyday language, the terms learning and discovery convey rather different\nmeanings. The former suggests a gradual process, while the latter suggests a more\nrapid mental event, often involving some form of insight. Learning may lead to\nan unconscious change in knowledge, while one is always aware that a discovery\nhas been made. The result of learning can be declarative or procedural, while the\nproduct of discovery is always declarative. Learning often involves a transfer of\nknowledge from teacher to student; in contrast, discovery involves acquiring\nknowledge from the environment without the aid of a tutor. Finally, all humans\nand most animals learn from experience, but we reserve the term discovery for\nthe accomplishments of a select few. These boundaries are admittedly vague, but\nthey exist nonetheless.\nDespite the natural distinction between these concepts, the field of machine\nlearning has always viewed discovery as one of its concerns. Undoubtedly, one\nreason for this interest is that, like learning, discovery often involves induction —\nthe act of reasoning from specific facts or data to general rules or laws which\nprovide a general characterization of those facts. Another reason (probably\nrelated to the first) is that, historically, researchers in machine discovery have also\nworked on learning problems and have applied related techniques to these tasks.\nThus, contemporary machine learning researchers tend to view discovery as a\ndifficult form of 'learning from observation' (Carbonell, Michalski, & Mitchell,\n1983).\nThe historical role of discovery within machine learning, together with recent\nprogress in automated discovery, suggested the need for a special treatment of\nthat topic, and the current issue of Machine Learning is the result. The three\npapers in this issue are representative",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "111": {
    "pdf_path": "data/pdfs\\machine learning_paper_28.pdf",
    "text_excerpt": "A comprehensive review of Quantum Machine Learning:\nfrom NISQ to Fault Tolerance\nYunfei Wang\nMartin A. Fisher School of Physics, Brandeis University, Waltham MA 02453, USA\nE-mail: yunfeiwang0214@gmail.com\nJunyu Liu\nChicago Quantum Exchange, Chicago, IL 60637, USA\nDepartment of Computer Science, The University of Chicago, Chicago, IL 60637, USA\nPritzker School of Molecular Engineering, The University of Chicago, Chicago, IL 60637,\nUSA\nKadanoff Center for Theoretical Physics, The University of Chicago, Chicago, IL 60637,\nUSA\nE-mail: junyuliu@uchicago\nAbstract. Quantum machine learning, which involves running machine learning algorithms\non quantum devices, has garnered significant attention in both academic and business circles.\nIn this paper, we offer a comprehensive and unbiased review of the various concepts that have\nemerged in the field of quantum machine learning. This includes techniques used in Noisy\nIntermediate-Scale Quantum (NISQ) technologies and approaches for algorithms compatible\nwith fault-tolerant quantum computing hardware. Our review covers fundamental concepts,\nalgorithms, and the statistical learning theory pertinent to quantum machine learning.arXiv:2401.11351v2  [quant-ph]  31 Mar 2024CONTENTS 2\nContents\n1 Introduction 2\n2 Noisy intermediate-scale quantum (NISQ) era 6\n2.1 Variational quantum algorithm (VQA) . . . . . . . . . . . . . . . . . . . . . 7\n2.1.1 Objective function . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.1.2 Parameterized quantum circuits (PQC) . . . . . . . . . . . . . . . . 9\n2.1.3 Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.1.4 Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.2 Quantum neural tangent kernel . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.3 Quantum landscapes and barren plateaus . . . . . . . . . . . . . . . . . . . . 17\n2.3.1 Influence of background noise . . . . . . . . . . . . . . . . . . . . . 19\n2.3.2 Laziness . . . . . . . . . ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "112": {
    "pdf_path": "data/pdfs\\machine learning_paper_29.pdf",
    "text_excerpt": "Augmented Q Imitation Learning (AQIL)\nXiao Lei Zhang\nYork University\nToronto, Canada\nzhang205@cse.yorku.caAnish Agarwal\nUniversity of Waterloo\nWaterloo, Canada\na22agarw@outlook.com\nThe study of unsupervised learning can be generally divided\ninto  two  categories:  imitation  learning  and  reinforcement\nlearning.  In  imitation  learning  the  machine  learns  by\nmimicking  the  behavior  of  an  expert  system  whereas  in\nreinforcement  learning  the  machine  learns  via  direct\nenvironment  feedback.  Traditional  deep  reinforcement\nlearning takes a significant time before the machine starts to\nconverge to an optimal policy. This paper proposes Augmented\nQ-Imitation-Learning, a method by which deep reinforcement\nlearning  convergence  can  be  accelerated  by  applying  Q-\nimitation-learning as the initial training process in traditional\nDeep Q-learning.\nReference code:  https://github.com/veda-s4dhak/AQIL\nKeywords  —  Q-imitation-learning,  Deep  Reinforcement\nLearning, Deep Q-learning, Behavioral Cloning, SQIL\nI.INTRODUCTION\nImitation learning in deep learning systems is generally \neither focused on modeling the behavior of an expert system\n(behavioral cloning) or focused on modeling the reward \nfunction which best approximates the expert system’s \nbehavior (inverse reinforcement learning). \nThe performance of an imitation learning system alone is \nlimited by the performance of the expert player. In the ideal \nsense we want systems which can increase the upper bound \nof performance by going beyond that of an expert system. \nTo achieve this, imitation learning alone is not sufficient. \nWe are bounded by the limits of supervision.\nCurrent fully unsupervised deep learning systems which \nhave performance beyond known expert systems have been \ndesigned using reinforcement learning, where machines are \nlearning from direct environment interaction. Deep \nreinforcement learning[6] (DQN) however requires quite a bit\nof training period till the model reaches expert lev",
    "title": "Linear, Machine Learning and Probabilistic Approaches for Time Series\n  Analysis",
    "abstract": "In this paper we study different approaches for time series modeling. The\nforecasting approaches using linear models, ARIMA alpgorithm, XGBoost machine\nlearning algorithm are described. Results of different model combinations are\nshown. For probabilistic modeling the approaches using copulas and Bayesian\ninference are considered.",
    "link": "http://arxiv.org/abs/1703.01977v1",
    "published": "2017-02-26T10:41:26Z"
  },
  "113": {
    "pdf_path": "data/pdfs\\machine learning_paper_290.pdf",
    "text_excerpt": "Machine Learning, 39, 169–202, 2000.\nc°2000 Kluwer Academic Publishers. Printed in The Netherlands.\nMachine Learning for Information Extraction\nin Informal Domains\nDAYNE FREITAG dayne@justresearch.com\nJustsystem Pittsburgh Research Center, 4616 Henry Street, Pittsburgh, PA 15213, USA\nEditor:William Cohen\nAbstract. We consider the problem of learning to perform information extraction in domains where linguistic\nprocessing is problematic, such as Usenet posts, email, and ﬁnger plan ﬁles. In place of syntactic and semanticinformation, other sources of information can be used, such as term frequency, typography, formatting, andmark-up. We describe four learning approaches to this problem, each drawn from a different paradigm: a rotelearner, a term-space learner based on Naive Bayes, an approach using grammatical induction, and a relationalrule learner. Experiments on 14 information extraction problems deﬁned over four diverse document collectionsdemonstratetheeffectivenessoftheseapproaches.Finally,wedescribeamultistrategyapproachwhichcombinestheselearnersandyieldsperformancecompetitivewithorbetterthanthebestofthem.Thistechniqueismodularand ﬂexible, and could ﬁnd application in other machine learning problems.\nKeywords: information extraction, multistrategy learning\n1. Introduction\nIf I were in the market for a bargain computer, then I would beneﬁt from a program that\nmonitors newsgroups where computers are offered for sale until it ﬁnds a suitable one forme.Thedesignofsuchaprogramisessentiallyan informationextraction (IE)problem.We\nknowwhateachdocument(post)saysingeneralterms;itdescribesacomputer.Informationextraction is the problem of summarizing the essential details particular to a given docu-ment. An individual summary produced by our program will take the form of a templatewithtypedslots,inwhicheachslotisﬁlledbyafragmentoftextfromthedocument(e.g.,type: “Pentium”; speed: “200 MHz.”; disksize: “1.5Gig”; etc.).\nExisting work in IE can give us some good ideas about how",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "114": {
    "pdf_path": "data/pdfs\\machine learning_paper_298.pdf",
    "text_excerpt": "Machin e Learnin g 3: 95-99 , 1988\n© 1988 Kluwe r Academi c Publisher s - Manufacture d in The Netherland s\nGUES T EDITORIA L\nGeneti c Algorithm s and Machin e Learnin g\nMetaphor s for learnin g\nTher e is no a priori  reaso n why machin e learnin g mus t borro w from nature .\nA field coul d exist , complet e wit h well-define d algorithms , dat a structures ,\nand theorie s of learning , withou t once referrin g to organisms , cognitiv e or\ngeneti c structures , and psychologica l or evolutionar y theories . Yet at the end\nof the day, with the positio n paper s written , the computer s plugge d in, and\nthe program s debugged , a learnin g edific e devoi d of natura l metapho r woul d\nlack something . It woul d ignor e the fact that all thes e creation s hav e becom e\npossibl e only afte r thre e billio n year s of evolutio n on this planet . It woul d\nmiss the poin t tha t the very idea s of adaptatio n and learnin g are concept s\ninvente d by the mos t recen t representative s of the specie s Homo  sapiens  from\nthe carefu l observatio n of themselve s and life aroun d them . It woul d miss the\npoint that natura l example s of learnin g and adaptatio n are treasur e trove s of\nrobus t procedure s and structures .\nFortunately , the field of machin e learnin g does rely upo n nature' s bount y\nfor bot h inspiratio n and mechanism . Man y machin e learnin g system s now\nborro w heavil y from curren t thinkin g in cognitiv e science , and rekindle d in-\nteres t in neura l network s and connectionis m is evidenc e of seriou s mechanisti c\nand philosophica l current s runnin g throug h the field . Anothe r area wher e nat-\nural exampl e has bee n tappe d is in wor k on genetic  algorithms  (GAs ) and\ngenetics-base d machin e learning . Roote d in the earl y cybernetic s movemen t\n(Holland , 1962) , progres s has been mad e in both theor y (Holland , 1975 ; Hol-\nland, Holyoak , Nisbett , & Thagard , 1986 ) and applicatio n (Goldberg , 1989 ;\nGrefenstette , 1985 , 1987 ) to",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "115": {
    "pdf_path": "data/pdfs\\machine learning_paper_3.pdf",
    "text_excerpt": "arXiv:1707.04849v1  [cs.LG]  16 Jul 2017Minimax deviation strategies for machine\nlearning and recognition with short learning\nsamples\nSchlesinger M.I. and Vodolazskiy E.V.\nAugust 31, 2018\nAbstract\nThe article is devoted to the problem of small learning sampl es in\nmachine learning. Theﬂaws of maximum likelihood learning a nd min-\nimax learning are looked into and the concept of minimax devi ation\nlearning is introduced that is free of those ﬂaws.\n1 Introduction\nThe small learning sample problem has been around in machine learning\nunder diﬀerent names during its whole life. The learning sample is used t o\ncompensate for the lack of knowledge about the recognized objec t when its\nstatistical model is not completely known. Naturally, the longer the learning\nsample is, the better is the subsequent recognition. However, whe n the learn-\ning sample becomes too small (2, 3, 5 elements) an eﬀect of small sam ples\nbecomes evident. In spite of the fact that any learning sample (eve n a very\nsmall one) provides some additional information about the object, it may be\nbetter to ignore the learning sample than to utilize it with the commonly\nused methods.\nExample 1. Let us consider an object that can be in one of two random\nstatesy= 1 and y= 2 with equal probabilities. In each state the object\ngenerates two independent Gaussian randomsignals x1andx2with variances\nequal 1. Mean values of signals depend on the state as it is shown on F ig.\n11. In the ﬁrst state the mean value is (2 ,0). In the second state the mean\nvalue depends on an unknown parameter θand is (0,θ). Even if no learning\nsample is given a minimax strategy can be used to make a decision about the\nstatey. The minimax strategy ignores the second signal and makes decision\ny∗= 1 when x1>1 and decision y∗= 2 when x1≤1.\nx2x1\n/Bullet /Bullet\nθ/Bullet\n2\n0y∗= 1\ny∗= 2p(x1,x2|y= 1)\np(x1,x2|y= 2)\nFigure 1: Example 1. ( x1,x2)∈R2– signal, y∈ {1,2}– state.\nNow let us assume that there is a sample of signals generated by an\nobjecti",
    "title": "Minimax deviation strategies for machine learning and recognition with\n  short learning samples",
    "abstract": "The article is devoted to the problem of small learning samples in machine\nlearning. The flaws of maximum likelihood learning and minimax learning are\nlooked into and the concept of minimax deviation learning is introduced that is\nfree of those flaws.",
    "link": "http://arxiv.org/abs/1707.04849v1",
    "published": "2017-07-16T09:15:08Z"
  },
  "116": {
    "pdf_path": "data/pdfs\\machine learning_paper_30.pdf",
    "text_excerpt": "Article\nTowards CRISP-ML(Q): A Machine Learning Process Model\nwith Quality Assurance Methodology\nStefan Studer1,∗\n, Thanh Binh Bui2,∗, Christian Drescher1, Alexander Hanuschkin3, Ludwig Winkler2,\nSteven Peters1\nand Klaus-Robert Müller4\n/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045 /gid00001\n/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046\nCitation: Studer, S.; Bui, T.B.;\nDrescher, C.; Hanuschkin, A.;\nWinkler, L.; Peters, S.; Müller,\nK.-R.; Towards CRISP-ML(Q): A\nMachine Learning Process Model\nwith Quality Assurance\nMethodology. Preprints 2021,1,\n0. https://doi.org/\nReceived: 1 March 2021\nAccepted:\nPublished:\nPublisher’s Note: MDPI stays\nneutral with regard to jurisdic-\ntional claims in published maps\nand institutional aﬃliations.1Mercedes-Benz AG, Group Research, Artiﬁcial Intelligence Research, 71059 Sindelﬁngen, Germany;\nstefan.studer@daimler.com (S.St.); christian.d.drescher@daimler.com (C.D.); steven.peters@daimler.com\n(S.P.)\n2Machine Learning Group, Technische Universität Berlin, 10587 Berlin, Germany; bui@tu-berlin.de\n(T.B.B.); winkler@tu-berlin.de (L.W.)\n3Mercedes-Benz AG, Group Research, Artiﬁcial Intelligence Research, Sindelﬁngen, Germany and Esslingen\nUniversity of Applied Sciences, Germany; alexander.hanuschkin@daimler.com (A.H.)\n4Machine Learning Group, Technische Universität Berlin, 10587 Berlin, Germany and Google Research,\nBrain team, Berlin, Germany and Dept. of Artiﬁcial Intelligence, Korea University, Seoul 136-713, South\nKorea and Max-Planck-Institut für Informatik, 66123 Saarbrücken, Germany;\nklaus.r.mueller@googlemail.com (K.-R.M.)\n*Correspondence: stefan.studer@daimler.com (S.St.) and bui@tu-berlin.de (T.B.B.); equal contribution\nAbstract: Machine learning is an established and frequently used technique in industry and academia\nbut a standard process model to improve success and eﬃciency of machine learning applications\nis still missing. Project organizations and machine learning practitioners",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "117": {
    "pdf_path": "data/pdfs\\machine learning_paper_305.pdf",
    "text_excerpt": "Machine Learning, 19, 29-43 (1995)\n© 1995 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nMonotonicity Maintenance in Information-Theoretic\nMachine Learning Algorithms\nARIE BEN-DAVID msariebd@pluto.mscc.huji.ac.il\nManagement Information Systems, School of Business Administration, The Hebrew University of Jerusalem, Mount\nScopus, Jerusalem 91905, Israel\nEditor: Paul Utgoff\nAbstract. Decision trees that are based on information-theory are useful paradigms for learning from exam-\nples. However, in some real-world applications, known information-theoretic methods frequently generate non-\nmonotonic decision trees, in which objects with better attribute values are sometimes classified to lower classes\nthan objects with inferior values. This property is undesirable for problem solving in many application domains,\nsuch as credit scoring and insurance premium determination, where monotonicity of subsequent classifications\nis important. An attribute-selection metric is proposed here that takes both the error as well as monotonicity\ninto account while building decision trees. The metric is empirically shown capable of significantly reducing the\ndegree of non-monotonicity of decision trees without sacrificing their inductive accuracy.\nKeywords: information theory, monotonic decision trees, consistency, accuracy, monotonic classification\nproblems\n1 Introduction\nSuppose a college admissions committee decides to use decision trees to determine whom\nto admit based on standardized test scores and grades. For reasons such as fairness and\nliability, the college would not want to use a decision tree that admits an applicant with\ncertain scores, and then rejects another who scores as high or higher on each measure.\nSimilarly, a life insurance company would not wish to rely on a decision tree that quotes a\nyoung and healthy applicant a higher premium rate than one that has been quoted to an old\nand unhealthy person.\nThe classifications in both the school admissions a",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "118": {
    "pdf_path": "data/pdfs\\machine learning_paper_31.pdf",
    "text_excerpt": "\nAbstract—Recentworksonrecurrentneuralnetwork\nanddeeplearningarchitecturehaveshownthepowerof\ndeeplearninginmodelingtimedependentinput\nsequences.Specificlearningstructuresuchas\nHigher-orderboltzmannmachine,gradient-based\nlearningmanifold,andRecurrent“grammarcell”reveal\ntheirabilitytolearnfeaturetransformationbetween\nrelatedinputmaps,andperformwellintime-related\nlearning&predictiontasksinhigherordercases.Inthis\narticle, we extend the conventional\nconvolutional-Restricted-Boltzmann-Machinetolearn\nhighlyabstractfeaturesamongabitrarynumberoftime\nrelatedinputmapsbyconstructingalayerof\nmultiplicativeunits,whichcapturetherelationsamong\ninputs.Insomecases,weonlycareabouthowonemap\ntransformsintoanother,sothemultiplicativeunittakes\nfeaturesfromonlythistwomaps.Inothercases,however,\nmorethantwomapsarestronglyrelated,soitis\nreasonabletomakemultiplicativeunitlearnrelations\namongmoreinputmaps,inotherwords,tofindthe\noptimalrelational-order(numberofrelatedinputmaps\nthatthemultiplicativeunitextractsfeaturesfrom)of\neachunit.Inordertoenableourmachinetolearn\nrelationalorder,wedevelopedareinforcement-learning\nmethodwhoseoptimalityisproventotrainthenetwork.\nKeywords:ArtificialNeuralNetwork;\nConvolutional-Restricted-Boltzmann-Machine;\nReinforcementlearning; Deeplearning;\nTemporal-related;Relational-order\nⅠ.INTRODUCTION\nUnsupervisedlearningcombinedwithdeeparchitecture\nhasunveiledpartofthemysteryofartificialintelligence.\nSuchlearningtechniqueshaveboarderapplicationsinareas\nlikevisualrecognition,naturallanguageprocessing,audio\ndetection,andcognitiveanalysis.Withgrowing\ncomputationalcapabilities,deeplearningframeworklike\nconvolutionaldeepbeliefnetwork[1]canbringmore\ncontributiontocognitivescience.Recently,researchersbegin\ntotakethenextstep,tryingtodevelopmodelthatcanhandle\nManuscriptreceivedOctober16,2016\nZizhuangWangiswithXiaoganSeniorHighSchool,China\n(email:1012125144@qq.com;web:http://kingofspace0wzz.github.io/)timedependentlearningtasks.Traditionally,recurrentneural\nnetwork(RNN)hasshownits",
    "title": "Classifying medical notes into standard disease codes using Machine\n  Learning",
    "abstract": "We investigate the automatic classification of patient discharge notes into\nstandard disease labels. We find that Convolutional Neural Networks with\nAttention outperform previous algorithms used in this task, and suggest further\nareas for improvement.",
    "link": "http://arxiv.org/abs/1802.00382v1",
    "published": "2018-02-01T16:46:00Z"
  },
  "119": {
    "pdf_path": "data/pdfs\\machine learning_paper_313.pdf",
    "text_excerpt": "Machine Learning, 28, 41–75 (1997)\nc°1997 Kluwer Academic Publishers. Manufactured in The Netherlands.\nMultitask Learning*\nRICH CARUANA caruana@cs.cmu.edu\nSchool of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213\nEditors: Lorien Pratt and Sebastian Thrun\nAbstract. Multitask Learning is an approach to inductive transfer that improves generalization by using the\ndomaininformationcontainedinthetrainingsignalsof relatedtasksasaninductivebias. Itdoesthisbylearning\ntasksinparallelwhileusingasharedrepresentation;whatislearnedforeachtaskcanhelpothertasksbelearnedbetter. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers taskrelatednesswithouttheneedofsupervisorysignals,andpresentsnewresultsforMTLwithk-nearestneighborandkernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitasklearningworks,andshowthattherearemanyopportunitiesformultitasklearninginrealdomains. Wepresentanalgorithmandresultsformultitasklearningwithcase-basedmethodslikek-nearestneighborandkernelregression,andsketchanalgorithmformultitasklearningindecisiontrees. Becausemultitasklearningworks,canbeappliedto many different kinds of domains, and can be used with different learning algorithms, we conjecture there willbe many opportunities for its use on real-world problems.\nKeywords: inductive transfer, parallel transfer, multitask learning, backpropagation, k-nearest neighbor, kernel\nregression, supervised learning, generalization\n1. Introduction\n1.1. OverviewMultitask Learning (MTL) is an inductive transfer mechanism whose principle goal is\nto improve generalization performance. MTL improves generalization by leveraging thedomain-speciﬁcinformationcontainedinthetrainingsignalsof relatedtasks. Itdoesthisby\ntrainingtasksinparallelwhileusingasharedrepresentation. Ineffect,thetrainingsignalsfor the extra tasks serve as an inductive bias. Section 1.2 argues that inductive transfer isimportantifwewishtosca",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "120": {
    "pdf_path": "data/pdfs\\machine learning_paper_32.pdf",
    "text_excerpt": "Spatial Neural Network with Transfer\nLearning\nHongjian Yang\nDepartment of Statistics, North Carolina State UniversityarXiv:2405.03720v1  [cs.LG]  5 May 20241 Introduction\nSpatial data is ubiquitous, encompassing a wide range of applications from environmental\nobservations and biological measurements to more recent fields like computer vision. A crit-\nical challenge in the analysis of spatial data is spatial prediction, which involves estimating\nunobserved values based on nearby observations under the assumption of certain correla-\ntions. Among parametric algorithms, Kriging is particularly notable ((Matheron (1963))).\nDescribed as the best linear unbiased estimator (BLUE), Kriging employs a weighted average\nof nearby observations, with weights determined by a covariance function typically presumed\nto be stationary. However, this assumption does not hold in many real-world scenarios, such\nas data from satellites, monitoring stations, and urban streets, which tend to exhibit non-\nstationarity (Katzfuss (2013)). Moreover, Kriging faces computational challenges with large\ndatasets, requiring the inversion of the covariance matrix, an operation with a computational\ncomplexity of O(N3) (Chen et al. (2020)).\nA variety of algorithms have been proposed to address the issues highlighted above. Mao\net al. (2022) introduced a model-free method for handling non-stationary spatial data, pre-\nsenting a significant advancement in the field. Another popular alternative to Kriging is the\nVecchia approximation, which simplifies the full Gaussian distribution by conditioning on\nneighboring values (Vecchia (1988)). Drawing on the concept of neighbor-based approxima-\ntion, Wang et al. (2019) developed a nearest neighbor neural network specifically for spatial\nprediction. Building on these ideas, Chen et al. (2020) introduced the innovative Deep Krig-\ning framework, which combines spatial basis functions with deep neural networks to model\nany spatial processes effectively.\nWhile deep lear",
    "title": "Mixed integer programming formulation of unsupervised learning",
    "abstract": "A novel formulation and training procedure for full Boltzmann machines in\nterms of a mixed binary quadratic feasibility problem is given. As a proof of\nconcept, the theory is analytically and numerically tested on XOR patterns.",
    "link": "http://arxiv.org/abs/2001.07278v1",
    "published": "2020-01-20T23:09:32Z"
  },
  "121": {
    "pdf_path": "data/pdfs\\machine learning_paper_322.pdf",
    "text_excerpt": "Machine Learning, 39, 59–91, 2000.\nc°2000 Kluwer Academic Publishers. Printed in The Netherlands.\nA Machine Learning Approach to POS Tagging\nLLU´IS M`ARQUEZ lluism@lsi.upc.es\nLLU´IS PADR ´O padro@lsi.upc.es\nHORACIO RODR ´IGUEZ horacio@lsi.upc.es\nDepartament de Llenguatges i Sistemes Inform `atics, Universitat Polit `ecnica de Catalunya, c/ Jordi Girona 1–3.\nBarcelona 08034, Catalonia\nEditor:Raymond Mooney\nAbstract. WehaveappliedtheinductivelearningofstatisticaldecisiontreesandrelaxationlabelingtotheNatural\nLanguage Processing ( NLP) task of morphosyntactic disambiguation (Part Of Speech Tagging). The learning\nprocess is supervised and obtains a language model oriented to resolve POSambiguities, consisting of a set of\nstatisticaldecisiontreesexpressingdistributionoftagsandwordsinsomerelevantcontexts.Theacquireddecisiontrees have been directly used in a tagger that is both relatively simple and fast, and which has been tested andevaluated on the Wall Street Journal (\nWSJ) corpus with competitive accuracy. However, better results can be\nobtained by translating the trees into rules to feed a ﬂexible relaxation labeling based tagger. In this directionwe describe a tagger which is able to use information of any kind ( n-grams, automatically acquired constraints,\nlinguistically motivated manually written constraints, etc.), and in particular to incorporate the machine-learneddecisiontrees.Simultaneously,weaddresstheproblemoftaggingwhenonlylimitedtrainingmaterialisavailable,which is crucial in any process of constructing, from scratch, an annotated corpus. We show that high levels ofaccuracy can be achieved with our system in this situation, and report some results obtained when using it todevelop a 5.5 million words Spanish corpus from scratch.\nKeywords: part of speech tagging, corpus-based and statistical language modeling, decision trees induction,\nconstraint satisfaction, relaxation labeling\n1. Introduction\nPartofSpeech( POS)TaggingisaverybasicandwellknownNaturalLanguag",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "122": {
    "pdf_path": "data/pdfs\\machine learning_paper_326.pdf",
    "text_excerpt": "Machin e Learning , 4, 251-25 4 (1989 )\n© 1989 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands .\nCan Machin e Learnin g Offe r Anythin g\nto Exper t Systems ?\nBRUC E G. BUCHANA N\nProfessor  of Computer  Science,  Medicine,  and Philosophy,  University  of Pittsburgh,  Pittsburgh,  PA 15260\nToday' s exper t system s have no abilit y to learn from experience . This commonl y hear d crit-\nicism , unfortunately , is largel y true. Excep t for simpl e classificatio n systems , exper t system s\ndo not emplo y a learnin g componen t to construc t part s of their knowledg e base s from\nlibrarie s of previousl y solve d cases . And none that I know of couple s learnin g into closed -\nloop modificatio n base d on experience , althoug h the SOA R architectur e [Rosenbloo m and\nNewel l 1985 ] come s the closes t to bein g the sort of integrate d syste m neede d for continuou s\nlearning . Learnin g capabilitie s are neede d for intelligen t system s that can remai n usefu l\nin the face of changin g environment s or changin g standard s of expertise . Why are the learn -\ning method s we know how to implemen t not bein g used to build or maintai n exper t system s\nin the commercia l world ?\nPart of the answe r lies in the syntacti c view we have traditionall y take n towar d learning .\nStatistica l technique s such as linea r regression , for example , are \"knowledge-poor \" proce -\ndures that are unabl e to use knowledg e we may bring to the learnin g task. However , learnin g\nis a problem-solvin g activity . As such , we can and shoul d analyz e the requirement s and\nfunctiona l specifications , the inpu t and output , and the assumption s and limitations . If there\nis one thin g we have learne d in AI it is that complex , i.e., combinatoriall y explosiv e and\nill-structured , problem s ofte n can be tame d by introducin g knowledg e into an otherwis e\nsyntacti c procedure .\nThere is not a singl e mode l for learnin g effectively , as is confirme d by the ",
    "title": "Machine learning for molecular simulation",
    "abstract": "Machine learning (ML) is transforming all areas of science. The complex and time-consuming calculations in molecular simulations are particularly suitable for an ML revolution and have already been profoundly affected by the application of existing ML methods. Here we review recent ML methods for molecular simulation, with particular focus on (deep) neural networks for the prediction of quantum-mechanical energies and forces, on coarse-grained molecular dynamics, on the extraction of free energy surfaces and kinetics, and on generative network approaches to sample molecular equilibrium structures and compute thermodynamics. To explain these methods and illustrate open methodological problems, we review some important principles of molecular physics and describe how they can be incorporated into ML structures. Finally, we identify and describe a list of open challenges for the interface between ML and molecular simulation. Expected final online publication date for the Annual Review of Physical Chemistry, Volume 71 is April 20, 2020. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",
    "link": "https://www.semanticscholar.org/paper/b674a7aee72e9b9cc5390eca13f9c5c7812f2ba0",
    "published": "2019-11-07"
  },
  "123": {
    "pdf_path": "data/pdfs\\machine learning_paper_34.pdf",
    "text_excerpt": "arXiv:1603.02185v1  [cs.LG]  7 Mar 2016Distributed Multi-Task Learning with Shared Representati on\nJialei Wang\nDepartment of Computer Science\nUniversity of Chicago\nChicago, IL 60637Mladen Kolar\nBooth School of Business\nUniversity of Chicago\nChicago, IL 60637Nathan Srebro\nToyota Technological Institute\nat Chicago\nChicago, IL 60637\nAbstract\nWe study the problem of distributed multi-\ntask learning with shared representation,\nwhere each machine aims to learn a sepa-\nrate, but related, task in an unknown shared\nlow-dimensionalsubspaces, i.e. when the pre-\ndictor matrix has low rank. We consider a\nsetting where each task is handled by a dif-\nferent machine, with samples for the task\navailable locally on the machine, and study\ncommunication-eﬃcient methods for exploit-\ning the shared structure.\n1 Introduction\nMulti-task learning is widely used learning framework\nin which similar tasks are considered jointly for the\npurpose of improving performance compared to learn-\ning the tasks separately [13]. By transferring informa-\ntion between relatedtasksit is hoped that sampleswill\nbe better utilized, leading to improved generalization\nperformance. Multi-task learning has been success-\nfully applied, for example, in natural language under-\nstanding [15], speech recognition [32], remote sensing\n[44], image classiﬁcation [25], spam ﬁltering [43], web\nsearch[14], diseaseprediction[49], and eQTLmapping\n[23] among other applications.\nHere, westudy multi-tasklearningin adistributedset-\nting, where each task is handled by a diﬀerent ma-\nchine and communication between machines is expen-\nsive. That is, each machine has access to data for a\ndiﬀerent task and needs to learn a predictor for that\ntask, where machines communicate with each other in\norder to leverage the relationship between the tasks.\nThis situation lies between a homogeneous distributed\nlearning setting [e.g. 36], where all machines have data\nfrom the same sourcedistribution, and inhomogeneousconsensus problems [e.g. 30, 11, ",
    "title": "Elements of effective machine learning datasets in astronomy",
    "abstract": "In this work, we identify elements of effective machine learning datasets in\nastronomy and present suggestions for their design and creation. Machine\nlearning has become an increasingly important tool for analyzing and\nunderstanding the large-scale flood of data in astronomy. To take advantage of\nthese tools, datasets are required for training and testing. However, building\nmachine learning datasets for astronomy can be challenging. Astronomical data\nis collected from instruments built to explore science questions in a\ntraditional fashion rather than to conduct machine learning. Thus, it is often\nthe case that raw data, or even downstream processed data is not in a form\namenable to machine learning. We explore the construction of machine learning\ndatasets and we ask: what elements define effective machine learning datasets?\nWe define effective machine learning datasets in astronomy to be formed with\nwell-defined data points, structure, and metadata. We discuss why these\nelements are important for astronomical applications and ways to put them in\npractice. We posit that these qualities not only make the data suitable for\nmachine learning, they also help to foster usable, reusable, and replicable\nscience practices.",
    "link": "http://arxiv.org/abs/2211.14401v2",
    "published": "2022-11-25T23:37:24Z"
  },
  "124": {
    "pdf_path": "data/pdfs\\machine learning_paper_341.pdf",
    "text_excerpt": "Machine Learning, 38, 9–40, 2000.\nc°2000 Kluwer Academic Publishers. Printed in The Netherlands.\nLEARNABLE EVOLUTION MODEL:\nEvolutionary Processes Guidedby Machine Learning\nRYSZARD S. MICHALSKI michalski@gmu.edu\nMachine Learning and Inference Laboratory, George Mason University, Fairfax, VA andInstitute of Computer Science, Polish Academy of Sciences, Warsaw, Poland\nEditors: Floriana Esposito & Lorenza Saitta\nAbstract. Anewclassofevolutionarycomputationprocessesispresented,called LearnableEvolutionModel or\nLEM.IncontrasttoDarwinian-typeevolutionthatreliesonmutation,recombination,andselectionoperators,LEM\nemploysmachinelearningtogeneratenewpopulations.Speciﬁcally,in MachineLearningmode ,alearningsystem\nseeks reasons why certain individuals in a population (or a collection of past populations) are superior to othersinperformingadesignatedclassoftasks.Thesereasons,expressedasinductivehypotheses,areusedtogeneratenew populations. A remarkable property of LEM is that it is capable of quantum leaps (“insight jumps”) of theﬁtnessfunction,unlikeDarwinian-typeevolutionthattypicallyproceedsthroughnumerousslightimprovements.In our early experimental studies, LEM signiﬁcantly outperformed evolutionary computation methods used inthe experiments, sometimes achieving speed-ups of two or more orders of magnitude in terms of the number ofevolutionarysteps.LEMhasapotentialforawiderangeofapplications,inparticular,insuchdomainsascomplexoptimization or search problems, engineering design, drug design, evolvable hardware, software engineering,economics, data mining, and automatic programming.\nKeywords: multistrategy learning, genetic algorithms, evolution model, evolutionary computation\n1. Introduction\nRecent years have witnessed signiﬁcant progress in the development and applications of\nmachine learning methods, in particular, in scaling them up to cope with large datasets(e.g., Clark & Niblett, 1989; Cohen, 1995; Dietterich, 1997; Mitchell, 1997; Michalski,Bratko,&Kubat,1998).Therehasal",
    "title": "Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data",
    "abstract": "A promise of machine learning in health care is the avoidance of biases in diagnosis and treatment; a computer algorithm could objectively synthesize and interpret the data in the medical record. Integration of machine learning with clinical decision support tools, such as computerized alerts or diagnostic support, may offer physicians and others who provide health care targeted and timely information that can improve clinical decisions. Machine learning algorithms, however, may also be subject to biases. The biases include those related to missing data and patients not identified by algorithms, sample size and underestimation, and misclassification and measurement error. There is concern that biases and deficiencies in the data used by machine learning algorithms may contribute to socioeconomic disparities in health care. This Special Communication outlines the potential biases that may be introduced into machine learning–based clinical decision support tools that use electronic health record data and proposes potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful. Existing health care disparities should not be amplified by thoughtless or excessive reliance on machines.",
    "link": "https://www.semanticscholar.org/paper/161ce338538f94b0b9be51ae2336db0aa4b012e5",
    "published": "2018-11-01"
  },
  "125": {
    "pdf_path": "data/pdfs\\machine learning_paper_344.pdf",
    "text_excerpt": "Machine Learning, 30, 127–132 (1998)\nc°1998 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nGuest Editors’ Introduction:\nOn Applied Research in Machine Learning\nFOSTER PROVOST provost@acm.org\nBell Atlantic Science and Technology, 400 Westchester Avenue, White Plains, New York 10604\nRON KOHAVI ronnyk@sgi.com\nData Mining and Visualization, Silicon Graphics Inc., 2011 N. Shoreline Blvd, Mountain View, CA. 94043\nCommon arguments for including applications papers in the Machine Learning literature\nareoftenbasedonthepapers’valueforadvertisingsuccessstoriesandforboostingmorale.Forexample,high-proﬁleapplicationscanhelptosecurefundingforfutureresearchandcanhelptoattracthighcaliberstudents. However,thereisanotherreasonwhysuchpapersareofvaluetotheﬁeld,whichis,arguably,evenmorevital. Applicationpapersare essentialin\norder for Machine Learning to remain a viable science. They focus research on importantunsolved problems that currently restrict the practical applicability of machine learningmethods.\nMuchofthe“science”ofMachineLearningisascienceofengineering.\n1Bythiswemean\nthat it is dedicated to creating and compiling veriﬁable knowledge related to the designand construction of artifacts. The scientiﬁc knowledge comprises theoretical arguments,observationalcategorizations,empiricalstudies,andpracticaldemonstrations. Theartifactsare computer programs that use data to build models that are practically or theoreticallyuseful. Becausetheobjectsofstudyareintendedtohavepracticalutility,itisessentialforresearch activities to be focused (in part) on the elimination of obstacles that impede theirpractical application.\nMostoftentheseobstaclestaketheformofrestrictivesimplifyingassumptionscommonly\nmade in research. Consider as an example the assumption, common in classiﬁer learningresearch, that misclassiﬁcation errors have equal costs. The vast majority of classiﬁerlearningresearchinMachineLearninghasbeenconductedunderthisassumption,throughthe use of classiﬁcation accur",
    "title": "Predicting Diabetes Mellitus With Machine Learning Techniques",
    "abstract": "Diabetes mellitus is a chronic disease characterized by hyperglycemia. It may cause many complications. According to the growing morbidity in recent years, in 2040, the world’s diabetic patients will reach 642 million, which means that one of the ten adults in the future is suffering from diabetes. There is no doubt that this alarming figure needs great attention. With the rapid development of machine learning, machine learning has been applied to many aspects of medical health. In this study, we used decision tree, random forest and neural network to predict diabetes mellitus. The dataset is the hospital physical examination data in Luzhou, China. It contains 14 attributes. In this study, five-fold cross validation was used to examine the models. In order to verity the universal applicability of the methods, we chose some methods that have the better performance to conduct independent test experiments. We randomly selected 68994 healthy people and diabetic patients’ data, respectively as training set. Due to the data unbalance, we randomly extracted 5 times data. And the result is the average of these five experiments. In this study, we used principal component analysis (PCA) and minimum redundancy maximum relevance (mRMR) to reduce the dimensionality. The results showed that prediction with random forest could reach the highest accuracy (ACC = 0.8084) when all the attributes were used.",
    "link": "https://www.semanticscholar.org/paper/5327bb691a1c63791a06de2d3f0478e47785add5",
    "published": "2018-11-06"
  },
  "126": {
    "pdf_path": "data/pdfs\\machine learning_paper_346.pdf",
    "text_excerpt": "Machin e Learning , 18, 109-114(1995 )\n© 1995 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands .\nResearch  Note\nClassificatio n Accuracy : Machin e Learnin g\nvs. Explici t Knowledg e Acquisitio n\nARIE BEN-DAVI D AN D JANIC E MANDE L msariebd@pluto.mscc.huji.ac.i l\nManagement  Information  Systems,  The Hebrew  University  of Jerusalem,  Mount  Scopus,  Jerusalem  91905  Israel\nEditor : Bruc e Porte r\nAbstract . This empirica l stud y provide s evidenc e that machin e learnin g model s can provid e bette r classificatio n\naccurac y than explici t knowledg e acquisitio n techniques . Th e finding s sugges t that the mai n contributio n of\nmachin e learnin g to exper t system s is not just cost reduction , but rathe r the provisio n of tools for the developmen t\nof bette r exper t systems .\nKeywords : Exper t systems , machin e learning , explici t vs. implici t knowledg e acquisition , classificatio n accurac y\n1. Introductio n\nCan machin e learnin g offer anythin g to exper t systems ? B.G . Buchana n (1989 ) raise d this\nimportan t questio n in an interestin g articl e unde r this title in Machine  Learning.  \"Th e\ncommercia l worl d of exper t system s at large seem s unconvince d that machin e learnin g has\nanythin g to offer yet. I strongl y disagree, \" he wrot e five year s ago. Despit e of the progres s\nwhic h has been mad e since , Buchanan' s observation s are still quit e valid . Perhap s the\nmain reaso n for the skepti c attitud e towar d machin e learnin g (ML ) in commercia l circle s\nstems from the fact that the academi c worl d has not presente d yet sufficien t evidenc e whic h\njustifie s abandonin g older , relativel y well established , method s in favo r of ML models .\nSurprisingl y man y firm s are involve d now in exper t system s (Ansar i & Modarress , 1990) .\nHow can one convinc e a compan y whic h currentl y develop s or uses exper t system s (ES)\nthat ML can be of substantia l benefit ? Developer s of exper t system s m",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "127": {
    "pdf_path": "data/pdfs\\machine learning_paper_347.pdf",
    "text_excerpt": "Machine Learning, 34, 5–9 (1999)\nc°1999 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nGuest Editors’ Introduction:\nMachine Learning and Natural Language\nCLAIRE CARDIE cardie@cs.cornell.edu\nDepartment of Computer Science, Cornell University, Ithaca, NY 14853-7501\nRAYMOND J. MOONEY mooney@cs.utexas.edu\nDepartment of Computer Sciences, Taylor Hall 2.124, University of Texas, Austin, TX 78712-1188\nThe application of machine learning techniques to natural language processing (NLP) has\nincreased dramatically in recent years under the name of “corpus-based,” “statistical,” or“empirical” methods. However, most of this research has been conducted outside thetraditionalmachinelearningresearchcommunity. Thisspecialissueattemptstobridgethisdivide by assembling an interesting variety of recent research papers on various aspectsof natural language learning – many from authors who do not generally publish in thetraditional machine learning literature – and presenting them to the readers of MachineLearning.\nInthelastﬁvetotenyearstherehasbeenadramaticshiftincomputationallinguisticsfrom\nmanuallyconstructinggrammarsandknowledgebasestopartiallyortotallyautomatingthisprocessbyusingstatisticallearningmethodstrainedonlargeannotatedorunannotatednat-ural language corpora. The success of statistical methods in speech recognition (Stolcke,1997;Jelinek,1998)hasbeenparticularlyinﬂuentialinmotivatingtheapplicationofsimilarmethods to other aspects of natural language processing. There is now a variety of workon applying learning methods to almost all other aspects of language processing as well(Brill&Mooney,1997),includingmorphologicalandsyntacticanalysis(Charniak,1997),semantic disambiguation and interpretation (Ng & Zelle, 1997), discourse processing andinformationextraction(Cardie,1997),andmachinetranslation(Knight,1997). Somecon-cretepublicationstatisticsclearlyillustratetheextentoftherevolutioninnaturallanguageresearch. According to data recently collected by Hirschber",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "128": {
    "pdf_path": "data/pdfs\\machine learning_paper_349.pdf",
    "text_excerpt": "Machine Learning, 37, 115–130 (1999)\nc°1999 Kluwer Academic Publishers. Manufactured in The Netherlands.\nProjection Learning⁄\nLESLIE G. VALIANT valiant@deas.harvard.edu\nDivision of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138, USA\nEditor:Lisa Hellerstein\nAbstract. A method of combining learning algorithms is described that preserves attribute-efﬁciency. It yields\nlearningalgorithmsthatrequireanumberofexamplesthatispolynomialinthenumberofrelevantvariablesandlogarithmicinthenumberofirrelevantones. Thealgorithmsaresimpletoimplementandrealizableonnetworkswith a number of nodes linear in the total number of variables. They include generalizations of Littlestone’sWinnowalgorithm,andare,therefore,goodcandidatesforexperimentationondomainshavingverylargenumbersof attributes but where nonlinear hypotheses are sought.\nKeywords: computational learning, attribute-efﬁcient learning, irrelevant attributes, Winnow algorithm\n1. Introduction\nOneareaofstarkcontrastbetweencurrentmachinelearningpracticeandnaturalbiological\nlearning is that of data preparation. In machine learning it is often found that in order toensuresuccessatanewlearningtaskconsiderableefforthastobeputintocreatingtherightsetofvariables,andintoeliminatingredundantonesiftherearelargenumbersofthese. Inbiological learning, on the other hand, no explicit methods for achieving these ends havebeen identiﬁed. The learning process appears to have mechanisms for overcoming theseproblems implicitly.\nThisdichotomysuggeststheexistenceofausefulclassoflearningalgorithmsthathave\nyettobediscoveredandexploited. Indeedithasbeensuggestedthatalgorithmsthatﬁllthisgaparefundamentaltosystemsthatlearnandmaintainlargeknowledgebasesforcognitivecomputations (Valiant, 1998).\nA simple but persuasive formulation of the problem at hand is the following: one needs\nalgorithmsthatlearn,asefﬁcientlyaspossible,functionsthatdependonasmallnumber k\namongamuchlargernumber nofavailablevariables.Thenecessityforthisissuggestedby\nth",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "129": {
    "pdf_path": "data/pdfs\\machine learning_paper_35.pdf",
    "text_excerpt": "1\nComponents of Machine Learning:\nBinding Bits and FLOPS\nAlexander Jung\nAbstract —Many machine learning problems and methods are\ncombinations of three components: data, hypothesis space and\nloss function. Different machine learning methods are obtained as\ncombinations of different choices for the representation of data,\nhypothesis space and loss function. After reviewing the mathe-\nmatical structure of these three components, we discuss intrinsic\ntrade-offs between statistical and computational properties of\nmachine learning methods.\nI. I NTRODUCTION\nMachine learning (ML) methods implement the scientiﬁc\nprinciple of continuous veriﬁcation and adaptation of a hy-\npothesis about an observable phenomenon (“observable fact or\nevent”) [1]. Examples of a phenomena are:\n\u000fthe visual scene recorded by the smartphone snapshot\ndepicted in Figure 2.\n\u000fthe hiking time required to reach the peak in Figure 2.\n\u000fthe water temperature of the lake in Figure 2.\nThe veriﬁcation and adaption of the hypothesis is based on the\nobservation of data. ML theory and methods revolve around\nthe implementation of the cycle underlying this principle using\nlimited computational resources such as computation time and\nstorage capacity.\nModern ML methods execute the cycle in Figure 1 within\na fraction of a second and using billions of data points [2].\nDeep Learning methods implement the cycle of Figure 1 by\nrepresenting hypotheses by artiﬁcial neural networks whose\nweights (parameters) are continuously adapted using (variants\nof) gradient descent [2].\nA typical ML method consists of three components:\n\u000fdata (mostly in the form of a huge number of bits)\n\u000fa hypothesis space (also referred to as a ML model)\nconsisting of computationally feasible predictor functions.\n\u000fa loss function that is used to assess the quality of a\nparticular predictor function.\nTo implement ML methods, given a limited amount of\ncomputational resources such as number of ﬂoating point\noperations per second (FLOPS), we need to be able ",
    "title": "Efficient Deep Learning on Multi-Source Private Data",
    "abstract": "Machine learning models benefit from large and diverse datasets. Using such\ndatasets, however, often requires trusting a centralized data aggregator. For\nsensitive applications like healthcare and finance this is undesirable as it\ncould compromise patient privacy or divulge trade secrets. Recent advances in\nsecure and privacy-preserving computation, including trusted hardware enclaves\nand differential privacy, offer a way for mutually distrusting parties to\nefficiently train a machine learning model without revealing the training data.\nIn this work, we introduce Myelin, a deep learning framework which combines\nthese privacy-preservation primitives, and use it to establish a baseline level\nof performance for fully private machine learning.",
    "link": "http://arxiv.org/abs/1807.06689v1",
    "published": "2018-07-17T22:18:19Z"
  },
  "130": {
    "pdf_path": "data/pdfs\\machine learning_paper_351.pdf",
    "text_excerpt": "Machine Learning, 34, 5–9 (1999)\nc°1999 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nGuest Editors’ Introduction:\nMachine Learning and Natural Language\nCLAIRE CARDIE cardie@cs.cornell.edu\nDepartment of Computer Science, Cornell University, Ithaca, NY 14853-7501\nRAYMOND J. MOONEY mooney@cs.utexas.edu\nDepartment of Computer Sciences, Taylor Hall 2.124, University of Texas, Austin, TX 78712-1188\nThe application of machine learning techniques to natural language processing (NLP) has\nincreased dramatically in recent years under the name of “corpus-based,” “statistical,” or“empirical” methods. However, most of this research has been conducted outside thetraditionalmachinelearningresearchcommunity. Thisspecialissueattemptstobridgethisdivide by assembling an interesting variety of recent research papers on various aspectsof natural language learning – many from authors who do not generally publish in thetraditional machine learning literature – and presenting them to the readers of MachineLearning.\nInthelastﬁvetotenyearstherehasbeenadramaticshiftincomputationallinguisticsfrom\nmanuallyconstructinggrammarsandknowledgebasestopartiallyortotallyautomatingthisprocessbyusingstatisticallearningmethodstrainedonlargeannotatedorunannotatednat-ural language corpora. The success of statistical methods in speech recognition (Stolcke,1997;Jelinek,1998)hasbeenparticularlyinﬂuentialinmotivatingtheapplicationofsimilarmethods to other aspects of natural language processing. There is now a variety of workon applying learning methods to almost all other aspects of language processing as well(Brill&Mooney,1997),includingmorphologicalandsyntacticanalysis(Charniak,1997),semantic disambiguation and interpretation (Ng & Zelle, 1997), discourse processing andinformationextraction(Cardie,1997),andmachinetranslation(Knight,1997). Somecon-cretepublicationstatisticsclearlyillustratetheextentoftherevolutioninnaturallanguageresearch. According to data recently collected by Hirschber",
    "title": "An Introduction to MCMC for Machine Learning",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/7c8858eba8571d86abd90252e734a11e3a6dd73f",
    "published": null
  },
  "131": {
    "pdf_path": "data/pdfs\\machine learning_paper_356.pdf",
    "text_excerpt": "Machine Learning, 30, 127–132 (1998)\nc°1998 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nGuest Editors’ Introduction:\nOn Applied Research in Machine Learning\nFOSTER PROVOST provost@acm.org\nBell Atlantic Science and Technology, 400 Westchester Avenue, White Plains, New York 10604\nRON KOHAVI ronnyk@sgi.com\nData Mining and Visualization, Silicon Graphics Inc., 2011 N. Shoreline Blvd, Mountain View, CA. 94043\nCommon arguments for including applications papers in the Machine Learning literature\nareoftenbasedonthepapers’valueforadvertisingsuccessstoriesandforboostingmorale.Forexample,high-proﬁleapplicationscanhelptosecurefundingforfutureresearchandcanhelptoattracthighcaliberstudents. However,thereisanotherreasonwhysuchpapersareofvaluetotheﬁeld,whichis,arguably,evenmorevital. Applicationpapersare essentialin\norder for Machine Learning to remain a viable science. They focus research on importantunsolved problems that currently restrict the practical applicability of machine learningmethods.\nMuchofthe“science”ofMachineLearningisascienceofengineering.\n1Bythiswemean\nthat it is dedicated to creating and compiling veriﬁable knowledge related to the designand construction of artifacts. The scientiﬁc knowledge comprises theoretical arguments,observationalcategorizations,empiricalstudies,andpracticaldemonstrations. Theartifactsare computer programs that use data to build models that are practically or theoreticallyuseful. Becausetheobjectsofstudyareintendedtohavepracticalutility,itisessentialforresearch activities to be focused (in part) on the elimination of obstacles that impede theirpractical application.\nMostoftentheseobstaclestaketheformofrestrictivesimplifyingassumptionscommonly\nmade in research. Consider as an example the assumption, common in classiﬁer learningresearch, that misclassiﬁcation errors have equal costs. The vast majority of classiﬁerlearningresearchinMachineLearninghasbeenconductedunderthisassumption,throughthe use of classiﬁcation accur",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "132": {
    "pdf_path": "data/pdfs\\machine learning_paper_357.pdf",
    "text_excerpt": "Machin e Learning , 20, 5-22 (1995 )\n& 199 5 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands ,\nEvaluatio n and Selectio n of Biase s in Machin e\nLearnin g\nDIAN A F. GORDO N gordon@aic.nrl.navy.mi l\nNaval  Research  Laboratory\nMARI E DESJARDIN S marie@erg.sri.co m\nSRI International\nEditor : Thoma s G. Dietteric h\nAbstract . In this introduction , we defin e the term bias  as it is used in machin e learnin g systems . We motivat e\nthe importanc e of automate d method s for evaluatin g and selectin g biase s usin g a framewor k of bias selectio n\nas searc h in bias and meta-bia s spaces . Recen t researc h in the field of machin e learnin g bias is summarized .\nKeywords : bias , concep t learnin g\n1. Introductio n\nThis specia l issue of Machine  Learning  focuse s on the evaluatio n and selectio n of biases .\nThe paper s in this issue describ e method s by whic h intelligen t system s automaticall y eval -\nuate and selec t thei r own biases , and tools for analyzin g and testin g variou s approache s to\nbias selection . In this paper , we motivat e the importanc e of this topic . Sinc e mos t reader s\nwill be familia r with supervise d concep t learning , we phras e our discussio n withi n that\nframework . However , bias as we presen t it here is a part of ever y type of learning .\nWe outlin e a framewor k for treatin g bias selectio n as a proces s of designin g appropriat e\nsearc h method s over the bias and meta-bia s spaces . Thi s framewor k has two essentia l\nfeatures : it divide s bias into representationa l and procedura l components , and it char -\nacterize s learnin g as searc h withi n multipl e tiers . The source s of bias withi n a syste m\ncan thus be identifie d and analyze d with respec t to thei r influenc e on this multi-tiere d\nsearc h process , and bias shift become s searc h at the bias level . The framewor k provide s\nan analyti c tool with whic h to compar e differen t system s (includin g those not develope d\nwithi n the ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "133": {
    "pdf_path": "data/pdfs\\machine learning_paper_36.pdf",
    "text_excerpt": "arXiv:2007.05479v1  [cs.AI]  10 Jul 2020Impact of Legal Requirements on Explainability in Machine L earning\nAdrien Bibal* 1Michael Lognoul* 2Alexandre de Streel2Benoˆ ıt Fr ´enay1\n1. Legal Requirements on Explainability\nThe requirements on explainability imposed by Euro-\npean laws and their implications for machine learning\n(ML) models are not always clear. In that perspective,\nour research ( Bibal et al. ,Forthcoming ) analyzes explana-\ntion obligations imposed for private and public decision-\nmaking, and how they can be implemented by machine\nlearning techniques.\nFor decisions adopted by ﬁrms or individuals, we mainly\nfocus on requirements imposed by general European legis-\nlation applicable to all the sectors of the economy. The obli -\ngations of the General Data Protection Regulation (GDPR)\n(art. 13-15 and 22) as interpreted by the European Data Pro-\ntection Board (EDPB) require the processors of personal\ndata to provide “the rationale behind or the criteria relied\non in reaching the decision,” under certain circumstances,\nwhen a fully automated decision is made (EDPB Guide-\nlines of 3 October 2017 on Automated individual decision-\nmaking and Proﬁling, p. 25; see also ( Edwards & Veale ,\n2018 ;Wachter et al. ,2017 )). Consumer protection law im-\nposes to online marketplaces to provide their consumers\nwith “the main parameters determining ranking [...] and\nthe relative importance of those parameters” (art. 6(a) of\nDirective 2011/83). The Online Platforms Regulation im-\nposes very similar obligations to online intermediation se r-\nvices and search engines towards their professional users\n(art. 5 of Regulation 2019/1150).\nSectoral rules are also analyzed. For instance, ﬁnancial re g-\nulators “may require the investment ﬁrm to provide [...] a\ndescription of the nature of its algorithmic trading strate -\ngies, details of the trading parameters or limits to which\nthe system is subject, the key compliance and risk controls\nthat it has in place [...]. The competent auth",
    "title": "Pymc-learn: Practical Probabilistic Machine Learning in Python",
    "abstract": "$\\textit{Pymc-learn}$ is a Python package providing a variety of\nstate-of-the-art probabilistic models for supervised and unsupervised machine\nlearning. It is inspired by $\\textit{scikit-learn}$ and focuses on bringing\nprobabilistic machine learning to non-specialists. It uses a general-purpose\nhigh-level language that mimics $\\textit{scikit-learn}$. Emphasis is put on\nease of use, productivity, flexibility, performance, documentation, and an API\nconsistent with $\\textit{scikit-learn}$. It depends on $\\textit{scikit-learn}$\nand $\\textit{pymc3}$ and is distributed under the new BSD-3 license,\nencouraging its use in both academia and industry. Source code, binaries, and\ndocumentation are available on http://github.com/pymc-learn/pymc-learn.",
    "link": "http://arxiv.org/abs/1811.00542v1",
    "published": "2018-10-31T22:54:12Z"
  },
  "134": {
    "pdf_path": "data/pdfs\\machine learning_paper_365.pdf",
    "text_excerpt": "Machin e Learning , 13, 151-15 2 (1993 )\n© 199 3 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands .\nA Repl y to Hellerstein' s Boo k Revie w of\nMachine  Learning:  A Theoretical  Approach\nB.K. NATARAJA N\nHewlett  Packard  Laboratories,  1501  Page  Mill  Road,  Palo  Alto,  CA 94304\nI wan t to than k Lisa Hellerstei n for her kind efforts , and Albert o Segr e for commission -\ning such .\nThe challeng e in writin g this book lay in presentin g technicall y difficul t materia l in a\nform that was accessibl e to the broade r audience , withou t compromisin g its integrity . A\ncompoundin g facto r was that this was to be the first text in the area , withou t prior leverag e\nin establishin g notationa l convention s or conceptua l organization . Such a challeng e demande d\nnumerou s editoria l and pedagogi c decisions , and I must confes s that man y of Hellerstein' s\npoint s bear directl y on thes e decisions . Whil e it is neithe r possibl e nor constructiv e for\nme to respon d to her revie w exhaustively , I woul d urge anyon e undertakin g a simila r work\nto pay her close attention . I will restric t my respons e to a few specifi c points .\nWhen I say that the PAC mode l appear s to be a good mode l of the natura l learnin g proc -\ness, I do not impl y that it model s the learnin g mechanism s of the livin g psyche . I mea n\nthat it is a good mode l of the input-outpu t behavio r of the natura l learnin g process—goo d\nbecaus e it is wort h studyin g in an aestheti c sense , and mor e importantly , good becaus e\nit promise s to be a step toward s explainin g huma n experience . As pointe d out by Valian t\n(1984) , both of the abov e qualitie s are desirabl e in a computationa l model . My statemen t\nis in the same spiri t as the statemen t that the Turin g machin e is a good mode l of the natura l\nnotio n of a computationa l algorithm .\nI am please d that Hellerstei n sees my book as a unifor m presentatio n of theoretica l results ,\nofferin g",
    "title": "Machine learning phases of matter",
    "abstract": "The success of machine learning techniques in handling big data sets proves ideal for classifying condensed-matter phases and phase transitions. The technique is even amenable to detecting non-trivial states lacking in conventional order. Condensed-matter physics is the study of the collective behaviour of infinitely complex assemblies of electrons, nuclei, magnetic moments, atoms or qubits1. This complexity is reflected in the size of the state space, which grows exponentially with the number of particles, reminiscent of the ‘curse of dimensionality’ commonly encountered in machine learning2. Despite this curse, the machine learning community has developed techniques with remarkable abilities to recognize, classify, and characterize complex sets of data. Here, we show that modern machine learning architectures, such as fully connected and convolutional neural networks3, can identify phases and phase transitions in a variety of condensed-matter Hamiltonians. Readily programmable through modern software libraries4,5, neural networks can be trained to detect multiple types of order parameter, as well as highly non-trivial states with no conventional order, directly from raw state configurations sampled with Monte Carlo6,7.",
    "link": "https://www.semanticscholar.org/paper/dd308eb0d7be24e593fe355476057fc37ab5bf0e",
    "published": "2016-03-17"
  },
  "135": {
    "pdf_path": "data/pdfs\\machine learning_paper_37.pdf",
    "text_excerpt": "arXiv:2007.14206v1  [physics.comp-ph]  27 Jul 2020Machine Learning Potential Repository\nAtsuto Seko1,∗\n1Department of Materials Science and Engineering, Kyoto Uni versity, Kyoto 606-8501, Japan\n(Dated: July 29, 2020)\nThis paper introduces a machine learning potential reposit ory that includes Pareto optimal ma-\nchine learning potentials. It also shows the systematic dev elopment of accurate and fast machine\nlearning potentials for a wide range of elemental systems. A s a result, many Pareto optimal machine\nlearning potentials are available in the repository from a w ebsite [1]. Therefore, the repository will\nhelp many scientists to perform accurate and fast atomistic simulations.\nI. INTRODUCTION\nMachine learning potential (MLP) has been increas-\ningly required to perform crystal structure optimizations\nand large-scale atomistic simulations more accurately\nthan with conventional interatomic potentials. There-\nfore, many recent studies have proposed a number of\nprocedures to develop MLPs and have shown their ap-\nplications [2–23]. Simultaneously, MLPs themselves are\nnecessary for their users to perform accurate atomistic\nsimulations. Therefore, the development and distribu-\ntion of MLPs for a wide range of systems should be use-\nful, similarly to the conventional interatomic potentials\ndistributed in several repositories [24, 25].\nThis study demonstrates an MLP repository available\nfrom a website [1]. The MLP repository includes Pareto\noptimal MLPs with diﬀerent trade-oﬀs between accuracy\nand computational eﬃciency because they are conﬂicting\nproperties and there is no single optimal MLP [26–28].\nThis study develops the repository by performing sys-\ntematic density functional theory (DFT) calculations for\napproximately460,000structuresandbycombiningthem\nwith existing DFT datasets in the literature [26, 29].\nPolynomial-based potential energy models [26, 29] and\ntheir revisionsarethen systematically applied to the con-\nstruction of MLPs for a wide range of elemental sys",
    "title": "Machine learning in physics: a short guide",
    "abstract": "Machine learning is a rapidly growing field with the potential to\nrevolutionize many areas of science, including physics. This review provides a\nbrief overview of machine learning in physics, covering the main concepts of\nsupervised, unsupervised, and reinforcement learning, as well as more\nspecialized topics such as causal inference, symbolic regression, and deep\nlearning. We present some of the principal applications of machine learning in\nphysics and discuss the associated challenges and perspectives.",
    "link": "http://arxiv.org/abs/2310.10368v1",
    "published": "2023-10-16T13:05:47Z"
  },
  "136": {
    "pdf_path": "data/pdfs\\machine learning_paper_378.pdf",
    "text_excerpt": "Machin e Learnin g 2: 229 246 , 1987\n© 198 7 Kluwe r Academi c Publishers , Bosto n - Manufacture d in The Netherland s\nLearnin g Decisio n List s\nRONAL D L. RIVES T (RIVEST@THEORY.LCS.MIT.EDU )\nLaboratory  for Computer  Science,  Massachusetts  Institute  of Technology,\nCambridge,  Massachusetts  02139,  U.S.A.\n(Received : Decembe r 29, 1986 )\n(Revised : Augus t 5, 1987 )\nKeywords : Learnin g from examples , decisio n lists , Boolea n formulae , polynomial-tim e\nidentification .\nAbstract . Thi s pape r introduce s a new representatio n for Boolea n functions , calle d\ndecision  lists,  and show s that they are efficientl y learnabl e from examples . Mor e precisely ,\nthis resul t is establishe d for k-D L the set of decisio n lists with conjunctiv e clause s of\nsize k at each decision . Sinc e k-DL  properl y include s othe r well-know n technique s for\nrepresentin g Boolea n function s suc h as k-CN F (formula e in conjunctiv e norma l form\nwith at mos t k literal s per clause) , k-DN F (formula e in disjunctiv e norma l form with\nat mos t k literal s per term) , and decisio n tree s of dept h k, our resul t strictl y increase s\nthe set of function s that are know n to be polynomiall y learnable , in the sens e of Valian t\n(1984) . Our proo f is constructive : we presen t an algorith m that can efficientl y construc t\nan elemen t of k-D L consisten t with a give n set of examples , if one exists .\n1. Introductio n\nOne goal of researc h in machin e learnin g is to identif y the larges t possibl e\nclass of concept s that are learnabl e from examples . In this pape r we restric t\nour attentio n to concept s that can be define d in term s of a give n set of n\nBoolea n attributes ; each assignmen t of true or false to the attribute s can\nbe classifie d as eithe r a positive  or a negative  instanc e of the concep t to be\nlearned .\nTo mak e precis e the notio n of \"learnabl e from examples, \" we adop t the\ndefinitio n of polynomial  learnability  pioneere d by Valian t ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "137": {
    "pdf_path": "data/pdfs\\machine learning_paper_38.pdf",
    "text_excerpt": "Quantum memristors for neuromorphic quantum machine learning  \n \nLucas Lamata  \nDepartamento de Física Atómica, Molecular, y Nuclear, Facultad de Física, Universidad \nde Sevilla, Apartado 1065, E -41080 Sevilla, Spain  \nEmail Address: llamata@us.es  \nQuantum machine learning may permit to realize more efficient machine learning \ncalculations with near -term quantum devices. Among the diverse quantum machine \nlearning paradigms which are currently being considered, quantum memristors are \npromising as a way  of combining, in the same quantum hardware, a unitary evolution \nwith the nonlinearity provided by the measurement and feedforward. Thus, an efficient \nway of deploying neuromorphic quantum computing for quantum machine learning may \nbe enabled.  \n \nIntroduction  \n \nQuantum machine learning is an emerging field inside quantum technologies, which may \nenable to carry out more efficient machine learning calculations with near -term \nquantum devices  [1-4]. It is apparent that a purely quantum unitary evolution is not \nenough for an efficient learning process, which implies irreversibility to some extent  [1,2] . \nTherefore, most quantum learning algorithms imply some variant of the so -called \nquantum -classical protocols, such as variational quantum  eigensolvers  [5-7], quantum \nBoltzmann machines  [8], quantum approximate optimization algorithms  [9], and \nquantum reinforcement learning protocols  [10-14]. Besides the basic digital quant um \ncircuit model, all these algorithms include quantum measurements and feedforward in \nsome part of the execution , with subsequent further unitary evolution, further classical \npart, and so on.  Usually, one considers a universal quantum computer based on the \ndigital paradigm for carrying out all these quantum algorithms. However, it may be \nsensible to employ specialized quantum har dware , which incorporates ab  initio both the \nunitary evolution and the quantum measurement part in the same building block. This is \nwhat a ",
    "title": "Quantum-Classical Machine learning by Hybrid Tensor Networks",
    "abstract": "Tensor networks (TN) have found a wide use in machine learning, and in\nparticular, TN and deep learning bear striking similarities. In this work, we\npropose the quantum-classical hybrid tensor networks (HTN) which combine tensor\nnetworks with classical neural networks in a uniform deep learning framework to\novercome the limitations of regular tensor networks in machine learning. We\nfirst analyze the limitations of regular tensor networks in the applications of\nmachine learning involving the representation power and architecture\nscalability. We conclude that in fact the regular tensor networks are not\ncompetent to be the basic building blocks of deep learning. Then, we discuss\nthe performance of HTN which overcome all the deficiency of regular tensor\nnetworks for machine learning. In this sense, we are able to train HTN in the\ndeep learning way which is the standard combination of algorithms such as Back\nPropagation and Stochastic Gradient Descent. We finally provide two applicable\ncases to show the potential applications of HTN, including quantum states\nclassification and quantum-classical autoencoder. These cases also demonstrate\nthe great potentiality to design various HTN in deep learning way.",
    "link": "http://arxiv.org/abs/2005.09428v2",
    "published": "2020-05-15T10:20:35Z"
  },
  "138": {
    "pdf_path": "data/pdfs\\machine learning_paper_384.pdf",
    "text_excerpt": "Machine Learning, 42, 321, 2001\nc°2001 Kluwer Academic Publishers. Manufactured in The Netherlands.\nErrata\nThepublisherapologizesandherebytakesallresponsibilityforanerrorwhichoccurred\ninVolume41,Number3,ofthejournal, MachineLearning . Onthefrontcover,thetitleof\nthearticlewrittenbyKristinP.Bennett,NelloCristianini,JohnShawe-TaylorandDonghuiWuisincorrect. Thecorrecttitleis“EnlargingtheMarginsinPerceptronDecisionTrees.”",
    "title": "Research Papers in Machine Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022603230145",
    "published": "2003-04-04T16:55:36Z"
  },
  "139": {
    "pdf_path": "data/pdfs\\machine learning_paper_385.pdf",
    "text_excerpt": "Machin e Learning , 18, 23-5 0 (1995 )\n© 1995 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands .\nProbabl y Almos t Discriminativ e Learnin g\nKENJ I YAMANISH I yamanisi@research.nj.nec.co m\nNEC  Research  Institute,  Inc.,  4 Independence  Way,  Princeton  NJ 08540\nEditor : Davi d Haussle r\nAbstract . This pape r develop s a new computationa l mode l for learnin g stochasti c rules , calle d PAD (Probabl y\nAlmos t Discriminative)-learnin g model , base d on statistica l hypothesi s testin g theory . The mode l deal s with the\nproble m of designin g a discriminatio n algorith m to test whethe r or not any give n test sequenc e of example s of\npairs of (instance , label ) has com e from a give n stochasti c rule P*. Her e a composit e hypothesi s P is unknow n\nother than it belong s to a give n class C.\nIn this model , we propos e a new discriminatio n algorith m on the basi s of the MD L (Minimu m Descriptio n\nLength ) principle , and then deriv e uppe r bound s on the least test sampl e size require d by the algorith m to guarante e\nthat two type s of error probabilitie s are respectivel y less than S1 and 62 provide d that the distanc e betwee n the\ntwo rules to be discriminate d is not less than e.\nFor the parametri c case wher e C is a parametri c class , this pape r show s that an uppe r boun d on test sampl e\nsize is give n by O( j In j- + 4j In -^ + 7 In 7 H—^~^) - Her e k is the numbe r of real-value d parameter s\nfor the composit e hypothesi s P, and i(M)  is the descriptio n lengt h for the countabl e mode l for P. Furthe r\nthis pape r show s that the MDL-base d discriminatio n algorith m perform s well in the sens e of sampl e complexit y\nefficiency , comparin g it with othe r kind s of information-criteria-base d discriminatio n algorithms . Thi s pape r\nalso show s how to transfor m any stochasti c PAC (Probabl y Approximatel y Correct)-learnin g algorith m into a\nPAD-learnin g algorithm .\nFor the non-parametri c case wher e C is a",
    "title": "Machine Learning and Concept Formation",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022896407371",
    "published": "2003-04-04T16:57:10Z"
  },
  "140": {
    "pdf_path": "data/pdfs\\machine learning_paper_39.pdf",
    "text_excerpt": "Journal of Machine Learning Research 21 (2020) 1-6 Submitted 8/19; Revised 7/20; Published 7/20\nmetric-learn: Metric Learning Algorithms in Python\nWilliam de Vazelhes\u0003wdevazelhes@gmail.com\nParis Research Center, Huawei Technologies\n92100 Boulogne-Billancourt, France\nCJ Carey perimosocordiae@gmail.com\nGoogle LLC\n111 8th Ave, New York, NY 10011, USA\nYuan Tang terrytangyuan@gmail.com\nAnt Group\n525 Almanor Ave, Sunnyvale, CA 94085, USA\nNathalie Vauquier nathalie.vauquier@inria.fr\nAur\u0013 elien Bellet aurelien.bellet@inria.fr\nMagnet Team, INRIA Lille { Nord Europe\n59650 Villeneuve d'Ascq, France\nEditor: Balazs Kegl\nAbstract\nmetric-learn is an open source Python package implementing supervised and weakly-\nsupervised distance metric learning algorithms. As part of scikit-learn-contrib , it\nprovides a uni\fed interface compatible with scikit-learn which allows to easily perform\ncross-validation, model selection, and pipelining with other machine learning estimators.\nmetric-learn is thoroughly tested and available on PyPi under the MIT license.\nKeywords: machine learning, python, metric learning, scikit-learn\n1. Introduction\nMany approaches in machine learning require a measure of distance between data points.\nTraditionally, practitioners would choose a standard distance metric (Euclidean, City-Block,\nCosine, etc.) using a priori knowledge of the domain. However, it is often di\u000ecult to design\nmetrics that are well-suited to the particular data and task of interest. Distance metric\nlearning, or simply metric learning (Bellet et al., 2015), aims at automatically constructing\ntask-speci\fc distance metrics from data. A key advantage of metric learning is that it\ncan be applied beyond the standard supervised learning setting (data points associated\nwith labels), in situations where only weaker forms of supervision are available (e.g., pairs\nof points that should be similar/dissimilar). The learned distance metric can be used to\nperform retrieval tasks such as \fnding elements (images,",
    "title": "A Review on Machine Unlearning",
    "abstract": "Recently, an increasing number of laws have governed the useability of users'\nprivacy. For example, Article 17 of the General Data Protection Regulation\n(GDPR), the right to be forgotten, requires machine learning applications to\nremove a portion of data from a dataset and retrain it if the user makes such a\nrequest. Furthermore, from the security perspective, training data for machine\nlearning models, i.e., data that may contain user privacy, should be\neffectively protected, including appropriate erasure. Therefore, researchers\npropose various privacy-preserving methods to deal with such issues as machine\nunlearning. This paper provides an in-depth review of the security and privacy\nconcerns in machine learning models. First, we present how machine learning can\nuse users' private data in daily life and the role that the GDPR plays in this\nproblem. Then, we introduce the concept of machine unlearning by describing the\nsecurity threats in machine learning models and how to protect users' privacy\nfrom being violated using machine learning platforms. As the core content of\nthe paper, we introduce and analyze current machine unlearning approaches and\nseveral representative research results and discuss them in the context of the\ndata lineage. Furthermore, we also discuss the future research challenges in\nthis field.",
    "link": "http://arxiv.org/abs/2411.11315v1",
    "published": "2024-11-18T06:18:13Z"
  },
  "141": {
    "pdf_path": "data/pdfs\\machine learning_paper_4.pdf",
    "text_excerpt": "Machine Learning for Clinical Predictive Analytics\nWei-Hung Weng1\nLearning Objectives\n\u000fUnderstand the basics of machine learning\ntechniques and the reasons behind why they\nare useful for solving clinical prediction\nproblems.\n\u000fUnderstand the intuition behind some ma-\nchine learning models, including regression,\ndecision trees, and support vector machines.\n\u000fUnderstand how to apply these models to\nclinical prediction problems using publicly\navailable datasets via case studies.\n1. Machine Learning for Healthcare\n1.1. Introduction\nIn this chapter, we provide a brief overview of applying ma-\nchine learning techniques for clinical prediction tasks. We\nbegin with a quick introduction to the concepts of machine\nlearning, and outline some of the most common machine\nlearning algorithms. Next, we demonstrate how to apply\nthe algorithms with appropriate toolkits to conduct machine\nlearning experiments for clinical prediction tasks.\nThis chapter is composed of ﬁve sections. First, we will\nexplain why machine learning techniques are helpful for\nresearchers in solving clinical prediction problems (section\n1). Understanding the motivations behind machine learn-\ning approaches in healthcare are essential, since precision\nand accuracy are often critical in healthcare problems, and\neverything from diagnostic decisions to predictive clinical\nanalytics could dramatically beneﬁt from data-based pro-\ncesses with improved efﬁciency and reliability. In the sec-\nond section, we will introduce several important concepts in\nmachine learning in a colloquial manner, such as learning\nscenarios, objective/target function, error and loss function\nand metrics, optimization and model validation, and ﬁnally\na summary of model selection methods (section 2). These\ntopics will help us utilize machine learning algorithms in\nan appropriate way. Following that, we will introduce some\n1MIT CSAIL, Cambridge, MA, USA. Correspondence to: Wei-\nHung Weng <ckbjimmy@mit.edu >.popular machine learning algorithms for ",
    "title": "Machine Learning for Clinical Predictive Analytics",
    "abstract": "In this chapter, we provide a brief overview of applying machine learning\ntechniques for clinical prediction tasks. We begin with a quick introduction to\nthe concepts of machine learning and outline some of the most common machine\nlearning algorithms. Next, we demonstrate how to apply the algorithms with\nappropriate toolkits to conduct machine learning experiments for clinical\nprediction tasks. The objectives of this chapter are to (1) understand the\nbasics of machine learning techniques and the reasons behind why they are\nuseful for solving clinical prediction problems, (2) understand the intuition\nbehind some machine learning models, including regression, decision trees, and\nsupport vector machines, and (3) understand how to apply these models to\nclinical prediction problems using publicly available datasets via case\nstudies.",
    "link": "http://arxiv.org/abs/1909.09246v1",
    "published": "2019-09-19T22:02:00Z"
  },
  "142": {
    "pdf_path": "data/pdfs\\machine learning_paper_40.pdf",
    "text_excerpt": "arXiv:2002.12364v1  [cs.LG]  27 Feb 2020Theoretical Models of Learning to Learn*\nJONATHAN BAXTER jon@syseng.anu.edu.au\nDepartment of Systems Engineering\nResearch School of Information Science and Engineering\nAustralian National University\nCanberra 0200\nAustralia\nAbstract. A Machine can only learn if it is biased in some way. Typically the bias is supplied by\nhand, for example through the choice of an appropriate set of features. However, if the learning\nmachine is embedded within an environment of related tasks, then it can learnits own bias\nby learning suﬃciently many tasks from the environment [4, 6 ]. In this paper two models of\nbias learning (or equivalently, learning to learn) are intr oduced and the main theoretical results\npresented. The ﬁrst model is a PAC-type model based on empiri cal process theory, while the\nsecond is a hierarchical Bayes model.\nKeywords: Learning to Learn, Bias Learning, Empirical Processes, Hie rarchical Bayes\n1. Introduction\nHume’sanalysis[10]showsthatthereisno a prioribasisforinduction. Inamachine\nlearning context, this means that a learner must be biased in some wa y for it to\ngeneralise well [11]. Typically such bias is introduced by hand through t he skill and\ninsightsofexperts, butdespitemanynotablesuccesses,thispro cessislimitedbythe\nexperts’ abilities. Hence a desirable goal is to ﬁnd ways of automatic allylearning\nthe bias. Bias learning is a form of learning to learn , and the two expressions will\nbe used interchangeably throughout this document.\nThe purpose of this chapter is to present an overview of two models ofsupervised\nbias learning. The ﬁrst [4, 3] is based on Empirical Process theory (h enceforth\ntheEPmodel) and the second [6] is based on Bayesian inference and informa tion\ntheory (henceforth the Bayesmodel). Empirical process theory is a general theory\nthat includes the analysis of pattern classiﬁcation ﬁrst introduced by Vapnik and\nChervonenkis [13, 12]. Note that these are models of supervised bias learning and\nas",
    "title": "Dex: Incremental Learning for Complex Environments in Deep Reinforcement\n  Learning",
    "abstract": "This paper introduces Dex, a reinforcement learning environment toolkit\nspecialized for training and evaluation of continual learning methods as well\nas general reinforcement learning problems. We also present the novel continual\nlearning method of incremental learning, where a challenging environment is\nsolved using optimal weight initialization learned from first solving a similar\neasier environment. We show that incremental learning can produce vastly\nsuperior results than standard methods by providing a strong baseline method\nacross ten Dex environments. We finally develop a saliency method for\nqualitative analysis of reinforcement learning, which shows the impact\nincremental learning has on network attention.",
    "link": "http://arxiv.org/abs/1706.05749v1",
    "published": "2017-06-19T00:16:24Z"
  },
  "143": {
    "pdf_path": "data/pdfs\\machine learning_paper_41.pdf",
    "text_excerpt": "An Aggregate and Iterative Disaggregate Algorithm with Proven\nOptimality in Machine Learning\nYoung Woong Park\u0003 y1and Diego Klabjanz2\n1Cox School of Business, Southern Methodist University, Dallas, TX, USA\n2Department of Industrial Engineering and Management Sciences, Northwestern\nUniversity, Evanston, IL, USA\nFeb 25, 2016\nAbstract\nWe propose a clustering-based iterative algorithm to solve certain optimization problems in machine\nlearning, where we start the algorithm by aggregating the original data, solving the problem on ag-\ngregated data, and then in subsequent steps gradually disaggregate the aggregated data. We apply the\nalgorithm to common machine learning problems such as the least absolute deviation regression problem,\nsupport vector machines, and semi-supervised support vector machines. We derive model-speci\fc data\naggregation and disaggregation procedures. We also show optimality, convergence, and the optimality\ngap of the approximated solution in each iteration. A computational study is provided.\n1 Introduction\nIn this paper, we propose a clustering-based iterative algorithm to solve certain optimization problems in\nmachine learning when data size is large and thus it becomes impractical to use out-of-the-box algorithms.\nWe rely on the principle of data aggregation and then subsequent disaggregations. While it is standard\npractice to aggregate the data and then calibrate the machine learning algorithm on aggregated data, we\nembed this into an iterative framework where initial aggregations are gradually disaggregated to the extent\nthat even an optimal solution is obtainable.\nEarly studies in data aggregation consider transportation problems [1, 10], where either demand or supply\nnodes are aggregated. Zipkin [31] studied data aggregation for linear programming (LP) and derived error\nbounds of the approximate solution. There are also studies on data aggregation for 0-1 integer programming\n[8, 13]. The reader is referred to Rogers et al [22] and Litvinchev an",
    "title": "Bridging belief function theory to modern machine learning",
    "abstract": "Machine learning is a quickly evolving field which now looks really different\nfrom what it was 15 years ago, when classification and clustering were major\nissues. This document proposes several trends to explore the new questions of\nmodern machine learning, with the strong afterthought that the belief function\nframework has a major role to play.",
    "link": "http://arxiv.org/abs/1504.03874v1",
    "published": "2015-04-15T12:04:58Z"
  },
  "144": {
    "pdf_path": "data/pdfs\\machine learning_paper_42.pdf",
    "text_excerpt": "Abstract : Though technical advance of artificial intelligence and machine learning has enabled many promising \nintelligent systems,  many computing tasks are still not able to be fully accomplished by machine intelligence. \nMotivated by the complementary nature of human and machine intelligence, an emerging trend is to involve humans \nin the loop of machine learning and decision -making. In this paper, we provide a macro -micro review of human -in-\nthe-loop machine learning. We first describe major machine learning challenges which can be addressed by human \nintervention in the loop. Then we examine closely the latest research and findings of introducing humans into each \nstep of the lifecycle of machine learning. Finally, we analyze current research gaps and point out future research \ndirections.  \n1. Introduction  \nAdvances in machine learning (ML) technologies have led to an explosion of major bre akthroughs in the field of \nartificial intelligence (AI), which in turn has given rise to ubiquitous intelligent systems in our daily life. Typical \napplications include autonomous vehicles, game playing (e.g., AlphaGo), precise medical diagnosis and \nprescri ption, assistive robots, face recognition, and so forth. Despite the increasing usage and power of ML in these \nAI applications, there is still a large spectrum of computing tasks that pure machine intelligence cannot fully handle \ndue to challenges such as lack of large -scale dataset, low data quality, insufficient training labels, and explanability \nand reliability issues of ML -based decision -making.  \nMotivated by the complementary nature of human and machine intelligence [Kamar 2016] [Wang 2019], an \napproac h that exploits human’s cognitive power to help address these ML challenges has emerged recently, which \nis referred to as “Human -in-the-loop Machine Learning (HML)”. HML is intended to integrate relevant human \ncompetences into the whole cycle of ML, rangin g from data collection, algorithm tuning, pa",
    "title": "Electre Tri-Machine Learning Approach to the Record Linkage Problem",
    "abstract": "In this short paper, the Electre Tri-Machine Learning Method, generally used\nto solve ordinal classification problems, is proposed for solving the Record\nLinkage problem. Preliminary experimental results show that, using the Electre\nTri method, high accuracy can be achieved and more than 99% of the matches and\nnonmatches were correctly identified by the procedure.",
    "link": "http://arxiv.org/abs/1505.06614v1",
    "published": "2015-05-25T13:02:32Z"
  },
  "145": {
    "pdf_path": "data/pdfs\\machine learning_paper_43.pdf",
    "text_excerpt": "arXiv:2407.05526v1  [cs.LG]  8 Jul 2024Can Machines Learn the True Probabilities?\nJinsook Kim1\nAbstract\nWhen there exists uncertainty, AI machines are\ndesigned to make decisions so as to reach the\nbest expected outcomes. Expectations are based\non true facts about the objective environment the\nmachines interact with, and those facts can be\nencoded into AI models in the form of true ob-\njective probability functions. Accordingly, AI\nmodels involve probabilistic machine learning in\nwhich the probabilities should be objectively in-\nterpreted. We prove under some basic assump-\ntions when machines can learn the true objective\nprobabilities, if any, and when machines cannot\nlearn them.\n1. Introduction\nIn the standard AI model under uncertainty, how to mea-\nsure the degree of uncertainty matters. This paper is about\ntreating such measures in the form of probabilities. In par-\nticular, we focus on the true objective probabilities, if an y.\nThere are various probabilistic contexts in which the true\nobjective probabilities matter. For example, causal rela-\ntions of physical events are widely regarded as objective\nfeatures of the world. Therefore, when causal relations are\nto be understood in terms of probabilities mainly due to var-\nious regularity issues, a probabilistic causal model shoul d\ninclude an objective probability function that measures th e\ntrue objective values about our world.\nThis paper addresses the question of whether machines can\nlearn the true objective probabilities from the data to per-\nform such probabilistic reasoning. Under some basic as-\nsumptions, we prove that machines can learn the true objec-\ntive probabilities if and only if the probabilities are dire ctly\nobservable by them. Roughly speaking, a true probability\nis directly observable by a machine when it can calculate\nthe probability by the empirical frequency of a true popula-\n1Underwood International College, Yonsei Univer-\nsity, Seoul, Korea. Correspondence to: Jinsook Kim\n<jki76364@gmail.com ",
    "title": "Theoretical Robopsychology: Samu Has Learned Turing Machines",
    "abstract": "From the point of view of a programmer, the robopsychology is a synonym for\nthe activity is done by developers to implement their machine learning\napplications. This robopsychological approach raises some fundamental\ntheoretical questions of machine learning. Our discussion of these questions is\nconstrained to Turing machines. Alan Turing had given an algorithm (aka the\nTuring Machine) to describe algorithms. If it has been applied to describe\nitself then this brings us to Turing's notion of the universal machine. In the\npresent paper, we investigate algorithms to write algorithms. From a pedagogy\npoint of view, this way of writing programs can be considered as a combination\nof learning by listening and learning by doing due to it is based on applying\nagent technology and machine learning. As the main result we introduce the\nproblem of learning and then we show that it cannot easily be handled in\nreality therefore it is reasonable to use machine learning algorithm for\nlearning Turing machines.",
    "link": "http://arxiv.org/abs/1606.02767v2",
    "published": "2016-06-08T21:46:20Z"
  },
  "146": {
    "pdf_path": "data/pdfs\\machine learning_paper_44.pdf",
    "text_excerpt": "On-the-Fly Learning in a Perpetual Learning \nMachine \nAndrew J.R. Simpson  #1 \n# Centre for Vision, Speech and Signal Processing, Un iversity of Surrey,UK \n1 Andrew.Simpson@Surrey.ac.uk \n \nAbstract —Despite the promise of brain-inspired machine \nlearning, deep neural networks (DNN) have frustrati ngly failed \nto bridge the deceptively large gap between learning  and memory . \nHere, we introduce a Perpetual Learning Machine ; a new type of \nDNN that is capable of brain-like dynamic ‘on the f ly’ learning \nbecause it exists in a self-supervised  state of Perpetual Stochastic \nGradient Descent . Thus, we provide the means to unify learning  \nand memory within a machine learning framework. We also \nexplore the elegant duality of abstraction  and synthesis : the Yin  \nand Yang  of deep learning. \n \nIndex terms —Perpetual Learning Machine, Perpetual \nStochastic Gradient Descent, self-supervised learni ng, parallel \ndither, Yin and Yang. \n \nI.  INTRODUCTION  \nIt is an embarassing fact that while deep neural ne tworks \n(DNN) are frequently compared to the brain, and eve n their \nperformance found to be similar in specific static tasks, there \nremains a critical difference; DNN do not exhibit t he fluid and \ndynamic learning of the brain but are static once t rained. For \nexample, to add a new class of data to a trained DN N it is \nnecessary to add the respective new training data t o the pre-\nexisting training data and re-train (probably from scratch) to \naccount for the new class. By contrast, learning is  essentially \nadditive in the brain – if we want to learn a new t hing, we do. \nThus, whilst there is little doubt that the learning  of the \nbrain and machine learning  are essentially the same, the \nlearning of the brain involves the emergent phenome non of \nmemory  which has failed to emerge from machine learning. \nIndeed, recent machine-inspired approaches to ‘memo ry’ have \ninvolved explicit add-on storage  facilities [e.g., 1] which \nexplicitly discriminate between ",
    "title": "Model-Agnostic Interpretability of Machine Learning",
    "abstract": "Understanding why machine learning models behave the way they do empowers\nboth system designers and end-users in many ways: in model selection, feature\nengineering, in order to trust and act upon the predictions, and in more\nintuitive user interfaces. Thus, interpretability has become a vital concern in\nmachine learning, and work in the area of interpretable models has found\nrenewed interest. In some applications, such models are as accurate as\nnon-interpretable ones, and thus are preferred for their transparency. Even\nwhen they are not accurate, they may still be preferred when interpretability\nis of paramount importance. However, restricting machine learning to\ninterpretable models is often a severe limitation. In this paper we argue for\nexplaining machine learning predictions using model-agnostic approaches. By\ntreating the machine learning models as black-box functions, these approaches\nprovide crucial flexibility in the choice of models, explanations, and\nrepresentations, improving debugging, comparison, and interfaces for a variety\nof users and models. We also outline the main challenges for such methods, and\nreview a recently-introduced model-agnostic explanation approach (LIME) that\naddresses these challenges.",
    "link": "http://arxiv.org/abs/1606.05386v1",
    "published": "2016-06-16T23:39:41Z"
  },
  "147": {
    "pdf_path": "data/pdfs\\machine learning_paper_45.pdf",
    "text_excerpt": "1 \n Scientific  Machine Learning Benchmarks  \n \nJeyan Thiyagalingam*, Mallikarjun Shankar†, Geoffrey Fox‡, and Tony Hey* \nAbstract  \n \n \nThe breakthrough in Deep Learning neural networks has transformed the use of AI and machine learning \ntechnologies for the analysis of very large  experimental datasets . These datasets are typically generated \nby large -scale experimental facilities  at national la boratories . In the context of science, scientific machine \nlearning focuses on training machines to identify patterns, trends, and anomalies to extract  meaningful \nscientific insights  from such datasets. With a new  generation of experimental facilities , the rate of data \ngeneration and the  scale of data volumes will increasingly require the use of more automated d ata \nanalysis .  \n \nAt present, i dentifying the most appropriate machine learning algorithm for the analysis of any given \nscientific dataset is still a challenge for scientists. Th is is due to  many different machine learning \nframeworks , computer architectures, and machine learning models . Historically, for modelling and \nsimulation  on HPC systems  such problems have been addressed through benchmarking  computer \napplications, algorithms,  and architectures . Extending  such a benchmarking approach and identifying \nmetrics for  the application of ma chine learning  methods to scientific datasets  is a new challenge for both \nscientists and computer scientists. In this paper, we describe our approach to the development of \nscientific machine learning benchmarks and review other approaches to  benchmarking scientific machine \nlearning .  \n \n1 Introduction  \n \nIn the past decade, a sub -field of artificial intelligence (AI), namely Deep Learn ing (DL) neural networks  (or \ndeep neural networks, DNNs) , has made significant breakthroughs in many  scientifically and commercially \n \n* Rutherford Appleton Laboratory, Science and Technology Facilities Council, Harwell Campus, United Kingdom.  \nt.jeyan @st",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "148": {
    "pdf_path": "data/pdfs\\machine learning_paper_46.pdf",
    "text_excerpt": "Some Insights into Lifelong Reinforcement Learning Systems\nChangjian Li1\nAbstract\nA lifelong reinforcement learning system is a\nlearning system that has the ability to learn\nthrough trail-and-error interaction with the en-\nvironment over its lifetime . In this paper, I give\nsome arguments to show that the traditional rein-\nforcement learning paradigm fails to model this\ntype of learning system. Some insights into life-\nlong reinforcement learning are provided, along\nwith a simplistic prototype lifelong reinforcement\nlearning system.\n1. Introduction\nAnagent is an abstraction of a decision-maker. At each time\ninstancet, it receives an observation ot2O, and outputs an\nactionat2Ato be carried out in the environment it lives in.\nHere,Ois the (ﬁnite) set of possible observations the agent\ncan receive, and Ais the (ﬁnite) set of actions the agent can\nchoose from. An agent’s observation otdepends on the cur-\nrent environment state st2Sthrough an agent observation\nfunctionS!O, whereSis the set of possible environment\nstates. The observation history ho\nt= (o1;o2:::;ot)is the\nsequence of observations the agent has received till timet.\nLetHo\ntbe the set of possible observation histories of length\nt, the policy\u0019t:Ho\nt!Aat timetis deﬁned as the map-\nping from an observation history of length tto the action the\nagent will take. An agent’s behavior can thus be fully speci-\nﬁed by its policy across all timesteps \u0019= (\u00191;\u00192;:::;\u0019t;:::).\nThroughout the paper, it is assumed that an agent has a ﬁnite\nlifespanT.\n1.1. Scalar Reward Reinforcement Learning System\nWe are interested in agents that can achieve some goal. In\nreinforcement learning, a goal is expressed by a scalar signal\nrt2Rcalled the reward . The reward is dependent on the\nagent’s observation history, and is assumed to be available\nto the agent at each timestep in addition to the observation\n1Department of Electrical and Computer Engineering, Uni-\nversity of Waterloo, Canada. Correspondence to: Changjian Li\n<changjian.li@uwater",
    "title": "The Top 10 Topics in Machine Learning Revisited: A Quantitative\n  Meta-Study",
    "abstract": "Which topics of machine learning are most commonly addressed in research?\nThis question was initially answered in 2007 by doing a qualitative survey\namong distinguished researchers. In our study, we revisit this question from a\nquantitative perspective. Concretely, we collect 54K abstracts of papers\npublished between 2007 and 2016 in leading machine learning journals and\nconferences. We then use machine learning in order to determine the top 10\ntopics in machine learning. We not only include models, but provide a holistic\nview across optimization, data, features, etc. This quantitative approach\nallows reducing the bias of surveys. It reveals new and up-to-date insights\ninto what the 10 most prolific topics in machine learning research are. This\nallows researchers to identify popular topics as well as new and rising topics\nfor their research.",
    "link": "http://arxiv.org/abs/1703.10121v1",
    "published": "2017-03-29T16:29:04Z"
  },
  "149": {
    "pdf_path": "data/pdfs\\machine learning_paper_47.pdf",
    "text_excerpt": "Bayesian Optimization for Machine Learning\nA Practical Guidebook\nIan Dewancker Michael McCourt Scott Clark\nSigOpt\nSan Francisco, CA 94108\n{ian, mike, scott}@sigopt.com\nAbstract\nThe engineering of machine learning systems is still a nascent ﬁeld; relying on a\nseemingly daunting collection of quickly evolving tools and best practices. It is\nour hope that this guidebook will serve as a useful resource for machine learning\npractitioners looking to take advantage of Bayesian optimization techniques. We\noutline four example machine learning problems that can be solved using open\nsource machine learning libraries, and highlight the beneﬁts of using Bayesian\noptimization in the context of these common machine learning applications.\n1 Introduction\nRecently, there has been interest in applying Bayesian black-box optimization strategies to better\nconduct optimization over hyperparameter conﬁgurations of machine learning models and systems\n[19] [21] [11]. Most of these techniques require that the objective be a scalar value depending on the\nhyperparamter conﬁguration x.\nxopt= arg max\nx2Xf(x)\nA more detailed introduction to Bayesian optimization and related techniques is provided in [ 8]. The\nfocus of this guidebook is on demonstrating several example problems where Bayesian optimization\nprovides a noted beneﬁt. Our hope is to clearly show how Bayesian optimization can assist in better\ndesigning and optimizing real-world machine learning systems. All of the examples in this guidebook\nhave corresponding code available on SigOpt’s example github repo.\n2 Tuning Text Classiﬁcation Pipelines with scikit-learn\nText classiﬁcation problems appear quite often in modern information systems, and you might\nimagine building a small document / tweet / blogpost classiﬁer for any number of purposes. In this\nexample, the classiﬁcation task is to label Amazon product reviews [ 5] as either favorable or not. The\nobjective is to ﬁnd a classiﬁer that is accurate in its predictions, but also one that",
    "title": "On conditional parity as a notion of non-discrimination in machine\n  learning",
    "abstract": "We identify conditional parity as a general notion of non-discrimination in\nmachine learning. In fact, several recently proposed notions of\nnon-discrimination, including a few counterfactual notions, are instances of\nconditional parity. We show that conditional parity is amenable to statistical\nanalysis by studying randomization as a general mechanism for achieving\nconditional parity and a kernel-based test of conditional parity.",
    "link": "http://arxiv.org/abs/1706.08519v1",
    "published": "2017-06-26T17:41:20Z"
  },
  "150": {
    "pdf_path": "data/pdfs\\machine learning_paper_48.pdf",
    "text_excerpt": "Towards A Rigorous Science of Interpretable Machine Learning\nFinale Doshi-Velez\u0003and Been Kim\u0003\nFrom autonomous cars and adaptive email-\flters to predictive policing systems, machine learn-\ning (ML) systems are increasingly ubiquitous; they outperform humans on speci\fc tasks [Mnih\net al., 2013, Silver et al., 2016, Hamill, 2017] and often guide processes of human understanding\nand decisions [Carton et al., 2016, Doshi-Velez et al., 2014]. The deployment of ML systems in\ncomplex applications has led to a surge of interest in systems optimized not only for expected\ntask performance but also other important criteria such as safety [Otte, 2013, Amodei et al., 2016,\nVarshney and Alemzadeh, 2016], nondiscrimination [Bostrom and Yudkowsky, 2014, Ruggieri et al.,\n2010, Hardt et al., 2016], avoiding technical debt [Sculley et al., 2015], or providing the right to\nexplanation [Goodman and Flaxman, 2016]. For ML systems to be used safely, satisfying these\nauxiliary criteria is critical. However, unlike measures of performance such as accuracy, these crite-\nria often cannot be completely quanti\fed. For example, we might not be able to enumerate all unit\ntests required for the safe operation of a semi-autonomous car or all confounds that might cause\na credit scoring system to be discriminatory. In such cases, a popular fallback is the criterion of\ninterpretability : if the system can explain its reasoning, we then can verify whether that reasoning\nis sound with respect to these auxiliary criteria.\nUnfortunately, there is little consensus on what interpretability in machine learning isand\nhow to evaluate it for benchmarking. Current interpretability evaluation typically falls into two\ncategories. The \frst evaluates interpretability in the context of an application: if the system is useful\nin either a practical application or a simpli\fed version of it, then it must be somehow interpretable\n(e.g. Ribeiro et al. [2016], Lei et al. [2016], Kim et al. [2015a], Doshi-Velez et al. [2015],",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "151": {
    "pdf_path": "data/pdfs\\machine learning_paper_49.pdf",
    "text_excerpt": "Infrastructure for Usable Machine Learning:\nThe Stanford DAWN Project\nPeter Bailis, Kunle Olukotun, Christopher R ´e, Matei Zaharia\nStanford DAWN Project\nhttp://dawn.cs.stanford.edu/\n21 May 2017\nAbstract\nDespite incredible recent advances in machine learning, building machine learning applications remains prohibitively\ntime-consuming and expensive for all but the best-trained, best-funded engineering organizations. This expense comes\nnot from a need for new and improved statistical models but instead from a lack of systems and tools for supporting\nend-to-end machine learning application development, from data preparation and labeling to productionization and\nmonitoring. In this document, we outline opportunities for infrastructure supporting usable, end-to-end machine\nlearning applications in the context of the nascent DAWN (Data Analytics for What’s Next) project at Stanford.\n1 Introduction and DA WN Project Goals\nA Gilded Dawn for Machine Learning and Artiﬁcial Intelligence. We are in the golden age of machine learning\nand artiﬁcial intelligence. Sustained algorithmic advances coupled with the availability of massive datasets and fast\nparallel computing have led to breakthroughs in applications that would have been considered science ﬁction even a few\nyears ago. Over the past ﬁve years, voice-driven personal assistants have become commonplace, image recognition\nsystems have reached human quality, and autonomous vehicles are rapidly becoming a reality. Given these successes,\nthere is no doubt that machine learning will transform most areas of our economy and society. Businesses, governments\nand scientiﬁc labs are clamoring to see how machine learning can tackle their problems.\nUnfortunately, although new machine learning (ML) applications are impressive, they are very expensive to build.\nEvery major new ML product, such as Apple Siri, Amazon Alexa, or Tesla Autopilot, requires large and costly teams of\ndomain experts, data scientists, data engineers, and DevOps. Ev",
    "title": "Privacy Preserving Machine Learning: Threats and Solutions",
    "abstract": "For privacy concerns to be addressed adequately in current machine learning\nsystems, the knowledge gap between the machine learning and privacy communities\nmust be bridged. This article aims to provide an introduction to the\nintersection of both fields with special emphasis on the techniques used to\nprotect the data.",
    "link": "http://arxiv.org/abs/1804.11238v1",
    "published": "2018-03-27T15:10:31Z"
  },
  "152": {
    "pdf_path": "data/pdfs\\machine learning_paper_5.pdf",
    "text_excerpt": "TOWARDS MODULAR MACHINE LEARNING SOLUTION DEVELOPMENT :\nBENEFITS AND TRADE -OFFS\nSamiyuru Menik1Lakshmish Ramaswamy1\nABSTRACT\nMachine learning technologies have demonstrated immense capabilities in various domains. They play a key\nrole in the success of modern businesses. However, adoption of machine learning technologies has a lot of\nuntouched potential. Cost of developing custom machine learning solutions that solve unique business problems\nis a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic\nnature prevalent in today’s machine learning applications stands in the way of efﬁcient and cost effective\ncustomized machine learning solution development. In this work we explore the beneﬁts of modular machine\nlearning solutions and discuss how modular machine learning solutions can overcome some of the major solution\nengineering limitations of monolithic machine learning solutions. We analyze the trade-offs between modular and\nmonolithic machine learning solutions through three deep learning problems; one text based and the two image\nbased. Our experimental results show that modular machine learning solutions have a promising potential to reap\nthe solution engineering advantages of modularity while gaining performance and data advantages in a way the\nmonolithic machine learning solutions do not permit.\n1 I NTRODUCTION\nMachine learning (ML) has gained a lot of attention over the\npast years. Machine learning technologies have become a\npart of many organizational workﬂows and day to day tasks\nof individuals knowingly or unknowingly. Big tech compa-\nnies and academic entities are taking the lead in developing\ncutting edge ML technologies that push the boundaries of\nwhat ML can accomplish. Cutting edge computer vision and\nlanguage modeling technologies provide a good example\nfor this. Beyond the heightened attention, existing appli-\ncations and large scale organization and academic driven\ndevelopments, there is a lot o",
    "title": "Towards Modular Machine Learning Solution Development: Benefits and\n  Trade-offs",
    "abstract": "Machine learning technologies have demonstrated immense capabilities in\nvarious domains. They play a key role in the success of modern businesses.\nHowever, adoption of machine learning technologies has a lot of untouched\npotential. Cost of developing custom machine learning solutions that solve\nunique business problems is a major inhibitor to far-reaching adoption of\nmachine learning technologies. We recognize that the monolithic nature\nprevalent in today's machine learning applications stands in the way of\nefficient and cost effective customized machine learning solution development.\nIn this work we explore the benefits of modular machine learning solutions and\ndiscuss how modular machine learning solutions can overcome some of the major\nsolution engineering limitations of monolithic machine learning solutions. We\nanalyze the trade-offs between modular and monolithic machine learning\nsolutions through three deep learning problems; one text based and the two\nimage based. Our experimental results show that modular machine learning\nsolutions have a promising potential to reap the solution engineering\nadvantages of modularity while gaining performance and data advantages in a way\nthe monolithic machine learning solutions do not permit.",
    "link": "http://arxiv.org/abs/2301.09753v1",
    "published": "2023-01-23T22:54:34Z"
  },
  "153": {
    "pdf_path": "data/pdfs\\machine learning_paper_50.pdf",
    "text_excerpt": "Techniques for Interpretable Machine Learning\nMengnan Du, Ninghao Liu, Xia Hu\nDepartment of Computer Science and Engineering, Texas A&M University\n{dumengnan,nhliu43,xiahu}@tamu.edu\nABSTRACT\nInterpretable machine learning tackles the important prob-\nlem that humans cannot understand the behaviors of com-\nplex machine learning models and how these models arrive at\na particular decision. Although many approaches have been\nproposed, a comprehensive understanding of the achieve-\nments and challenges is still lacking. We provide a survey\ncovering existing techniques to increase the interpretability\nof machine learning models. We also discuss crucial issues\nthat the community should consider in future work such as\ndesigning user-friendly explanations and developing compre-\nhensive evaluation metrics to further push forward the area\nof interpretable machine learning.\n1. INTRODUCTION\nMachine learning is progressing at an astounding rate,\npowered by complex models such as ensemble models and\ndeep neural networks (DNNs). These models have a wide\nrange of real-world applications, such as movie recommenda-\ntions of Net\rix, neural machine translation of Google, speech\nrecognition of Amazon Alexa. Despite the successes, ma-\nchine learning has its own limitations and drawbacks. The\nmost signi\fcant one is the lack of transparency behind their\nbehaviors, which leaves users with little understanding of\nhow particular decisions are made by these models. Con-\nsider, for instance, an advanced self-driving car equipped\nwith various machine learning algorithms doesn't brake or\ndecelerate when confronting a stopped \fretruck. This un-\nexpected behavior may frustrate and confuse users, making\nthem wonder why. Even worse, the wrong decisions could\ncause severe consequences if the car is driving at highway\nspeeds and might \fnally crash the \fretruck. The concerns\nabout the black-box nature of complex models have ham-\npered their further applications in our society, especially in\nthose critical ",
    "title": "An $O(N)$ Sorting Algorithm: Machine Learning Sort",
    "abstract": "We propose an $O(N\\cdot M)$ sorting algorithm by Machine Learning method,\nwhich shows a huge potential sorting big data. This sorting algorithm can be\napplied to parallel sorting and is suitable for GPU or TPU acceleration.\nFurthermore, we discuss the application of this algorithm to sparse hash table.",
    "link": "http://arxiv.org/abs/1805.04272v2",
    "published": "2018-05-11T08:28:55Z"
  },
  "154": {
    "pdf_path": "data/pdfs\\machine learning_paper_51.pdf",
    "text_excerpt": "Solving machine learning optimization problems\nusing quantum computers\nVenkat R. Dasari, Ph.D.a, Mee Seong Im, Ph.D.b, and Lubjana Beshaj, Ph.D.c\naU.S. Army Research Laboratory, Aberdeen Proving Ground, MD 21005\nbU.S. Military Academy, West Point, NY 10996\ncArmy Cyber Institute, West Point, NY 10996\nAbstract —Classical optimization algorithms in machine learn-\ning often take a long time to compute when applied to a multi-\ndimensional problem and require a huge amount of CPU and\nGPU resource. Quantum parallelism has a potential to speed up\nmachine learning algorithms. We describe a generic mathematical\nmodel to leverage quantum parallelism to speed-up machine\nlearning algorithms. We also apply quantum machine learning\nand quantum parallelism applied to a 3-dimensional image that\nvary with time.\nIndex Terms —quantum machine learning, higher-dimensional\ndata sets, quantum computation, quantum parallelism.\nI. I NTRODUCTION\nMachine learning (ML) is generally considered an opti-\nmization problem and sometimes they are computationally\nintense, requiring HPC computing resource to complete the\noptimization. Classical algorithms do not do well when dealing\nwith high dimensional problem space. Optimization problems\nin ML often reach NP-hard in their complexity and will be\ndifﬁcult to handle by the classical computers. One of the\nbeneﬁts of quantum computing is its parallelism which can\nspeed up the optimization computations to quickly compute\nglobal minimum and maximum. In our previous work we\nhave described the programmable quantum networks to sup-\nport quantum applications using a uniﬁed quantum channel,\nachieved through openﬂow abstractions to carry quantum\nmetadata on classical channels [1]. In a different study [2], we\nhave also described a mathematical model to synchronize the\nquantum and classical channels to coordinate the performance\nof quantum application on a multi-node quantum network\ntopology.\nII. M ACHINE LEARNING\nMachine learning is a branch of Artiﬁcial Intelli",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "155": {
    "pdf_path": "data/pdfs\\machine learning_paper_52.pdf",
    "text_excerpt": "Lale: Consistent Automated Machine Learning\nGuillaume Baudart, Martin Hirzel, Kiran Kate, Parikshit Ram, and Avraham Shinnar\nIBM Research, USA\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n1314\n15\n16\n17\n18\n19\n20\nFigure 1: Lale example for consistent automated machine learning, explained in Section 3.\nABSTRACT\nAutomated machine learning makes it easier for data scientists to\ndevelop pipelines by searching over possible choices for hyperpa-\nrameters, algorithms, and even pipeline topologies. Unfortunately,\nthe syntax for automated machine learning tools is inconsistent\nwith manual machine learning, with each other, and with error\nchecks. Furthermore, few tools support advanced features such as\ntopology search or higher-order operators. This paper introduces\nLale , a library of high-level Python interfaces that simplifies and\nunifies automated machine learning in a consistent way.\n1 INTRODUCTION\nMachine learning (ML) is widely used in various data science prob-\nlems. There are many ML operators for data preprocessing (scaling,\nmissing imputation, categorical encoding), feature extraction (prin-\ncipal component analysis, non-negative matrix factorization), and\nmodeling (boosted trees, neural networks). A machine learning\npipeline consists of one or more operators that take the input data\nthrough a series of transformations to finally generate predictions.\nGiven the plethora of ML operators (for example, in the widely\nused scikit-learn library [ 5]), the task of finding a good ML pipeline\nfor the data at hand (which involves not only selecting operators\nbut also appropriately configuring their hyperparameters) can be\ntedious and time consuming if done manually. This has led to a\nwider adoption of automated machine learning (AutoML), with\nAutoML Workshop @ KDD’20, August 22–27, 2020, Page 1\nhttps://sites.google.com/view/automl2020-workshop/.the development of novel algorithms (such as SMAC [ 12], hyper-\nopt [ 3], and subsequent recent work [ 16]), open source libraries\n(auto-sklearn [ 10], ",
    "title": "TherML: Thermodynamics of Machine Learning",
    "abstract": "In this work we offer a framework for reasoning about a wide class of\nexisting objectives in machine learning. We develop a formal correspondence\nbetween this work and thermodynamics and discuss its implications.",
    "link": "http://arxiv.org/abs/1807.04162v3",
    "published": "2018-07-11T14:39:17Z"
  },
  "156": {
    "pdf_path": "data/pdfs\\machine learning_paper_53.pdf",
    "text_excerpt": "DIFFERENTIAL REPLICATION IN MACHINE LEARNING\nA P REPRINT\nIrene Unceta\nBBV A Data & Analytics\nDepartment of Mathematics and Computer Science\nUniversitat de Barcelona\nirene.unceta@bbvadata.com\nJordi Nin\nDepartment of Operations, Innovation and Data Sciences\nUniversitat Ramon Llull, ESADE\njordi.nin@esade.edu\nOriol Pujol\nDepartment of Mathematics and Computer Science\nUniversitat de Barcelona\noriol_pujol@ub.edu\nABSTRACT\nWhen deployed in the wild, machine learning models are usually confronted with data and require-\nments that constantly vary, either because of changes in the generating distribution or because external\nconstraints change the environment where the model operates. To survive in such an ecosystem,\nmachine learning models need to adapt to new conditions by evolving over time. The idea of model\nadaptability has been studied from different perspectives. In this paper, we propose a solution based\non reusing the knowledge acquired by the already deployed machine learning models and leveraging\nit to train future generations. This is the idea behind differential replication of machine learning\nmodels.\n1 Survival of the ﬁttest\n“If during the long course of ages and under varying conditions of life, organic beings vary at all in the several parts of\ntheir organization, [...] I think it would be a most extraordinary fact if no variation ever had occurred useful to each\nbeing’s own welfare, in the same way as so many variations have occurred useful to man. But if variations useful to any\norganic being do occur, assuredly individuals thus characterized will have the best chance of being preserved in the\nstruggle for life; and from the strong principle of inheritance they will tend to produce offspring similarly characterized.\nThis principle of preservation, I have called, for the sake of brevity, Natural Selection. ” [Charles Darwin, Origin of the\nSpecies, p.127, 1859]\nNatural Selection explores how organisms adapt to a changing environment in their struggle for surviva",
    "title": "ML-Schema: Exposing the Semantics of Machine Learning with Schemas and\n  Ontologies",
    "abstract": "The ML-Schema, proposed by the W3C Machine Learning Schema Community Group,\nis a top-level ontology that provides a set of classes, properties, and\nrestrictions for representing and interchanging information on machine learning\nalgorithms, datasets, and experiments. It can be easily extended and\nspecialized and it is also mapped to other more domain-specific ontologies\ndeveloped in the area of machine learning and data mining. In this paper we\noverview existing state-of-the-art machine learning interchange formats and\npresent the first release of ML-Schema, a canonical format resulted of more\nthan seven years of experience among different research institutions. We argue\nthat exposing semantics of machine learning algorithms, models, and experiments\nthrough a canonical format may pave the way to better interpretability and to\nrealistically achieve the full interoperability of experiments regardless of\nplatform or adopted workflow solution.",
    "link": "http://arxiv.org/abs/1807.05351v1",
    "published": "2018-07-14T08:07:31Z"
  },
  "157": {
    "pdf_path": "data/pdfs\\machine learning_paper_54.pdf",
    "text_excerpt": "arXiv:2008.08080v2  [stat.CO]  14 Dec 2020picture(0,0)(-35,0)(1,0)30 (0,35)(0,-1)30 picture picture(0,0)(35,0)(-1,0)30 (0,35)(0,-1)30 picture\n“mlr3proba” — 2020/12/15 — page 1 — #1picture(0,0)(-35,0)(1,0)30 (0,-35)(0,1)30 picture picture(0,0)(35,0)(-1,0)30 (0,-35)(0,1)30 pictureDoc-StartBioinformatics\ndoi.10.1093/bioinformatics/xxxxxx\nAdvance Access Publication Date: Day Month Y ear\nManuscript Category\nSubject Section\nmlr3proba: An R Package for Machine Learning in\nSurvival Analysis\nRaphael Sonabend1,∗, Franz J. Király1, Andreas Bender2, Bernd Bischl2and\nMichel Lang2\n1Department of Statistical Science, University College Lon don, London, WC1E 6BT , UK and\n2Department of Statistics, LMU Munich, Munich, 80539, Germa ny.\n∗T o whom correspondence should be addressed.\nAssociate Editor: XXXXXXX\nReceived on XXXXX; revised on XXXXX; accepted on XXXXX\nAbstract\nMotivation: As machine learning has become increasingly popular over th e last few decades, so\ntoo has the number of machine learning interfaces for implem enting these models. Whilst many R\nlibraries exist for machine learning, very few offer extend ed support for survival analysis. This is\nproblematic considering its importance in ﬁelds like medic ine, bioinformatics, economics, engineering,\nand more. mlr3proba provides a comprehensive machine learning interface for su rvival analysis\nand connects with mlr3 ’s general model tuning and benchmarking facilities to prov ide a systematic\ninfrastructure for survival modeling and evaluation. Availability: mlr3proba is available under an\nLGPL-3 license on CRAN and at https://github.com/mlr-org/ mlr3proba, with further documentation at\nhttps://mlr3book.mlr-org.com/survival.html.\nContact: raphael.sonabend.15@ucl.ac.uk\n1 Introduction\nSurvival analysis is the ﬁeld of statistics concerned with t he estimation of\ntime-to-event distributions while accounting for censori ng and truncation.\nmlr3proba introduces survival modelling to the mlr3 (Lang et al. , 2019a)\necosystem of machine",
    "title": "Characterizing machine learning process: A maturity framework",
    "abstract": "Academic literature on machine learning modeling fails to address how to make\nmachine learning models work for enterprises. For example, existing machine\nlearning processes cannot address how to define business use cases for an AI\napplication, how to convert business requirements from offering managers into\ndata requirements for data scientists, and how to continuously improve AI\napplications in term of accuracy and fairness, and how to customize general\npurpose machine learning models with industry, domain, and use case specific\ndata to make them more accurate for specific situations etc. Making AI work for\nenterprises requires special considerations, tools, methods and processes. In\nthis paper we present a maturity framework for machine learning model lifecycle\nmanagement for enterprises. Our framework is a re-interpretation of the\nsoftware Capability Maturity Model (CMM) for machine learning model development\nprocess. We present a set of best practices from our personal experience of\nbuilding large scale real-world machine learning models to help organizations\nachieve higher levels of maturity independent of their starting point.",
    "link": "http://arxiv.org/abs/1811.04871v1",
    "published": "2018-11-12T17:32:24Z"
  },
  "158": {
    "pdf_path": "data/pdfs\\machine learning_paper_55.pdf",
    "text_excerpt": "Teaching Uncertainty Quantiﬁcation in Machine Learning through Use Cases\nMatias Valdenegro-Toro1\nAbstract\nUncertainty in machine learning is not generally\ntaught as general knowledge in Machine Learning\ncourse curricula. In this paper we propose a short\ncurriculum for a course about uncertainty in ma-\nchine learning, and complement the course with a\nselection of use cases, aimed to trigger discussion\nand let students play with the concepts of uncer-\ntainty in a programming setting. Our use cases\ncover the concept of output uncertainty, Bayesian\nneural networks and weight distributions, sources\nof uncertainty, and out of distribution detection.\nWe expect that this curriculum and set of use cases\nmotivates the community to adopt these important\nconcepts into courses for safety in AI.\n1. Introduction\nNeural networks and machine learning models are ubiqui-\ntous in real-world applications, but in general model and\ndata uncertainty are not well explored, and this propagates\non how machine learning is taught at different levels. Un-\ncertainty is an important concept that should be taught to all\nstudents interested in machine learning.\nOverall Uncertainty Quantiﬁcation of machine learning\nmodels (Gawlikowski et al., 2021) is not part of the stan-\ndard curricula at the undergraduate or graduate level, mostly\nbeing present in advanced summer schools (like MLSS,\nEEML, DeepLearn, SMILES, etc), with some exceptions at\ngraduate courses aimed mostly at theory of Bayesian NNs\n(BNNs).\nIn this paper we aim to develop a concept for teaching\nuncertainty quantiﬁcation in machine learning, ﬁrst with\na short curriculum, and then through different use cases,\nstarting from why we need models with uncertainty and\nending at out of distribution detection. We hope that this\nmaterial can be used for easier planning of future courses.\n1German Research Center for Artiﬁcial Intelligence, Bremen,\nGermany. Correspondence to: Matias Valdenegro-Toro <ma-\ntias.valdenegro@dfki.de >.\nProceedings of the 2n",
    "title": "Towards Identifying and Managing Sources of Uncertainty in AI and\n  Machine Learning Models - An Overview",
    "abstract": "Quantifying and managing uncertainties that occur when data-driven models\nsuch as those provided by AI and machine learning methods are applied is\ncrucial. This whitepaper provides a brief motivation and first overview of the\nstate of the art in identifying and quantifying sources of uncertainty for\ndata-driven components as well as means for analyzing their impact.",
    "link": "http://arxiv.org/abs/1811.11669v1",
    "published": "2018-11-28T16:49:37Z"
  },
  "159": {
    "pdf_path": "data/pdfs\\machine learning_paper_56.pdf",
    "text_excerpt": "Introduction to Machine Learning for Physicians:\nA Survival Guide for Data Deluge\nRiˇcards Marcinkevi ˇcs, MSc, Ece Ozkan, PhD, and Julia E. V ogt, PhD*\nDepartment of Computer Science, ETH Zurich, Zurich, Switzerland\nAbstract\nMany modern research ﬁelds increasingly rely on collecting and analysing massive, often unstructured,\nand unwieldy datasets. Consequently, there is growing interest in machine learning and artiﬁcial intel-\nligence applications that can harness this ‘data deluge’. This broad nontechnical overview provides a\ngentle introduction to machine learning with a speciﬁc focus on medical and biological applications.\nWe explain the common types of machine learning algorithms and typical tasks that can be solved, il-\nlustrating the basics with concrete examples from healthcare. Lastly, we provide an outlook on open\nchallenges, limitations, and potential impacts of machine-learning-powered medicine.\nKeywords: Machine Learning, Artiﬁcial Intelligence, Data Science, Precision Medicine, Predictive Medicine\nMachine learning (ML) is a discipline emerging from computer science [1] with close ties to statistics and\napplied mathematics. Its fundamental goal is the design of computer programs [2], or algorithms , that learn\nto perform a certain task in an automated manner. Without explicit rules or knowledge, ML algorithms\nobserve and possibly, interact with the surrounding world by the use of available data. Typically, as a result\nof learning, algorithms distil observations of complex phenomena into a general model which summarises\nthepatterns , or regularities, discovered from the data.\nModern ML algorithms regularly break records achieving impressive performance at a wide range of tasks,\ne.g.game playing [3], protein structure prediction [4], searching for particles in high-energy physics [5], and\nforecasting precipitation [6]. The utility of machine learning methods for healthcare is apparent: it is often\n*julia.vogt@inf.ethz.ch\n1arXiv:2212.12303v1  [cs.LG]  ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "160": {
    "pdf_path": "data/pdfs\\machine learning_paper_57.pdf",
    "text_excerpt": "Published at the ICLR 2023 Workshop on Machine Learning for Materials\nMACHINE LEARNING -ASSISTED CLOSE -SET X-RAY\nDIFFRACTION PHASE IDENTIFICATION OF TRANSITION\nMETALS\nMaksim Zhdanov\nNUST MISIS\ndwdcodes@gmail.comAndrey Zhdanov\nTomsk Polytechnic University\nzhdanov.andrei24@gmail.com\nABSTRACT\nMachine learning has been applied to the problem of X-ray diffraction phase\nprediction with promising results. In this paper, we describe a method for us-\ning machine learning to predict crystal structure phases from X-ray diffraction\ndata of transition metals and their oxides. We evaluate the performance of our\nmethod and compare the variety of its settings. Our results demonstrate that the\nproposed machine learning framework achieves competitive performance. This\ndemonstrates the potential for machine learning to signiﬁcantly impact the ﬁeld\nof X-ray diffraction and crystal structure determination. Open-source implemen-\ntation: https://github.com/maxnygma/NeuralXRD .\n1 I NTRODUCTION\nThe determination of crystal structure phases from X-ray diffraction data is an important task in\nmaterials science, with applications ranging from drug design (Datta & Grant, 2004) to the study of\ncomplex materials for hydrogen absorption (Akiba et al., 2006). It’s crucial to address the task of\nunnecessary phases of elements which can occur in the process of X-ray diffraction. Notably, oxides\ncan occur in the process of synthesis. Traditionally, this problem has been approached through the\nuse of techniques such as direct methods (Duax et al., 1972) and density functional theory (van de\nStreek & Neumann, 2014). However, these methods can be computationally difﬁcult to operate with\n(Burke, 2012) and may not always yield accurate results (Burke, 2012). In recent years, there has\nbeen a growing interest in using machine learning methods to tackle this problem.\nMachine learning algorithms have the ability to learn patterns and relationships in data, making\nthem well-suited for the analysis of large an",
    "title": "Radiological images and machine learning: trends, perspectives, and\n  prospects",
    "abstract": "The application of machine learning to radiological images is an increasingly\nactive research area that is expected to grow in the next five to ten years.\nRecent advances in machine learning have the potential to recognize and\nclassify complex patterns from different radiological imaging modalities such\nas x-rays, computed tomography, magnetic resonance imaging and positron\nemission tomography imaging. In many applications, machine learning based\nsystems have shown comparable performance to human decision-making. The\napplications of machine learning are the key ingredients of future clinical\ndecision making and monitoring systems. This review covers the fundamental\nconcepts behind various machine learning techniques and their applications in\nseveral radiological imaging areas, such as medical image segmentation, brain\nfunction studies and neurological disease diagnosis, as well as computer-aided\nsystems, image registration, and content-based image retrieval systems.\nSynchronistically, we will briefly discuss current challenges and future\ndirections regarding the application of machine learning in radiological\nimaging. By giving insight on how take advantage of machine learning powered\napplications, we expect that clinicians can prevent and diagnose diseases more\naccurately and efficiently.",
    "link": "http://arxiv.org/abs/1903.11726v1",
    "published": "2019-03-27T23:11:15Z"
  },
  "161": {
    "pdf_path": "data/pdfs\\machine learning_paper_58.pdf",
    "text_excerpt": "arXiv:2306.14624v2  [cs.LG]  23 Jan 2024InsightsFromInsuranceforFairMachineLearning\nChristianFröhlich christian.froehlich@uni-tuebingen.de\nDepartmentofComputerScience\nUniversityofTübingen\nandTübingenAICenter\nRobertC.Williamson bob.williamson@uni-tuebingen.de\nDepartmentofComputerScience\nUniversityofTübingen\nandTübingenAICenter\nAbstract\nWe argue that insurance can act as an analogon for the social situatedness of machine learning\nsystems, hence allowingmachine learning scholars totakeinsightsf rom therich and interdisci-\nplinaryinsuranceliterature. Tracingtheinteractionofuncertainty, fairnessandresponsibilityin\ninsuranceprovidesafreshperspectiveonfairnessinmachinelearning .Welinkinsurancefairness\nconceptionstotheirmachinelearningrelatives,andusethisbridgeto problematizefairnessascal-\nibration. Inthisprocess,webringtotheforefronttwothemesthath avebeenlargelyoverlooked\ninthemachinelearningliterature:responsibilityandaggregate- individualtensions.\n/one.osf Introduction\nInsuranceis“interestinglyuninteresting”./one.osfInthiswork,wearguethatinfactinsuranceisfarfromuninteresting\nandindeedarichsourceofinspirationandinsighttoscholarshipinteres tedinsocialissuessurroundingmachine\nlearning, speciﬁcallytheﬁeldnowknownasfairmachinelearning. Our proposalisthatinsurancecanbeviewed\nasananalogontomachinelearningwithrespecttotheseissuesarising fromthesocialsituatedness.Whilemachine\nlearningisarelativelyrecenttechnology,debatesregarding socialis suesinthecontextofinsurancehavebeen on-\ngoingforalongtime. Thus,wearguethattakinginspirationfromstudie sofinsurancecancontributetoamore\nintegrativeviewofmachinelearningsystemsas socio-technicalsystems(Selbstetal.,/two.osf/zero.osf/one.osf/nine.osf).\nBoth machine learning and insurance are ﬁrmly based on a statistical, p robabilistic mode of reasoning —\nanactuarial mode. Indeed, insurance can be viewed as the ﬁrst commercial test of proba bility theory\n(Gigerenzeretal., /one.osf/nine.osf/eight.osf/nine.osf; McFall, /two.osf/zero.osf/one.osf/one.o",
    "title": "A Game of Dice: Machine Learning and the Question Concerning Art",
    "abstract": "We review some practical and philosophical questions raised by the use of\nmachine learning in creative practice. Beyond the obvious problems regarding\nplagiarism and authorship, we argue that the novelty in AI Art relies mostly on\na narrow machine learning contribution : manifold approximation. Nevertheless,\nthis contribution creates a radical shift in the way we have to consider this\nmovement. Is this omnipotent tool a blessing or a curse for the artists?",
    "link": "http://arxiv.org/abs/1904.01957v1",
    "published": "2019-04-02T09:37:44Z"
  },
  "162": {
    "pdf_path": "data/pdfs\\machine learning_paper_59.pdf",
    "text_excerpt": "QUANTUM DYNAMICS OF MACHINE LEARNING\nPeng WangB\nChengdu Institution of Computer Application\nChinese Academy of Sciences\nChengdu, 610213, Sichuan, China.\nBwp002005@163.comMaimaitiniyazi Maimaitiabudula\nSouthwest Minzu University\nChengdu, 610213, Sichuan, China.\nmai1232021@163.com\nABSTRACT\nThe quantum dynamic equation (QDE) of machine learning is obtained based on Schrödinger\nequation and potential energy equivalence relationship. Through Wick rotation, the relationship\nbetween quantum dynamics and thermodynamics is also established in this paper. This equation\nreformulates the iterative process of machine learning into a time-dependent partial differential\nequation with a clear mathematical structure, offering a theoretical framework for investigating\nmachine learning iterations through quantum and mathematical theories. Within this framework,\nthe fundamental iterative process, the diffusion model, and the Softmax and Sigmoid functions are\nexamined, validating the proposed quantum dynamics equations. This approach not only presents\na rigorous theoretical foundation for machine learning but also holds promise for supporting the\nimplementation of machine learning algorithms on quantum computers.\nKeywords Quantum Dynamics ·Machine Learning ·Diffusion Model ·Quantum Dynamics Framework ·Schrödinger\nEquation ·Quantum Dynamics Equation\n1 Introduction\nMachine learning is a typical optimisation problem, and its learning process is an iterative optimisation process in the\nparameter space. It is a natural way of thinking to consider the iterative motion process of this algorithm as a kinetic\nprocess. The theory of dynamics has been developed over a long period of time and is very complete, with quantum\ndynamics, Newtonian dynamics, thermodynamics, electrodynamics and molecular dynamics, which theoretically\ndescribes the laws of motion by establishing a set of kinetic equations. The establishment of the dynamics theory of\nmachine learning is expected to address the lack of theor",
    "title": "Collaborative Machine Learning Markets with Data-Replication-Robust\n  Payments",
    "abstract": "We study the problem of collaborative machine learning markets where multiple\nparties can achieve improved performance on their machine learning tasks by\ncombining their training data. We discuss desired properties for these machine\nlearning markets in terms of fair revenue distribution and potential threats,\nincluding data replication. We then instantiate a collaborative market for\ncases where parties share a common machine learning task and where parties'\ntasks are different. Our marketplace incentivizes parties to submit high\nquality training and true validation data. To this end, we introduce a novel\npayment division function that is robust-to-replication and customized output\nmodels that perform well only on requested machine learning tasks. In\nexperiments, we validate the assumptions underlying our theoretical analysis\nand show that these are approximately satisfied for commonly used machine\nlearning models.",
    "link": "http://arxiv.org/abs/1911.09052v1",
    "published": "2019-11-08T13:58:31Z"
  },
  "163": {
    "pdf_path": "data/pdfs\\machine learning_paper_6.pdf",
    "text_excerpt": "Introduction to Machine Learning\n67577 - Fall, 2008\nAmnon Shashua\nSchool of Computer Science and Engineering\nThe Hebrew University of Jerusalem\nJerusalem, IsraelarXiv:0904.3664v1  [cs.LG]  23 Apr 2009Contents\n1 Bayesian Decision Theory page 1\n1.1 Independence Constraints 5\n1.1.1 Example: Coin Toss 7\n1.1.2 Example: Gaussian Density Estimation 7\n1.2 Incremental Bayes Classi\fer 9\n1.3 Bayes Classi\fer for 2-class Normal Distributions 10\n2 Maximum Likelihood/ Maximum Entropy Duality 12\n2.1 ML and Empirical Distribution 12\n2.2 Relative Entropy 14\n2.3 Maximum Entropy and Duality ML/MaxEnt 15\n3 EM Algorithm: ML over Mixture of Distributions 19\n3.1 The EM Algorithm: General 21\n3.2 EM with i.i.d. Data 24\n3.3 Back to the Coins Example 24\n3.4 Gaussian Mixture 26\n3.5 Application Examples 27\n3.5.1 Gaussian Mixture and Clustering 27\n3.5.2 Multinomial Mixture and \"bag of words\" Application 27\n4 Support Vector Machines and Kernel Functions 30\n4.1 Large Margin Classi\fer as a Quadratic Linear Programming 31\n4.2 The Support Vector Machine 34\n4.3 The Kernel Trick 36\n4.3.1 The Homogeneous Polynomial Kernel 37\n4.3.2 The non-homogeneous Polynomial Kernel 38\n4.3.3 The RBF Kernel 39\n4.3.4 Classifying New Instances 39\niiiiv Contents\n5 Spectral Analysis I: PCA, LDA, CCA 41\n5.1 PCA: Statistical Perspective 42\n5.1.1 Maximizing the Variance of Output Coordinates 43\n5.1.2 Decorrelation: Diagonalization of the Covariance\nMatrix 46\n5.2 PCA: Optimal Reconstruction 47\n5.3 The Case n>>m 49\n5.4 Kernel PCA 49\n5.5 Fisher's LDA: Basic Idea 50\n5.6 Fisher's LDA: General Derivation 52\n5.7 Fisher's LDA: 2-class 54\n5.8 LDA versus SVM 54\n5.9 Canonical Correlation Analysis 55\n6 Spectral Analysis II: Clustering 58\n6.1 K-means Algorithm for Clustering 59\n6.1.1 Matrix Formulation of K-means 60\n6.2 Min-Cut 62\n6.3 Spectral Clustering: Ratio-Cuts and Normalized-Cuts 63\n6.3.1 Ratio-Cuts 64\n6.3.2 Normalized-Cuts 65\n7 The Formal (PAC) Learning Model 69\n7.1 The Formal Model 69\n7.2 The Rectangle Learning Problem 73\n7.3 Learn",
    "title": "Introduction to Machine Learning: Class Notes 67577",
    "abstract": "Introduction to Machine learning covering Statistical Inference (Bayes, EM,\nML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),\nand PAC learning (the Formal model, VC dimension, Double Sampling theorem).",
    "link": "http://arxiv.org/abs/0904.3664v1",
    "published": "2009-04-23T11:40:57Z"
  },
  "164": {
    "pdf_path": "data/pdfs\\machine learning_paper_60.pdf",
    "text_excerpt": "arXiv:2412.00464v1  [cs.LG]  30 Nov 2024On the Conditions for Domain Stability for\nMachine Learning: a Mathematical Approach\nGabriel Pedroza\n[0000−0002−7889−2892]\nAnsys, France\ngabriel.pedroza@ansys.com, pedrozafm@gmail.com\nwww.linkedin.com/in/gabriel-pedroza-89bb5338\nAbstract. Thisworkproposesamathematical approachthat(re)deﬁnes\na property of Machine Learning models named stability and de termines\nsuﬃcient conditions to validate it. Machine Learning model s are repre-\nsented as functions, and the characteristics in scope depen d upon the\ndomain of the function, what allows us to adopt topological a nd met-\nric spaces theory as a basis. Finally, this work provides som e equiva-\nlences useful to prove and test stability in Machine Learnin g models.\nThe results suggest that whenever stability is aligned with the notion\nof function smoothness, then the stability of Machine Learn ing models\nprimarily depends upon certain topological, measurable pr operties of the\nclassiﬁcation sets within the ML model domain.\nKeywords: Machine Learning, Classiﬁers, Stability, Operational De-\nsign Domain, Metric/Topological Spaces\n1 Introduction\nIn recent years, the study of Artiﬁcial Intelligence and rel ated techniques,\nlike Machine Learning, has attracted considerable attenti on from practi-\ntioners, engineers and researchers from many sectors. Such interest has\nbeen, in many respects, driven by questions and concerns rai sed and\nshared by the involved communities. In particular, it can be mentioned\nthe need for safety of systems integrating AI algorithms and the possibil-\nity to achieve acceptable means for compliance, necessary f or regulation\nand certiﬁcation [3][2][1]. Despite autonomy is a well-kno wn notion within\nthe Systems, SW and HW engineering arenas, the usage of AI tec hnol-\nogy, (GenAI, LLMs, ML/DL) to carry out functions traditiona lly con-\nducted under human supervision and control, induces new cha llenges [4].\nDiscussing referred challenges is out of the scope of this",
    "title": "Analysis of Software Engineering for Agile Machine Learning Projects",
    "abstract": "The number of machine learning, artificial intelligence or data science\nrelated software engineering projects using Agile methodology is increasing.\nHowever, there are very few studies on how such projects work in practice. In\nthis paper, we analyze project issues tracking data taken from Scrum (a popular\ntool for Agile) for several machine learning projects. We compare this data\nwith corresponding data from non-machine learning projects, in an attempt to\nanalyze how machine learning projects are executed differently from normal\nsoftware engineering projects. On analysis, we find that machine learning\nproject issues use different kinds of words to describe issues, have higher\nnumber of exploratory or research oriented tasks as compared to implementation\ntasks, and have a higher number of issues in the product backlog after each\nsprint, denoting that it is more difficult to estimate the duration of machine\nlearning project related tasks in advance. After analyzing this data, we\npropose a few ways in which Agile machine learning projects can be better\nlogged and executed, given their differences with normal software engineering\nprojects.",
    "link": "http://arxiv.org/abs/1912.07323v1",
    "published": "2019-12-16T12:40:26Z"
  },
  "165": {
    "pdf_path": "data/pdfs\\machine learning_paper_61.pdf",
    "text_excerpt": "Distributed Multitask Learning\nJialei Wang\nDepartment of Computer Science\nUniversity of Chicago\njialei@uchicago.edu\nMladen Kolar\nBooth School of Business\nUniversity of Chicago\nmkolar@chicagobooth.edu\nNathan Srebro\nToyota Technological Institute at Chicago\nnati@ttic.edu\nOctober 5, 2015\nAbstract\nWe consider the problem of distributed multi-task learning, where each machine\nlearns a separate, but related, task. Speciﬁcally, each machine learns a linear pre-\ndictor in high-dimensional space, where all tasks share the same small support. We\npresent a communication-efﬁcient estimator based on the debiased lasso and show\nthat it is comparable with the optimal centralized method.\n1 Introduction\nLearning multiple tasks simultaneously allows transferring information between related\ntasks and for improved performance compared to learning each tasks separately [Caru-\nana, 1997]. It has been successfully exploited in, e.g., spam ﬁltering [Weinberger et al.,\n2009], web search [Chapelle et al., 2010], disease prediction [Zhou et al., 2013] and\neQTL mapping [Kim and Xing, 2010].\n1arXiv:1510.00633v1  [stat.ML]  2 Oct 2015Tasks could be related to each other in a number of ways. In this paper, we focus\non the high-dimensional multi-task setting with joint support where a few variables are\nrelated to all tasks, while others are not predictive [Turlach et al., 2005; Obozinski et al.,\n2011; Lounici et al., 2011]. The standard approach is to use the mixed `1=`2or`1=`1\npenalty, as such penalties encourage selection of variables that affect all tasks. Using a\nmixed norm penalty leads to better performance in terms of prediction, estimation and\nmodel selection compared to using the `1norm penalty, which is equivalent to consider-\ning each task separately.\nShared support multi-task learning is generally considered in a centralized setting\nwhere data from all tasks is available on a single machine, and the estimator is computed\nusing a standard single-thread algorithm. With the growth of mode",
    "title": "DriveML: An R Package for Driverless Machine Learning",
    "abstract": "In recent years, the concept of automated machine learning has become very\npopular. Automated Machine Learning (AutoML) mainly refers to the automated\nmethods for model selection and hyper-parameter optimization of various\nalgorithms such as random forests, gradient boosting, neural networks, etc. In\nthis paper, we introduce a new package i.e. DriveML for automated machine\nlearning. DriveML helps in implementing some of the pillars of an automated\nmachine learning pipeline such as automated data preparation, feature\nengineering, model building and model explanation by running the function\ninstead of writing lengthy R codes. The DriveML package is available in CRAN.\nWe compare the DriveML package with other relevant packages in CRAN/Github and\nfind that DriveML performs the best across different parameters. We also\nprovide an illustration by applying the DriveML package with default\nconfiguration on a real world dataset. Overall, the main benefits of DriveML\nare in development time savings, reduce developer's errors, optimal tuning of\nmachine learning models and reproducibility.",
    "link": "http://arxiv.org/abs/2005.00478v3",
    "published": "2020-05-01T16:40:25Z"
  },
  "166": {
    "pdf_path": "data/pdfs\\machine learning_paper_62.pdf",
    "text_excerpt": "Distributed Stochastic Multi-Task Learning with Graph\nRegularization\nWeiran Wang*, Jialei Wang†, Mladen Kolar‡, and Nathan Srebro*\n*Toyota Technological Institute at Chicago, IL, USA\n†Department of Computer Science, University of Chicago, IL, USA\n‡Booth School of Business, University of Chicago, IL, USA\nAbstract\nWe propose methods for distributed graph-based multi-task learning that are based on\nweighted averaging of messages from other machines. Uniform averaging or diminishing stepsize\nin these methods would yield consensus (single task) learning. We show how simply skewing\nthe averaging weights or controlling the stepsize allows learning diﬀerent, but related, tasks on\nthe diﬀerent machines.\n1 Introduction\nWe consider a distributed learning problem in a multi-task setting: each machine ihas access to\nsamples from a diﬀerent data distribution Di, with potentially a diﬀerent optimal predictor, and\nthus a diﬀerent learning task, but where we still assume some similarity between diﬀerent tasks.\nThe goal of each machine is to ﬁnd a good predictor for its own task, based on its own local data,\nas well as communicating with the other machines so as to leverage the similarity to other related\ntasks.\nDistributed multi-task learning lies between a homogeneous distributed learning setting (e.g.\nShamir and Srebro, 2014), where all machines have data from the same source distribution, and\ninhomogeneous consensus problems (e.g. Ram et al., 2010; Boyd et al., 2011; Balcan et al., 2012),\nwhere each machine sees data from a diﬀerent source, but the goal is to reach a single consensus\npredictor. In many distributed learning problems, diﬀerent machines do indeed see diﬀerent dis-\ntributions. For example, machines might serve diﬀerent geographical regions. In a more extreme\n“federated learning” (Konecny et al., 2015) scenario, each machine is a single user device, and its\ndata distribution might reﬂect e.g. the user’s speech, language biases, usage patterns, etc. Such\nheterogeneity ",
    "title": "A combinatorial conjecture from PAC-Bayesian machine learning",
    "abstract": "We present a proof of a combinatorial conjecture from the second author's\nPh.D. thesis. The proof relies on binomial and multinomial sums identities. We\nalso discuss the relevance of the conjecture in the context of PAC-Bayesian\nmachine learning.",
    "link": "http://arxiv.org/abs/2006.01387v2",
    "published": "2020-06-02T04:36:50Z"
  },
  "167": {
    "pdf_path": "data/pdfs\\machine learning_paper_63.pdf",
    "text_excerpt": "arXiv:2106.07032v1  [cs.LG]  13 Jun 2021\n© Shiebler & Gavranovi´ c & Wilson\nThis work is licensed under the\nCreative Commons Attribution License.Category Theory in Machine Learning\nDan Shiebler\nUniversity of Oxford\ndaniel.shiebler@kellog.ox.ac.ukBruno Gavranovi´ c\nUniversity of Strathclyde\nbruno@brunogavranovic.comPaul Wilson\nUniversity of Southampton\npaul@statusfailed.com\nOver the past two decades machine learning has permeated alm ost every realm of technology. At the\nsame time, many researchers have begun using category theor y as a unifying language, facilitating\ncommunication between different scientiﬁc disciplines. I t is therefore unsurprising that there is a\nburgeoning interest in applying category theory to machine learning. We aim to document the moti-\nvations, goals and common themes across these applications . We touch on gradient-based learning,\nprobability, and equivariant learning.\nContents\n1 Introduction 2\n2 Gradient-based Learning 3\n2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2.1.1 Applications, Successes, and Motivation . . . . . . . . . . . . . . . . . . . . . . 3\n2.1.2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.1.3 Big Ideas and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.2 Computing the Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2.1 Cartesian Differential Categories . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2.2 Reverse Derivative Categories . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.2.3 Automatic Differentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.3 Optics and Lenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.4 Para . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.5 Learners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "title": "Integrating Machine Learning with Physics-Based Modeling",
    "abstract": "Machine learning is poised as a very powerful tool that can drastically\nimprove our ability to carry out scientific research. However, many issues need\nto be addressed before this becomes a reality. This article focuses on one\nparticular issue of broad interest: How can we integrate machine learning with\nphysics-based modeling to develop new interpretable and truly reliable physical\nmodels? After introducing the general guidelines, we discuss the two most\nimportant issues for developing machine learning-based physical models:\nImposing physical constraints and obtaining optimal datasets. We also provide a\nsimple and intuitive explanation for the fundamental reasons behind the success\nof modern machine learning, as well as an introduction to the concurrent\nmachine learning framework needed for integrating machine learning with\nphysics-based modeling. Molecular dynamics and moment closure of kinetic\nequations are used as examples to illustrate the main issues discussed. We end\nwith a general discussion on where this integration will lead us to, and where\nthe new frontier will be after machine learning is successfully integrated into\nscientific modeling.",
    "link": "http://arxiv.org/abs/2006.02619v1",
    "published": "2020-06-04T02:35:10Z"
  },
  "168": {
    "pdf_path": "data/pdfs\\machine learning_paper_64.pdf",
    "text_excerpt": "Energy-Harvesting Distributed Machine Learning\nBa¸ sak Güler\nUniversity of California, Riverside\nRiverside, California\nbguler@ece.ucr.eduAylin Yener\nThe Ohio State University\nColumbus, Ohio\nyener@ece.osu.edu\nAbstract —This paper provides a ﬁrst study of utilizing en-\nergy harvesting for sustainable machine learning in distributed\nnetworks. We consider a distributed learning setup in which\na machine learning model is trained over a large number of\ndevices that can harvest energy from the ambient environment,\nand develop a practical learning framework with theoretical\nconvergence guarantees. We demonstrate through numerical\nexperiments that the proposed framework can signiﬁcantly out-\nperform energy-agnostic benchmarks. Our framework is scalable,\nrequires only local estimation of the energy statistics, and can be\napplied to a wide range of distributed training settings, including\nmachine learning in wireless networks, edge computing, and\nmobile internet of things.\nI. I NTRODUCTION\nThe environmental impact of large-scale machine learning\nis a major challenge against the sustainability of future smart\necosystems. For instance, the carbon emission of training\na single machine learning model can get as large as the\nlifetime of ﬁve cars [1]. The environmental impact will be even\ngreater with the emergence of machine learning in distributed\nenvironments, where millions of devices are expected to\nparticipate in training on a regular basis. This, combined\nwith the fact that state-of-the-art machine learning models\nare trained over billions of parameters [2], calls for a novel\ndesign paradigm for large-scale machine learning.\nIn this paper, we propose energy harvesting [3] for the\ndesign of sustainable distributed machine learning systems. We\nconsider a distributed training scenario with Nclients (users),\nwho wish to collaborate to train a machine learning model. Each\nuser holds a local dataset Di, and the goal is to train a machine\nlearning model over the joint dataset D1;:::",
    "title": "Power Consumption Variation over Activation Functions",
    "abstract": "The power that machine learning models consume when making predictions can be\naffected by a model's architecture. This paper presents various estimates of\npower consumption for a range of different activation functions, a core factor\nin neural network model architecture design. Substantial differences in\nhardware performance exist between activation functions. This difference\ninforms how power consumption in machine learning models can be reduced.",
    "link": "http://arxiv.org/abs/2006.07237v1",
    "published": "2020-06-12T14:40:46Z"
  },
  "169": {
    "pdf_path": "data/pdfs\\machine learning_paper_65.pdf",
    "text_excerpt": "arXiv:1909.09248v1  [cs.LG]  19 Sep 2019Representation Learning for Electronic Health Records\nWei-Hung Weng1Peter Szolovits1\nAbstract\nInformation in electronic health records (EHR),\nsuch as clinical narratives, examination reports,\nlab measurements, demographics, and other pa-\ntient encounter entries, can be transformed into\nappropriate data representations that can be used\nfor downstream clinical machine learning tasks\nusing representation learning. Learning better\nrepresentations is critical to improve the perfor-\nmance of downstream tasks. Due to the advances\nin machine learning, we now can learn better and\nmeaningful representations from EHR through\ndisentangling the underlying factors inside data\nand distilling large amounts of information and\nknowledge from heterogeneous EHR sources.\nIn this chapter, we ﬁrst introduce the background\nof learning representations and reasons why we\nneed good EHR representations in machine learn-\ning for medicine and healthcare in Section 1.\nNext, we explain the commonly-used machine\nlearning and evaluation methods for representa-\ntion learning using a deep learning approach in\nSection 2. Following that, we review recent re-\nlated studies of learning patient state represen-\ntation from EHR for clinical machine learning\ntasks in Section 3. Finally, in Section 4 we dis-\ncuss more techniques, studies, and challenges for\nlearning natural language representations when\nfree texts, such as clinical notes, examination\nreports, or biomedical literature are used. We\nalso discuss challenges and opportunities in these\nrapidly growing research ﬁelds.\n1. Learning Representations for Medicine and\nHealthcare\nMedicine and healthcare has become one of the key applied\nmachine learning research domains due to increasing adop-\ntion of electronic health records (EHR) and the increasing\n1MIT CSAIL, Cambridge, MA, USA. Correspondence to:\nWei-Hung Weng <ckbjimmy@mit.edu >.power of computation ( Charles et al. ,2013 ;Topol ,2019 ).\nResearchers have framed ",
    "title": "Classification with Quantum Machine Learning: A Survey",
    "abstract": "Due to the superiority and noteworthy progress of Quantum Computing (QC) in a\nlot of applications such as cryptography, chemistry, Big data, machine\nlearning, optimization, Internet of Things (IoT), Blockchain, communication,\nand many more. Fully towards to combine classical machine learning (ML) with\nQuantum Information Processing (QIP) to build a new field in the quantum world\nis called Quantum Machine Learning (QML) to solve and improve problems that\ndisplayed in classical machine learning (e.g. time and energy consumption,\nkernel estimation). The aim of this paper presents and summarizes a\ncomprehensive survey of the state-of-the-art advances in Quantum Machine\nLearning (QML). Especially, recent QML classification works. Also, we cover\nabout 30 publications that are published lately in Quantum Machine Learning\n(QML). we propose a classification scheme in the quantum world and discuss\nencoding methods for mapping classical data to quantum data. Then, we provide\nquantum subroutines and some methods of Quantum Computing (QC) in improving\nperformance and speed up of classical Machine Learning (ML). And also some of\nQML applications in various fields, challenges, and future vision will be\npresented.",
    "link": "http://arxiv.org/abs/2006.12270v1",
    "published": "2020-06-22T14:05:31Z"
  },
  "170": {
    "pdf_path": "data/pdfs\\machine learning_paper_66.pdf",
    "text_excerpt": "arXiv:1810.03548v1  [cs.LG]  8 Oct 2018Meta-Learning: A Survey\nMeta-Learning: A Survey\nJoaquin Vanschoren j.vanschoren@tue.nl\nEindhoven University of Technology\n5600MB Eindhoven, The Netherlands\nAbstract\nMeta-learning, or learning to learn , is the science of systematically observing how diﬀerent\nmachine learning approaches perform on a wide range of learning tas ks, and then learning\nfrom this experience, or meta-data , to learn new tasks much faster than otherwise possible.\nNot only does this dramatically speed up and improve the design of mac hine learning\npipelines or neural architectures, it also allows us to replace hand-e ngineered algorithms\nwith novelapproacheslearnedin a data-drivenway. In this chapte r, weprovideanoverview\nof the state of the art in this fascinating and continuously evolving ﬁ eld.\n1. Introduction\nWhenwelearnnewskills, werarely -if ever -start fromscrat ch. We startfromskills learned\nearlier in related tasks, reuseapproaches that worked well before, and focus on what is likely\nworth trying based on experience (Lake et al., 2017). With ev ery skill learned, learning\nnew skills becomes easier, requiring fewer examples and les s trial-and-error. In short, we\nlearn how to learn across tasks. Likewise, when building machine learning mod els for a\nspeciﬁc task, we often build on experience with related task s, or use our (often implicit)\nunderstandingofthebehaviorof machinelearningtechniqu es tohelpmake theright choices.\nThe challenge in meta-learning is to learn from prior experi ence in a systematic, data-\ndriven way. First, we need to collect meta-data that describe prior learning tasks and\npreviously learned models. They comprise the exact algorithm conﬁgurations used to train\nthe models, including hyperparameter settings, pipeline c ompositions and/or network ar-\nchitectures, the resulting model evaluations , such as accuracy and training time, the learned\nmodel parameters, such as the trained weights of a neural net , as well as measurabl",
    "title": "Machine Learning and Computational Mathematics",
    "abstract": "Neural network-based machine learning is capable of approximating functions\nin very high dimension with unprecedented efficiency and accuracy. This has\nopened up many exciting new possibilities, not just in traditional areas of\nartificial intelligence, but also in scientific computing and computational\nscience. At the same time, machine learning has also acquired the reputation of\nbeing a set of \"black box\" type of tricks, without fundamental principles. This\nhas been a real obstacle for making further progress in machine learning. In\nthis article, we try to address the following two very important questions: (1)\nHow machine learning has already impacted and will further impact computational\nmathematics, scientific computing and computational science? (2) How\ncomputational mathematics, particularly numerical analysis, {can} impact\nmachine learning? We describe some of the most important progress that has been\nmade on these issues. Our hope is to put things into a perspective that will\nhelp to integrate machine learning with computational mathematics.",
    "link": "http://arxiv.org/abs/2009.14596v1",
    "published": "2020-09-23T23:16:46Z"
  },
  "171": {
    "pdf_path": "data/pdfs\\machine learning_paper_67.pdf",
    "text_excerpt": "Introduction to intelligent computing unit 1\nIsa Inuwa-Dutse\ndutsei@edgehill.ac.uk\nNovember 20, 2017\nAbstract\nThis brief note highlights some basic concepts required toward understanding the evolution\nof machine learning and deep learning models. The note starts with an overview of arti\fcial\nintelligence and its relationship to biological neuron that ultimately led to the evolution of\ntodays intelligent models.\niarXiv:1711.06552v1  [cs.LG]  15 Nov 2017Contents\nAbstract i\n1 Introduction 1\n1.1 Arti\fcial Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Information processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.3 Suitability of ANN in solving real life problems . . . . . . . . . . . . . . . . . . . . . 2\n1.4 Intelligent computing unit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2 Arti\fcial Neural Network 4\n2.1 Learning Processes in ANN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.2 Training ANN to learn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2.1 Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2.2 Unsupervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.2.3 Reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.3 ANN Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.3.1 Feedforward architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.3.2 Feedback architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.4 Activation Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.4.1 Step Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.4.2 Sigmoid Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.4.3 Model's Learning Rate . . . . . . . . .",
    "title": "Risk Assessment for Machine Learning Models",
    "abstract": "In this paper we propose a framework for assessing the risk associated with\ndeploying a machine learning model in a specified environment. For that we\ncarry over the risk definition from decision theory to machine learning. We\ndevelop and implement a method that allows to define deployment scenarios, test\nthe machine learning model under the conditions specified in each scenario, and\nestimate the damage associated with the output of the machine learning model\nunder test. Using the likelihood of each scenario together with the estimated\ndamage we define \\emph{key risk indicators} of a machine learning model.\n  The definition of scenarios and weighting by their likelihood allows for\nstandardized risk assessment in machine learning throughout multiple domains of\napplication. In particular, in our framework, the robustness of a machine\nlearning model to random input corruptions, distributional shifts caused by a\nchanging environment, and adversarial perturbations can be assessed.",
    "link": "http://arxiv.org/abs/2011.04328v1",
    "published": "2020-11-09T10:50:50Z"
  },
  "172": {
    "pdf_path": "data/pdfs\\machine learning_paper_68.pdf",
    "text_excerpt": "In-Machine-Learning Database:\nReimagining Deep Learning with Old-School SQL\nLen Du\nAustralian National University\nlen.du@anu.edu.au\nABSTRACT\nIn-database machine learning has been very popular, almost\nbeing a cliche. However, can we do it the other way around?\nIn this work, we say yes by applying plain old SQL to deep\nlearning, in a sense implementing deep learning algorithms\nwith SQL.\nMost deep learning frameworks, as well as generic ma-\nchine learning ones, share a de facto standard of multidi-\nmensional array operations, underneath fancier infrastruc-\nture such as automatic di\u000berentiation. As SQL tables can\nbe regarded as generalisations of (multi-dimensional) arrays,\nwe have found a way to express common deep learning oper-\nations in SQL, encouraging a di\u000berent way of thinking and\nthus potentially novel models. In particular, one of the lat-\nest trend in deep learning was the introduction of sparsity in\nthe name of graph convolutional networks, whereas we take\nsparsity almost for granted in the database world.\nAs both databases and machine learning involve trans-\nformation of datasets, we hope this work can inspire further\nworks utilizing the large body of existing wisdom, algorithms\nand technologies in the database \feld to advance the state of\nthe art in machine learning, rather than merely integerating\nmachine learning into databases.\n1. INTRODUCTION\nBoth machine learning and databases obviously involve\ntransformation of (or computation over) collections of num-\nbers. Combining the two \felds is then an obvious conclusion.\nBut the way of such fusion seems to have been unilateral.\nMuch more e\u000bort has been spent towards providing machine\nlearning capabilities in a database context, or so-called In-\nDatabase Machine Learning [18], compared to integeration\nin the opposite direction, which we call In-Machine-Learning\nDatabase.\nWe speculate that the connotation of databases has been\nmore towards systems than towards algorithms, compared to\nthat of machine learning, ma",
    "title": "Adversarial Machine Learning Attacks on Condition-Based Maintenance\n  Capabilities",
    "abstract": "Condition-based maintenance (CBM) strategies exploit machine learning models\nto assess the health status of systems based on the collected data from the\nphysical environment, while machine learning models are vulnerable to\nadversarial attacks. A malicious adversary can manipulate the collected data to\ndeceive the machine learning model and affect the CBM system's performance.\nAdversarial machine learning techniques introduced in the computer vision\ndomain can be used to make stealthy attacks on CBM systems by adding\nperturbation to data to confuse trained models. The stealthy nature causes\ndifficulty and delay in detection of the attacks. In this paper, adversarial\nmachine learning in the domain of CBM is introduced. A case study shows how\nadversarial machine learning can be used to attack CBM capabilities.\nAdversarial samples are crafted using the Fast Gradient Sign method, and the\nperformance of a CBM system under attack is investigated. The obtained results\nreveal that CBM systems are vulnerable to adversarial machine learning attacks\nand defense strategies need to be considered.",
    "link": "http://arxiv.org/abs/2101.12097v1",
    "published": "2021-01-28T16:34:04Z"
  },
  "173": {
    "pdf_path": "data/pdfs\\machine learning_paper_69.pdf",
    "text_excerpt": "Machine Learning Interpretability: A Science rather than a tool\nAbdul Karim1, Avinash Mishra1, MA Hakim Newton1Abdul Sattar1\n1Institute of Integrated and Intelligent Systems, Grifﬁth University, Queensland, Nathan, Australia\nabdul.karim@grifﬁthuni.edu.au\nAbstract\nThe term “interpretability” is oftenly used by ma-\nchine learning researchers each with their own in-\ntuitive understanding of it. There is no univer-\nsal well agreed upon deﬁnition of interpretability\nin machine learning. As any type of science dis-\ncipline is mainly driven by the set of formulated\nquestions rather than by different tools in that disci-\npline, e.g. astrophysics is the discipline that learns\nthe composition of stars, not as the discipline that\nuse the spectroscopes. Similarly, we propose that\nmachine learning interpretability should be a disci-\npline that answers speciﬁc questions related to in-\nterpretability. These questions can be of statistical\n(associational), causal and counterfactual nature.\nTherefore, there is a need to look into the inter-\npretability problem of machine learning in the con-\ntext of questions that need to be addressed rather\nthan different tools. We discuss about a hypothet-\nical interpretability framework driven by a ques-\ntion based scientiﬁc approach rather than some spe-\nciﬁc machine learning model. Using a question\nbased notion of interpretability, we can step to-\nwards understanding the science of machine learn-\ning rather than its engineering. This notion will also\nhelp us understanding any speciﬁc problem more in\ndepth rather than relying solely on machine learn-\ning methods.\n1 Introduction\nA dramatic hype in the success of machine learning (ML)\nmodels achieving the human level performance in many dif-\nferent areas led to a great need of deploying these systems\nin real world applications. The ongoing research aims to pro-\nduce autonomous end to end systems which will perceive, ob-\nserve the surrounding, learn from the data and surrounding,\nand take a decisio",
    "title": "Confronting Machine Learning With Financial Research",
    "abstract": "This study aims to examine the challenges and applications of machine\nlearning for financial research. Machine learning algorithms have been\ndeveloped for certain data environments which substantially differ from the one\nwe encounter in finance. Not only do difficulties arise due to some of the\nidiosyncrasies of financial markets, there is a fundamental tension between the\nunderlying paradigm of machine learning and the research philosophy in\nfinancial economics. Given the peculiar features of financial markets and the\nempirical framework within social science, various adjustments have to be made\nto the conventional machine learning methodology. We discuss some of the main\nchallenges of machine learning in finance and examine how these could be\naccounted for. Despite some of the challenges, we argue that machine learning\ncould be unified with financial research to become a robust complement to the\neconometrician's toolbox. Moreover, we discuss the various applications of\nmachine learning in the research process such as estimation, empirical\ndiscovery, testing, causal inference and prediction.",
    "link": "http://arxiv.org/abs/2103.00366v2",
    "published": "2021-02-28T01:10:09Z"
  },
  "174": {
    "pdf_path": "data/pdfs\\machine learning_paper_7.pdf",
    "text_excerpt": "The Tribes of Machine Learning and the Realm of Computer\nArchitecture\nAYAZ AKRAM, University of California, Davis\nJASON LOWE-POWER, University of California, Davis\nMachine learning techniques have influenced the field of computer architecture like many other fields. This\npaper studies how the fundamental machine learning techniques can be applied towards computer architecture\nproblems. We also provide a detailed survey of computer architecture research that employs different machine\nlearning methods. Finally, we present some future opportunities and the outstanding challenges that need to\nbe overcome to exploit full potential of machine learning for computer architecture.\nAdditional Key Words and Phrases: machine learning, computer architecture\n1 INTRODUCTION\nMachine learning (ML) refers to the process in which computers learn to make decisions based on\nthe given data set without being explicitly programmed to do so [ 8]. There are various classifications\nof the ML algorithms. One of the more insightful classifications has been done by Pedro Domingos\nin his book The Master Algorithm [39]. Domingos presents five fundamental tribes of ML: the\nsymbolists, the connectionists, the evolutionaries, the bayesians and the analogizers. Each of these\nbelieve in a different strategy to go through the learning process. These tribes or schools of thought\nof ML along-with their primary algorithms and origins are shown in Table 1. There are existing\nproofs that given the enough amount of data, each of these algorithms can fundamentally learn\nanything. Most of the well known ML techniques/algorithms1belong to one of these tribes of ML.\nTable 1. Five Tribes of ML (taken from [39])\nTribe Origins Master Algorithms\nSymbolists Logic, philosophy Inverse deduction\nConnectionists Neuroscience Backpropagation\nEvolutionaries Evolutionary biology Genetic programming\nBayesians Statistics Probabilistic infernce\nAnalogizers Psychology Kernel machines\nIn this paper, we look at these five school of",
    "title": "The Tribes of Machine Learning and the Realm of Computer Architecture",
    "abstract": "Machine learning techniques have influenced the field of computer\narchitecture like many other fields. This paper studies how the fundamental\nmachine learning techniques can be applied towards computer architecture\nproblems. We also provide a detailed survey of computer architecture research\nthat employs different machine learning methods. Finally, we present some\nfuture opportunities and the outstanding challenges that need to be overcome to\nexploit full potential of machine learning for computer architecture.",
    "link": "http://arxiv.org/abs/2012.04105v1",
    "published": "2020-12-07T23:10:51Z"
  },
  "175": {
    "pdf_path": "data/pdfs\\machine learning_paper_70.pdf",
    "text_excerpt": "Automated Machine Learning on Graphs: A Survey\nZiwei Zhang\u0003,Xin Wang\u0003and Wenwu Zhuy\nTsinghua University, Beijing, China\nzw-zhang16@mails.tsinghua.edu.cn, fxinwang,wwzhug@tsinghua.edu.cn\nAbstract\nMachine learning on graphs has been extensively\nstudied in both academic and industry. However,\nas the literature on graph learning booms with a\nvast number of emerging methods and techniques,\nit becomes increasingly difﬁcult to manually design\nthe optimal machine learning algorithm for differ-\nent graph-related tasks. To solve this critical chal-\nlenge, automated machine learning (AutoML) on\ngraphs which combines the strength of graph ma-\nchine learning and AutoML together, is gaining at-\ntention from the research community. Therefore,\nwe comprehensively survey AutoML on graphs in\nthis paper1, primarily focusing on hyper-parameter\noptimization (HPO) and neural architecture search\n(NAS) for graph machine learning. We further\noverview libraries related to automated graph ma-\nchine learning and in-depth discuss AutoGL, the\nﬁrst dedicated open-source library for AutoML on\ngraphs. In the end, we share our insights on fu-\nture research directions for automated graph ma-\nchine learning. This paper is the ﬁrst systematic\nand comprehensive review of automated machine\nlearning on graphs to the best of our knowledge.\n1 Introduction\nGraph data is ubiquitous in our daily life. We can use graphs\nto model the complex relationships and dependencies be-\ntween entities ranging from small molecules in proteins and\nparticles in physical simulations to large national-wide power\ngrids and global airlines. Therefore, machine learning on\ngraphs has long been an important research direction for\nboth academics and industry [1]. In particular, network em-\nbedding [2; 3; 4; 5 ]and graph neural networks (GNNs) [6;\n7; 8]have drawn increasing attention in the last decade.\nThey are successfully applied to recommendation systems [9;\n10], fraud detection [11], bioinformatics [12; 13 ], physical\n\u0003Equal contr",
    "title": "New Trends in Quantum Machine Learning",
    "abstract": "Here we will give a perspective on new possible interplays between Machine\nLearning and Quantum Physics, including also practical cases and applications.\nWe will explore the ways in which machine learning could benefit from new\nquantum technologies and algorithms to find new ways to speed up their\ncomputations by breakthroughs in physical hardware, as well as to improve\nexisting models or devise new learning schemes in the quantum domain. Moreover,\nthere are lots of experiments in quantum physics that do generate incredible\namounts of data and machine learning would be a great tool to analyze those and\nmake predictions, or even control the experiment itself. On top of that, data\nvisualization techniques and other schemes borrowed from machine learning can\nbe of great use to theoreticians to have better intuition on the structure of\ncomplex manifolds or to make predictions on theoretical models. This new\nresearch field, named as Quantum Machine Learning, is very rapidly growing\nsince it is expected to provide huge advantages over its classical counterpart\nand deeper investigations are timely needed since they can be already tested on\nthe already commercially available quantum machines.",
    "link": "http://arxiv.org/abs/2108.09664v1",
    "published": "2021-08-22T08:23:30Z"
  },
  "176": {
    "pdf_path": "data/pdfs\\machine learning_paper_71.pdf",
    "text_excerpt": "arXiv:2201.06921v1  [cs.CY]  13 Dec 2021Can Machine Learning be Moral?\nMiguel Sicart\nDigital Design Department\nIT University of Copenhagen\nDenmark\nmiguel@itu.dkIrina Shklovski\nDepartment of Computer Science\nUniversity of Copenhagen\nDenmark\nias@di.ku.dk\nMirabelle Jones\nDepartment of Computer Science\nUniversity of Copenhagen\nDenmark\nmsd@di.ku.dk\n1 Introduction\nThe ethics of Machine Learning has become an unavoidable top ic in the AI Community. The de-\nployment of machine learning systems in multiple social con texts has resulted in a closer ethical\nscrutiny of the design, development, and application of the se systems. The AI/ML community has\ncome to terms with the imperative to think about the ethical i mplications of machine learning, not\nonly as a product but also as a practice (Birhane, 2021; Shen e t al. 2021). The critical question that\nis troubling many debates is what can constitute an ethicall y accountable machine learning system.\nIn this paper we explore possibilities for ethical evaluati on of machine learning methodologies. We\nscrutinize techniques, methods and technical practices in machine learning from a relational ethics\nperspective, taking into consideration how machine learni ng systems are part of the world and how\nthey relate to different forms of agency. Taking a page from P hil Agre (1997) we use the notion\nof a critical technical practice as a means of analysis of mac hine learning approaches. Our radical\nproposal is that supervised learning appears to be the only m achine learning method that is ethically\ndefensible.\n2 Machine Learning and its Ethical Discontent\nIssues like bias and the moral challenges of data gathering a nd classiﬁcation are central to the prob-\nlems of developing and deploying ethically accountable mac hine learning systems (Greene et al.\n2019). Where data and bias emerged as the primary movers to br ing attention to the potential ethical\nfoibles of machine learning, we know that these are not the wh ole story. In fact prob",
    "title": "Systematic Training and Testing for Machine Learning Using Combinatorial\n  Interaction Testing",
    "abstract": "This paper demonstrates the systematic use of combinatorial coverage for\nselecting and characterizing test and training sets for machine learning\nmodels. The presented work adapts combinatorial interaction testing, which has\nbeen successfully leveraged in identifying faults in software testing, to\ncharacterize data used in machine learning. The MNIST hand-written digits data\nis used to demonstrate that combinatorial coverage can be used to select test\nsets that stress machine learning model performance, to select training sets\nthat lead to robust model performance, and to select data for fine-tuning\nmodels to new domains. Thus, the results posit combinatorial coverage as a\nholistic approach to training and testing for machine learning. In contrast to\nprior work which has focused on the use of coverage in regard to the internal\nof neural networks, this paper considers coverage over simple features derived\nfrom inputs and outputs. Thus, this paper addresses the case where the supplier\nof test and training sets for machine learning models does not have\nintellectual property rights to the models themselves. Finally, the paper\naddresses prior criticism of combinatorial coverage and provides a rebuttal\nwhich advocates the use of coverage metrics in machine learning applications.",
    "link": "http://arxiv.org/abs/2201.12428v1",
    "published": "2022-01-28T21:33:31Z"
  },
  "177": {
    "pdf_path": "data/pdfs\\machine learning_paper_72.pdf",
    "text_excerpt": "Compressive Classiﬁcation (Machine Learning without learning)\nVincent Schellekens\u0003and Laurent Jacques\u0003\nAbstract— Compressive learning is a framework where (so far\nunsupervised) learning tasks use not the entire dataset but a com-\npressed summary (sketch) of it. We propose a compressive learn-\ning classiﬁcation method, and a novel sketch function for images.\n1 Introduction and background\nMachine Learning (ML)—inferring models from datasets of\nnumerous learning examples—recently showed unparalleled\nsuccess on a wide variety of problems. However, modern mas-\nsive datasets necessitate a long training time and large memory\nstorage. The recent Compressive Learning (CL) framework al-\nleviates those drawbacks by computing a compressed summary\nof the dataset—its sketch —prior to any learning [1]. The sketch\nis easily computed in a single parallelizable pass, and its re-\nquired size (to capture enough information for successful learn-\ning) does not grow with the number of examples: CLs time and\nmemory requirements are thus unaffected by the dataset size.\nSo far, CL focused on unsupervised ML tasks, where learn-\ning examples don’t belong to a (known) class [1, 2, 3]. We\nshow that CL easily extends to supervised ML tasks by propos-\ning (Sec. 2) and experimentally validating (Sec. 3) a ﬁrst simple\ncompressive classiﬁcation method using only a sketch of the la-\nbeled dataset (Fig. 1). We also introduce a sketch feature func-\ntion leveraging a random convolutional neural network to bet-\nter capture information in images. While not as accurate as ML\nmethods learning from the full dataset, this compressive clas-\nsiﬁcation scheme still attains remarkable accuracy considering\nitsunlearned nature . Our method also enjoys from a nice geo-\nmetric interpretation, i.e., Maximum A Posteriori classiﬁcation\nperformed in the Reproducible Kernel Hilbert Space associated\nwith the sketch.\n(Unsupervised) Compressive Learning: Unsupervised ML\nusually amount to estimate parameters of a distribution ",
    "title": "Software Testing for Machine Learning",
    "abstract": "Machine learning has become prevalent across a wide variety of applications.\nUnfortunately, machine learning has also shown to be susceptible to deception,\nleading to errors, and even fatal failures. This circumstance calls into\nquestion the widespread use of machine learning, especially in safety-critical\napplications, unless we are able to assure its correctness and trustworthiness\nproperties. Software verification and testing are established technique for\nassuring such properties, for example by detecting errors. However, software\ntesting challenges for machine learning are vast and profuse - yet critical to\naddress. This summary talk discusses the current state-of-the-art of software\ntesting for machine learning. More specifically, it discusses six key challenge\nareas for software testing of machine learning systems, examines current\napproaches to these challenges and highlights their limitations. The paper\nprovides a research agenda with elaborated directions for making progress\ntoward advancing the state-of-the-art on testing of machine learning.",
    "link": "http://arxiv.org/abs/2205.00210v1",
    "published": "2022-04-30T08:47:10Z"
  },
  "178": {
    "pdf_path": "data/pdfs\\machine learning_paper_73.pdf",
    "text_excerpt": "A Survey  on Resilient  Machine Learning \n  \nAtul Kumar, Sameep Mehta  \nIBM Research, India   \n \n \nABSTRACT  \nMachine learning based system are increasingly being used for \nsensitive tasks such as security surveillance , guiding autonomous \nvehicle, taking investment decisions, detecting and blocking \nnetwork intrusion and malware etc. However, recent  research has \nshown that machine learning models ar e venerable  to attacks by \nadversaries at all phases of machine learning (e.g., training data \ncollection, training, operation). All model classes of machine \nlearning systems can be misled by  providing carefully crafted \ninputs  making them wrongly classify in puts. Maliciously created \ninput samples can affect the learning process of a ML system by \neither slowing the learning process, or affecting the performance of \nthe learned model or causing the system make error only in \nattacker’s planned scenario.  Because of these developments, \nunderstanding security  of machin e learning algorithms and systems  \nis emerging as  an important research area among computer  security \nand machine learning researchers and practitioners . We present a \nsurvey of this emerging area.  \nCatego ries and Subject Descriptors  \nD.4.6 [Security and Protection ]: Invasive software ; I.2.6 \n[Learning ]: Concept learning ; I.5.1 [Models ]: Neural nets ; I.5.2 \n[Design Methodology ]:  Classifier design and evaluation . \nGeneral Terms  \nAlgorithms, Design, Security, Theory.  \nKeywords  \nResilience, Adversari al Learning, Computer Security , Intrusion \nDetection . \n1. INTRODUCTION  \nOver last few years, machine leaning has become a prominent \ntechnological tool  in several application areas such  as computer \nvision, speech recognition,  natural language understanding , \nrecommender systems , information retrieval , computer gaming, \nmedical diagnosis , market analysis  etc. In many areas, it is no \nlonger a promising but immature technology as machine learning \nbased systems have reached ",
    "title": "PSI Draft Specification",
    "abstract": "This document presents the draft specification for delivering machine\nlearning services over HTTP, developed as part of the Protocols and Structures\nfor Inference project, which concluded in 2013. It presents the motivation for\nproviding machine learning as a service, followed by a description of the\nessential and optional components of such a service.",
    "link": "http://arxiv.org/abs/2205.09488v1",
    "published": "2022-05-02T02:42:16Z"
  },
  "179": {
    "pdf_path": "data/pdfs\\machine learning_paper_74.pdf",
    "text_excerpt": "An Introduction to MM Algorithms for Machine\nLearning and Statistical Estimation\nHien D. Nguyen\nNovember 12, 2016\nSchool of Mathematics and Physics, University of Queensland, St. Lucia.\nCentre for Advanced Imaging, University of Queensland, St. Lucia.\nAbstract\nMM (majorization–minimization) algorithms are an increasingly pop-\nular tool for solving optimization problems in machine learning and sta-\ntistical estimation. This article introduces the MM algorithm framework\nin general and via three popular example applications: Gaussian mix-\nture regressions, multinomial logistic regressions, and support vector ma-\nchines. Speciﬁc algorithms for the three examples are derived and numer-\nical demonstrations are presented. Theoretical and practical aspects of\nMM algorithm design are discussed.\n1 Introduction\nLetX∈X⊂RpandY∈Y⊂Rqbe random variables, which we shall refer to\nas the input and target variables, respectively. We shall denote a sample of nin-\ndependent and identically distributed (IID) pairs of variables D={(Xi,Yi)}n\ni=1\n1\narXiv:1611.03969v1  [stat.CO]  12 Nov 2016as the data, and ¯D={(xi,yi)}n\ni=1as an observed realization of the data. Under\nthe empirical risk minimization (ERM) framework of Vapnik (1998, Ch. 1) or\nthe extremum estimation (EE) framework of Amemiya (1985, Ch. 4), a large\nnumber of machine learning and statistical estimation problems can be phrased\nas the computation of\nmin\nθ∈ΘR/parenleftbig\nθ;¯D/parenrightbig\norˆθ= arg min\nθ∈ΘR/parenleftbig\nθ;¯D/parenrightbig\n, (1)\nwhereR/parenleftbig\nθ;¯D/parenrightbig\nis a risk function deﬁned over the observed data ¯Dand is de-\npendent on some parameter θ∈Θ.\nCommonriskfunctionsthatareusedinpracticearethenegativelog-likelihood\nfunctions, which can be expressed as\nR/parenleftbig\nθ;¯D/parenrightbig\n=−1\nnn/summationdisplay\ni=1logf(xi,yi;θ),\nwheref(x,y;θ)is a density function over the support of XandY, which\ntakesparameter θ. Theminimizationoftheriskinthiscaseyieldsthemaximum\nlikelihood (ML) estimate for the data ¯D, ",
    "title": "Practical Attacks on Machine Learning: A Case Study on Adversarial\n  Windows Malware",
    "abstract": "While machine learning is vulnerable to adversarial examples, it still lacks\nsystematic procedures and tools for evaluating its security in different\napplication contexts. In this article, we discuss how to develop automated and\nscalable security evaluations of machine learning using practical attacks,\nreporting a use case on Windows malware detection.",
    "link": "http://arxiv.org/abs/2207.05548v1",
    "published": "2022-07-12T14:17:58Z"
  },
  "180": {
    "pdf_path": "data/pdfs\\machine learning_paper_75.pdf",
    "text_excerpt": "arXiv:1810.11383v2  [cs.LG]  8 Nov 2018Some Requests for Machine Learning Research from\nthe East African Tech Scene\nMilan Cvitkovic\nDepartment of Computing and Mathematical Sciences\nCalifornia Institute of Technology\nPasadena, CA 91106\nmcvitkov@caltech.edu\nBased on 46 in–depth interviews with scientists, engineers , and CEOs, this document presents a list\nof concrete machine research problems, progress on which wo uld directly beneﬁt tech ventures in\nEast Africa.1\nThe goal of this work is to give machine learning researchers a fuller picture of where and how their\nefforts as scientists can be useful. The goal is thus notto highlight research problems that are unique\nto East Africa — indeed many of the problems listed below are o f general interest in machine\nlearning. The problems on the list are united solely by the fa ct that technology practitioners and\norganizations in East Africa reported a pressing need for th eir solution.\nThe author is aware that listing machine learning problems w ithout also providing data for them is\nnot a recipe for getting those problems solved. If the reader is interested in any of the problems\nbelow, please get in touch. I will gladly introduce them to th e organizations or people with access\nto data for those problems. But to protect privacy and intell ectual property, I have not attributed\nproblems to speciﬁc organizations or people in this documen t.\nResearch Problems\nNatural Language Processing\nMobile phone ownership and use, particularly of feature pho nes, is widespread in East Africa. SMS\nand voice interactions are one of the few big data sources in t he region. Moreover, since literacy\n(technological and otherwise) remains low, natural langua ge interfaces and conversational agents\nhave huge potential for impact.\nA few organizations in East Africa are trying to leverage NLP methods, but they face many chal-\nlenges, due in part to the following.\nHandling Rapid Code–Switching with Models trained on Singl e Language Corpora - In ",
    "title": "Fairness and Randomness in Machine Learning: Statistical Independence\n  and Relativization",
    "abstract": "Fair Machine Learning endeavors to prevent unfairness arising in the context\nof machine learning applications embedded in society. Despite the variety of\ndefinitions of fairness and proposed \"fair algorithms\", there remain unresolved\nconceptual problems regarding fairness. In this paper, we dissect the role of\nstatistical independence in fairness and randomness notions regularly used in\nmachine learning. Thereby, we are led to a suprising hypothesis: randomness and\nfairness can be considered equivalent concepts in machine learning.\n  In particular, we obtain a relativized notion of randomness expressed as\nstatistical independence by appealing to Von Mises' century-old foundations for\nprobability. This notion turns out to be \"orthogonal\" in an abstract sense to\nthe commonly used i.i.d.-randomness. Using standard fairness notions in machine\nlearning, which are defined via statistical independence, we then link the ex\nante randomness assumptions about the data to the ex post requirements for fair\npredictions. This connection proves fruitful: we use it to argue that\nrandomness and fairness are essentially relative and that both concepts should\nreflect their nature as modeling assumptions in machine learning.",
    "link": "http://arxiv.org/abs/2207.13596v2",
    "published": "2022-07-27T15:55:05Z"
  },
  "181": {
    "pdf_path": "data/pdfs\\machine learning_paper_76.pdf",
    "text_excerpt": "FUNDAMENTALS\nMachine learning and deep learning\nChristian Janiesch1&Patrick Zschech2&Kai Heinrich3\nReceived: 7 October 2020 / Accepted: 19 March 2021\n#The Author(s) 2021\nAbstract\nToday, intelligent systems that offer artificial intelligence capabilities often rely on machine learning. Machine learning describes\nthe capacity of systems to learn from problem-specific training data to automate the process of analytical model building andsolve associated tasks. Deep learning is a machine learning concept based on artificial neural networks. For many applications,\ndeep learning models outperform shallow machine learning models and traditional data analysis approaches. In this article, we\nsummarize the fundamentals of machine learning and deep learning to generate a broader understanding of the methodicalunderpinning of current intelligent systems. In particular, we provide a conceptual distinction between relevant terms and\nconcepts, explain the process of automated analytical model building through machine learning and deep learning, and discuss\nthe challenges that arise when implementing such intelligent systems in the field of electronic markets and networked business.These naturally go beyond technological aspects and highlight issues in human-machine interaction and artificial intelligence\nservitization.\nKeywords Machine learning .Deep learning .Artificial intelligence .Artificial neural networks .Analytical model building\nJEL classification C6.C8.M15 .O3\nIntroduction\nIt is considered easier to explain to a child the nature of what\nconstitutes a sports car as opposed to a normal car by showing\nhim or her examples, rather than trying to formulate explicitrules that define a sports car.\nSimilarly, instead of codifying knowledge into computers,\nmachine learning (ML) seeks to automatically learnmeaningful relationships and patterns from examples and ob-\nservations (Bishop 2006 ). Advances in ML have enabled the\nrecent rise of intelligent systems with human-like cognitiv",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "182": {
    "pdf_path": "data/pdfs\\machine learning_paper_77.pdf",
    "text_excerpt": "International Journal of Computer Trends and Technology (IJCTT) – volume 10 number  4 – Apr  2014  \n        ISSN: 2231 -2803                       http://www.ijcttjournal.org                Page 214 \n Application of Machine Learning Techniques \nin Aquaculture  \n \nAkhlaqur Rahman1 and Sumaira Tasnim2 \n1Department of Electrical and Electronic Engineering U ttara University, Bangladesh  \n 2School of Engineering , Deakin University, Australia  \n \nABSTRACT:   \nIn this paper we present applications of different machine \nlearning algorithms in aquaculture . Machine learning \nalgorithms learn models from historical data. In \naquaculture historical data are obtained from farm \npractices, yields, and environmental data sources. \nAssociations between these different variables can be \nobtained by applying machine learning algorithms to \nhistorical data. In this paper we present applic ations of \ndifferent machine learning algorithms  in aquaculture \napplications.    \n \nKeywords:  aquaculture, machine learning, decision \nsupport system  \n \n1. INTRODUCTION  \nAquaculture refers to the farming of aquatic \norganisms such as fish and aquatic plants. It \ninvolves cultivating freshwater and saltwater \npopulations under controlled conditions. Use of \nsensor technologies to monitor the environment \nwhere aquaculture operat ions take place is a recent \ntrend. The sensors collect data about the \naquaculture environment that are using by farm \nmanagers for decision making purposes.  \nThe literature states a number of activities \nrelated to decision support systems in Aquaculture \nfarm operations. A number of d ecision support \nsystems have been developed for.  Some of them \nuse machine learning and methods and other do \nnot. For the sake of completeness we briefly \ndiscuss these other methods first and detail the \nmachine learning based metho ds in the following \nsection.  \nBourke et al. [1] developed a framework where \nreal-time water quality indicators, as well as \noperational information",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "183": {
    "pdf_path": "data/pdfs\\machine learning_paper_78.pdf",
    "text_excerpt": "arXiv:1612.04251v1  [cs.DC]  13 Dec 2016TF.Learn: TensorFlow’s High-level Module for Distributed\nMachine Learning\nYuan Tang terry.tang@uptake.com\nUptake Technologies, Inc., 600 W. Chicago Ave, Suite 620, Ch icago, IL 60654\nEditor: Blank\nAbstract\nTF.Learn is a high-level Python module for distributed machine learnin g inside Tensor-\nFlow (Abadi et al., 2015). It provides an easy-to-use Scikit-learn ( Pedregosa et al., 2011)\nstyle interface to simplify the process of creating, conﬁguring, tr aining, evaluating, and\nexperimenting a machine learning model. TF.Learn integrates a wide ra nge of state-of-\nart machine learning algorithms built on top of TensorFlow’s low level AP Is for small to\nlarge-scale supervised and unsupervised problems. This module foc uses on bringing ma-\nchine learning to non-specialists using a general-purpose high-level language as well as\nresearchers who want to implement, benchmark, and compare the ir new methods in a\nstructured environment. Emphasis is put on ease of use, perform ance, documentation, and\nAPI consistency.\nKeywords: machine learning, deep learning, distributed system, open-source , Python\n1. Introduction\nTensorFlow, a general-purpose numerical computation libr ary open-sourced by Google in\nNovember 2015, has ﬂexible implementation and architectur e enables users to focus on\nbuilding the computation graph and deploy the model with lit tle eﬀorts on heterogeous\nplatforms such as mobile devices, hundreds of machines, or t housands of computational\ndevices.\nTensorFlow is generally very straightforward to use in a sen se that most of the re-\nsearchers in the research area without experience of using t his library could understand\nwhat’s happening behind the code blocks. TensorFlow provid es a good backbone for build-\ning diﬀerent shapes of machine learning applications.\nHowever, there’s a large number of potential users, includi ng some researchers, data\nscientists, andstudentswhomay befamiliar withmanymachi nelearningalgorithmsalrea",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "184": {
    "pdf_path": "data/pdfs\\machine learning_paper_79.pdf",
    "text_excerpt": "Machine Learning as Ecology\nOwen Howell,1,\u0003Cui Wenping,1, 2Robert Marsland III,1and Pankaj Mehta1,y\n1Department of Physics, Boston University, 590 Commonwealth Ave., Boston, MA 02215, USA\n2Department of Physics, Boston College, 140 Commonwealth Avenue, Chestnut Hill, MA 02467\n(Dated: August 26, 2019)\nMachine learning methods have had spectacular success on numerous problems. Here we show\nthat a prominent class of learning algorithms - including Support Vector Machines (SVMs) { have\na natural interpretation in terms of ecological dynamics. We use these ideas to design new online\nSVM algorithms that exploit ecological invasions, and benchmark performance using the MNIST\ndataset. Our work provides a new ecological lens through which we can view statistical learning\nand opens the possibility of designing ecosystems for machine learning.\nINTRODUCTION\nMachine learning (ML) is one of the most exciting and\nuseful areas of modern computer science [1, 2]. One com-\nmon machine learning task is classi\fcation: given labeled\ndata from one or more categories, predict the category\nof a new, unlabeled data point. Another common task\nis to perform outlier detection (i.e. \fnd data points that\nappear to be irregular). Both of these di\u000ecult problems\ncan be solved e\u000eciently using kernel-based methods such\nas Support Vector Machines (SVMs) [1, 3, 4].\nThe basic idea behind SVMs is to use a non-linear\nmap to embed the input data in a high-dimensional fea-\nture space where it can be classi\fed using a simple linear\nclassi\fer (see Figure 1). To ensure good generalization\nand avoid over\ftting, SVMs focus on the \\hardest to\nclassify\" points that lie closest to the linear decision sur-\nface in the high-dimensional feature space. These points\nare called \\support vectors\" and play a prominent role\nin SVM algorithms.\nThe real power and utility of SVMs comes from the\nfact that these ideas can be implemented quickly and\ne\u000eciently using kernel methods and quadratic optimiza-\ntion [1, 4]. The idea of a ",
    "title": "A Survey on Bias and Fairness in Machine Learning",
    "abstract": "With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",
    "link": "https://www.semanticscholar.org/paper/0090023afc66cd2741568599057f4e82b566137c",
    "published": "2019-08-23"
  },
  "185": {
    "pdf_path": "data/pdfs\\machine learning_paper_8.pdf",
    "text_excerpt": "Generated using the oﬃcial AMS L ATEX template v6.1 two-column layout. This work has been submitted for\npublication. Copyright in this work may be transferred without further notice, and this version may no longer be\naccessible.\nA Machine Learning Tutorial for Operational Meteorology, Part I: Traditional Machine\nLearning\nR/a.pc/n.pc/d.pc/y.pc J. C/h.pc/a.pc/s.pc/e.pca,b,c, D/a.pc/v.pc/i.pc/d.pc R. H/a.pc/r.pc/r.pc/i.pc/s.pc/o.pc/n.pcb,d,e, A/m.pc/a.pc/n.pc/d.pc/a.pc B/u.pc/r.pc/k.pc/e.pcb,c, G/a.pc/r.pc/y.pc M. L/a.pc/c.pc/k.pc/m.pc/a.pc/n.pc/n.pcf/a.pc/n.pc/d.pc A/m.pc/y.pc M/c.pcG/o.pc/v.pc/e.pc/r.pc/n.pca,b,c\naSchool of Computer Science, University of Oklahoma, Norman OK USA\nbSchool of Meteorology, University of Oklahoma, Norman OK USA\ncNSF AI Institute for Research on Trustworthy AI in Weather, Climate, and Coastal Oceanography, University of Oklahoma, Norman OK\nUSA\ndCooperative Institute for Severe and High-Impact Weather Research and Operations, University of Oklahoma, Norman OK USA\neNOAA/NWS/Storm Prediction Center, Norman, Oklahoma\nfDepartment of Marine, Earth, and Atmospheric Sciences, North Carolina State University, Raleigh, North Carolina\nABSTRACT: Recently, theuseofmachinelearninginmeteorologyhas increasedgreatly. Whilemany machinelearningmethodsarenot\nnew,universityclassesonmachinelearningarelargelyunavailabletometeorologystudentsandarenotrequiredtobecomeameteorologist.\nThelackofformalinstructionhascontributedtoperceptionthatmachinelearningmethodsare’blackboxes’andthusend-usersarehesitant\nto apply the machine learning methods in their every day workﬂow. To reduce the opaqueness of machine learning methods and lower\nhesitancytowardsmachinelearninginmeteorology,thispaperprovidesasurveyofsomeofthemostcommonmachinelearningmethods.\nA familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics\nusing plain language. The following machine learning methods are demonstrated: linear regressio",
    "title": "A Machine Learning Tutorial for Operational Meteorology, Part I:\n  Traditional Machine Learning",
    "abstract": "Recently, the use of machine learning in meteorology has increased greatly.\nWhile many machine learning methods are not new, university classes on machine\nlearning are largely unavailable to meteorology students and are not required\nto become a meteorologist. The lack of formal instruction has contributed to\nperception that machine learning methods are 'black boxes' and thus end-users\nare hesitant to apply the machine learning methods in their every day workflow.\nTo reduce the opaqueness of machine learning methods and lower hesitancy\ntowards machine learning in meteorology, this paper provides a survey of some\nof the most common machine learning methods. A familiar meteorological example\nis used to contextualize the machine learning methods while also discussing\nmachine learning topics using plain language. The following machine learning\nmethods are demonstrated: linear regression; logistic regression; decision\ntrees; random forest; gradient boosted decision trees; naive Bayes; and support\nvector machines. Beyond discussing the different methods, the paper also\ncontains discussions on the general machine learning process as well as best\npractices to enable readers to apply machine learning to their own datasets.\nFurthermore, all code (in the form of Jupyter notebooks and Google Colaboratory\nnotebooks) used to make the examples in the paper is provided in an effort to\ncatalyse the use of machine learning in meteorology.",
    "link": "http://arxiv.org/abs/2204.07492v2",
    "published": "2022-04-15T14:48:04Z"
  },
  "186": {
    "pdf_path": "data/pdfs\\machine learning_paper_80.pdf",
    "text_excerpt": "1 \n Using Deep Learning and Machine Learning to Detect Epileptic \nSeizure with Electroencephalography (EEG) Data  \n \n \nHaotian Liu  \nNortheast Yucai Foreign Language School  \nShenyang, Liaoning, China  \nEmail: 2312938389@qq.com  Lin Xi  \nNortheast Yucai Foreign Language School  \nShenyang, Liaoning, China  \nEmail: 939791667@qq.com  \n \nYing Zhao * \nDepartment of Engineering Science and \nApplied Math  \nNorthwestern University  \nEvanston , IL, U.S.A  \nEric940825@gmail.com  \n  \nZhixiang Li* \nDepartment of Biomedical Engineering  \nShenyang Pharmaceutical University  \nLiaoning, China  \n106040205@syphu.edu.cn  \n \n \n \nAbstract:  \n  \nThe prediction of epileptic seizure has always been extremely challenging in medical \ndomain. However, as the development of computer technology, the application of machine \nlearning introduced new ideas for seizure forecasting. Applying machine learning model onto \nthe predication of epileptic seizure could help us obtain a better result and there have been \nplenty of scientists who have been doing such works so that there are sufficient medical data \nprovided for researchers to do training of machine learning models.  \n \n \nKeywords:  \n \nEpileptic Seizure Detection, Machine Learning, Deep Learnin g, \nElectroencephalography , Convolutional Neural Network, Recurrent Neural Network  \n  2 \n Contents  \n \nAbstract             1 \n1. Introduction           3 \n2. Related Works          3 \n3. Methods           4 \n3.1 Dataset                      4 \n3.2 Pre-processing          4 \n3.3 Machine Learning Classifiers        5 \n3.4 Deep Learning Neural Networks        6 \n3.5 Evaluation           7 \n3.6 Tools            8 \n4. Results           9 \n4.1 Results and Analysis         9 \n4.2 Limitations and Future Work        10 \n5. Conclusion           10 \nAcknowledgement            11 \nReference s            12 \n  3 \n 1. Introduction:  \n \nThe prediction of epileptic seizure has always been extremely challenging in medical \ndomain. However, as the development of compu",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "187": {
    "pdf_path": "data/pdfs\\machine learning_paper_81.pdf",
    "text_excerpt": "International Journal of Network Security & Its Applications (IJNSA) Vol. 11, No.5, September 2019  \nDOI: 10.5121/ijnsa.2019.11501                                                                                                                         1                                                                                                                       \nMACHINE LEARNING IN NETWORK SECURITY  \nUSING KNIME  ANALYTICS  \n \nMuntherAbualkibash  \n \nSchool of Information Security and Applied Computing , College of Technology, Eastern \nMichigan University , Ypsilanti, MI, USA  \n \nABSTRACT  \n \nMachine learning has more and more effect on our every day’s life. This field keeps growing and \nexpanding into new areas. Machine learning is based on the implementation of artificial intelligence that \ngives systems the capability to automatically learn and enhance from experiments without being explicitly \nprogrammed. Machine Learning algorithms apply mathematical equations to analyze datasets and predict \nvalues based on the dataset. In the field of cybersecurity, machine learning algorithms can be utilized to \ntrain and analyze the Intrusion  Detection Systems (IDSs) on security -related datasets. In this paper, we \ntested different machine learning algorithms to analyze NSL -KDD dataset using KNIME analytics.  \n \nKEYWORDS  \n \nNetwork S ecurity, KNIME, NSL -KDD, and  Machine Learning  \n     \n1. INTRODUCTION  \n \nIn today’s connected world , where billions of people access  the internet, anything that depends on \nthe internet for communication, or is connected to a computer or any type of smart device, can be \naffected by several kind s of cyber  attacks. As a result, m any organizations , either public or \nprivate , have to deal with continuous and complicated different types of cyber  attacks and cyber  \nthreats.  The fact that cyber  threats  and cyber  attacks now permeate every facet of society shows \nwhy cybersecurity is cruciall y important.  \n \nCybersecurity is",
    "title": "Membership Inference Attacks Against Machine Learning Models",
    "abstract": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
    "link": "https://www.semanticscholar.org/paper/f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d",
    "published": "2016-10-18"
  },
  "188": {
    "pdf_path": "data/pdfs\\machine learning_paper_82.pdf",
    "text_excerpt": " \nSELM: Software Engineering of Machine Learning Models  \nNafiseh Jafar i, ICT Department , Malek Ashtar University of Technology , Tehran, Iran  \nMohammad Reza Besharati1, Department of Computer Engineering , Sharif University  of Technology , Tehran, Iran  \nMohammad Izadi , Department o f Computer Engineering , Sharif University  of Technology , Tehran, Iran  \nMaryam Hourali , ICT Department , Malek Ashtar University of Technology , Tehran, Iran  \n \nAbstract  \nOne of the pillars of any machine learning model is its concepts. Using software \nengineering, we can engineer these concepts and then develop and expand them. In this \narticle, we present a SELM framework for Software Engineering of machine Learning \nModels.  We then evaluate this framework through a case study. Using the SELM \nframework, we can improve a machine learning process efficiency and provide more \naccuracy in learning with less processing hardware resources and a smaller training dataset. \nThis issue h ighlights the importance of an interdisciplinary approach to machine learning. \nTherefore, in this article, we have provided interdisciplinary teams' proposals for machine \nlearning.  \n \nKeywords: Software Engineering, Machine Learning Models, Engineering Meth odology, \nLearning Efficiency  \n \n1. Introduction  \nMachine learning usually aims to find and develop a computational model for an intelligent \ntask on a practical problem. Development based on calculation can be called engineering \n(1). In software engineering, s oftware systems are calculated and engineered by models. In \nfact, these software models are platforms for analysis, design, development, and system \nengineering.  \n                   \n1  besharati@ce.sharif.edu  \n For modeling in software engineering, we need several elements: 1. The modeling \nperspective, 2.  The system under modeling, and 3. Modeling language and tools (5). So we \nlook at a system under modeling from one or several perspectives, and we discover a set \nof meanings",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "189": {
    "pdf_path": "data/pdfs\\machine learning_paper_83.pdf",
    "text_excerpt": "Challenges and Opportunities in Quantum Machine Learning\nM. Cerezo,1, 2, 3Guillaume Verdon,4, 5, 6Hsin-Yuan Huang,7, 8Lukasz Cincio,9, 3and Patrick J. Coles10, 9, 3\n1Information Sciences, Los Alamos National Laboratory, Los Alamos, NM 87545, USA\n2Center for Nonlinear Studies, Los Alamos National Laboratory, Los Alamos, New Mexico 87545, USA\n3Quantum Science Center, Oak Ridge, TN 37931, USA\n4X, Mountain View, CA, USA\n5Institute for Quantum Computing, University of Waterloo, ON, Canada\n6Department of Applied Mathematics, University of Waterloo, ON, Canada\n7Institute for Quantum Information and Matter, California Institute of Technology, USA\n8Department of Computing and Mathematical Sciences, California Institute of Technology, USA\n9Theoretical Division, Los Alamos National Laboratory, Los Alamos, New Mexico 87545, USA\n10Normal Computing Corporation, New York, New York, USA\nAt the intersection of machine learning and quantum computing, Quantum Machine Learning\n(QML)hasthepotentialofacceleratingdataanalysis, especiallyforquantumdata, withapplications\nfor quantum materials, biochemistry, and high-energy physics. Nevertheless, challenges remain\nregarding the trainability of QML models. Here we review current methods and applications for\nQML. We highlight diﬀerences between quantum and classical machine learning, with a focus on\nquantumneuralnetworksandquantumdeeplearning. Finally, wediscussopportunitiesforquantum\nadvantage with QML.\nI. INTRODUCTION\nThe recognition that the world is quantum mechanical\nhas allowed researchers to embed well-established, but\nclassical, theories into the framework of quantum Hilbert\nspaces. Shannon’s information theory, which is the ba-\nsis of communication technology, has been generalized\nto quantum Shannon theory (or quantum information\ntheory), opening up the possibility that quantum eﬀects\ncould make information transmission more eﬃcient [1].\nThe ﬁeld of biology has been extended to quantum bi-\nology to allow for a deeper understanding of ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "190": {
    "pdf_path": "data/pdfs\\machine learning_paper_84.pdf",
    "text_excerpt": "arXiv:2407.05520v1  [cs.LG]  7 Jul 2024A Theory of Machine Learning\nJinsook Kim∗\nUnderwood International College\nYonsei University\nSeoul, Korea 03722\njki76364@gmail.comJinho Kang\nEmeritus, Seoul National University\nSeoul, Korea 08826\njhkang@snu.ac.kr\nAbstract\nWe critically review three major theories of machine learni ng and provide a new\ntheory according to which machines learn a function when the machines success-\nfully compute it. We show that this theory challenges common assumptions in\nthe statistical and the computational learning theories, f or it implies that learning\ntrue probabilities is equivalent neither to obtaining a cor rect calculation of the true\nprobabilities nor to obtaining an almost-sure convergence to them. We also brieﬂy\ndiscuss some case studies from natural language processing and macroeconomics\nfrom the perspective of the new theory.\n1 Introduction\nIn this paper, we examine three major theories of machine lea rning. We will call them the possible\nworlds theory ,the recognition theory , and the operation theory . Both the possible worlds theory and\nthe recognition theory are based on what we will call the epistemic approach to machine learning,\nwhereas the operation theory is based on what we will call the behavioral approach. We will prove\nthat all three theories have important problems. We will the n provide a new theory of machine\nlearning according to which machines learn a function when m achines successfully compute it. We\nwill show that this theory challenges common assumptions in the statistical and the computational\nlearning theories, for it implies that learning true probab ilities is equivalent neither to obtaining a\ncorrect calculation of true probabilities nor to obtaining an almost-sure convergence to them. Lastly,\nwe will discuss when machines can or cannot learn a probabili ty function in the perspective of our\nnew theory by considering two case studies, the ﬁrst one from natural language processing and the\nsecond one from ma",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "191": {
    "pdf_path": "data/pdfs\\machine learning_paper_85.pdf",
    "text_excerpt": "Learning Moore Machines from Input-Output Traces?\nGeorgios Giantamidis1and Stavros Tripakis1;2\n1Aalto University, Finland\n2University of California, Berkeley, USA\nAbstract. The problem of learning automata from example traces (but no equivalence or membership queries) is\nfundamental in automata learning theory and practice. In this paper we study this problem for ﬁnite state machines\nwith inputs and outputs, and in particular for Moore machines. We develop three algorithms for solving this prob-\nlem: (1) the PTAP algorithm, which transforms a set of input-output traces into an incomplete Moore machine and\nthen completes the machine with self-loops; (2) the PRPNI algorithm, which uses the well-known RPNI algorithm\nfor automata learning to learn a product of automata encoding a Moore machine; and (3) the MooreMI algorithm,\nwhich directly learns a Moore machine using PTAP extended with state merging. We prove that MooreMI has the\nfundamental identiﬁcation in the limit property. We also compare the algorithms experimentally in terms of the size\nof the learned machine and several notions of accuracy, introduced in this paper. Finally, we compare with OSTIA,\nan algorithm that learns a more general class of transducers, and ﬁnd that OSTIA generally does not learn a Moore\nmachine, even when fed with a characteristic sample .\n1 Introduction\nAn abundance of data from the internet and from other sources (e.g., sensors) is revolutionizing many sectors of\nscience, technology, and ultimately our society. At the heart of this revolution lies machine learning , a broad spectrum\nof techniques to derive information from data. Traditionally, objects studied by machine learning include classiﬁers,\ndecision trees, and neural networks, with applications to ﬁelds as diverse as artiﬁcial intelligence, marketing, ﬁnance,\nor medicine [38].\nIn the context of system design, an important problem, with numerous applications, is automatically generating\nmodels from data. There are many variants o",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "192": {
    "pdf_path": "data/pdfs\\machine learning_paper_86.pdf",
    "text_excerpt": "How Developers Iterate on Machine Learning Workflows\nA Survey of the Applied Machine Learning Literature\nDoris Xin, Litian Ma, Shuchen Song, Aditya Parameswaran\nUniversity of Illinois, Urbana-Champaign (UIUC)\n{dorx0,litianm2,ssong18,adityagp}@illinois.edu\nABSTRACT\nMachine learning workflow development is anecdotally regarded to\nbe an iterative process of trial-and-error with humans-in-the-loop.\nHowever, we are not aware of statistics-based evidence corroborat-\ning this popular belief. A statistical characterization of iteration can\nserve as a benchmark for machine learning workflow development\nin practice, and can aid the development of human-in-the-loop\nmachine learning systems. To this end, we conduct a small-scale\nsurvey of the applied machine learning literature from five distinct\napplication domains. We use statistics collected from the papers to\nestimate the role of iteration within machine learning workflow\ndevelopment, and report preliminary trends and insights from our\ninvestigation, as a starting point towards this benchmark. Based\non our findings, we finally describe desiderata for effective and\nversatile human-in-the-loop machine learning systems that can\ncater to users in diverse domains.\n1 INTRODUCTION\nDevelopment of machine learning (ML) applications is governed\nby an iterative process: starting with an initial workflow, develop-\ners iteratively modify their workflow, based on previous results,\nto improve performance. They may add or modify data sources,\nfeatures, hyperparameters, and training algorithms, among others.\nThese iterations of trial-and-error are necessary due to data vari-\nability, algorithmic complexity, and overall unpredictability of ML.\nAdetailed, statistical characterization of how developers iteratively\nmodify ML workflows can serve as a benchmark for human-in-the-\nloop ML systems . At present, due to the lack of such studies, we are\nforced to resort to anecdotal evidence to identify usage patterns\nand motivate design decisions.\nTo ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "193": {
    "pdf_path": "data/pdfs\\machine learning_paper_87.pdf",
    "text_excerpt": "Practical Solutions for Machine Learning Safety in Autonomous Vehicles\nSina Mohseni,1,2Mandar Pitale,2Vasu Singh,3Zhangyang Wang1\n1Texas A&M University, College Station, TX\n2NVIDIA, Santa Clara, CA\n3NVIDIA, Munich, Germany\nfsina.mohseni,atlaswang g@tamu.edu,fmpitale,vasusg@nvidia.com\nAbstract\nAutonomous vehicles rely on machine learning to solve chal-\nlenging tasks in perception and motion planning. However,\nautomotive software safety standards have not fully evolved\nto address the challenges of machine learning safety such as\ninterpretability, veriﬁcation, and performance limitations. In\nthis paper, we review and organize practical machine learn-\ning safety techniques that can complement engineering safety\nfor machine learning based software in autonomous vehi-\ncles. Our organization maps safety strategies to state-of-the-\nart machine learning techniques in order to enhance depend-\nability and safety of machine learning algorithms. We also\ndiscuss security limitations and user experience aspects of\nmachine learning components in autonomous vehicles.\n1 Introduction\nAdvances in machine learning (ML) have been one of the\nbiggest innovations of the last decade. Nowadays, ML mod-\nels are used extensively in different industrial ﬁelds like au-\ntonomous vehicles, medical diagnosis, and robotics to per-\nform various tasks such as speech recognition, object de-\ntection, and motion planning. Among different ML models,\nDeep Neural Networks (DNNs) (LeCun, Bengio, and Hin-\nton 2015) are well-known and widely used for their pow-\nerful representation learning in high-dimensional data. For\ninstance, in the ﬁeld of autonomous driving, various DNN\nobject detection and image segmentation algorithms have\nbeen used as perception units to process camera (e.g., Pilot-\nNet (Bojarski et al. 2016), Fast RCNN (Wang, Shrivastava,\nand Gupta 2017)) and Lidar (e.g., V oxelNet (Zhou and Tuzel\n2018)) data.\nThe development of safety critical systems relies on strin-\ngent safety methodologies, desig",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "194": {
    "pdf_path": "data/pdfs\\machine learning_paper_88.pdf",
    "text_excerpt": "Julia Language in Machine Learning: Algorithms, Applications, and Open Issues\nKaifeng Gaoa, Gang Meia,\u0003, Francesco Picciallib,\u0003, Salvatore Cuomob,\u0003, Jingzhi Tua, Zenan Huoa\naSchool of Engineering and Technology, China University of Geosciences (Beijing), 100083, Beijing, China\nbDepartment of Mathematics and Applications R. Caccioppoli, University of Naples Federico II, Naples, Italy\nAbstract\nMachine learning is driving development across many ﬁelds in science and engineering. A simple and efﬁ-\ncient programming language could accelerate applications of machine learning in various ﬁelds. Currently,\nthe programming languages most commonly used to develop machine learning algorithms include Python,\nMATLAB, and C/C ++. However, none of these languages well balance both efﬁciency and simplicity. The\nJulia language is a fast, easy-to-use, and open-source programming language that was originally designed\nfor high-performance computing, which can well balance the efﬁciency and simplicity. This paper sum-\nmarizes the related research work and developments in the applications of the Julia language in machine\nlearning. It ﬁrst surveys the popular machine learning algorithms that are developed in the Julia language.\nThen, it investigates applications of the machine learning algorithms implemented with the Julia language.\nFinally, it discusses the open issues and the potential future directions that arise in the use of the Julia\nlanguage in machine learning.\nKeywords: Julia language, Machine learning, Supervised learning, Unsupervised learning, Deep learning,\nArtiﬁcial neural networks\nContents\n1 Introduction 4\n2 A Brief Introduction to the Julia Language 6\n3 Julia in Machine Learning: Algorithms 7\n3.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n\u0003Corresponding author\nEmail addresses: gang.mei@cugb.edu.cn (Gang Mei), francesco.piccialli@unina.it (Francesco Piccialli),\nsalvatore.cuomo@unina.it (Salvatore Cuomo)\nPreprint submitted",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "195": {
    "pdf_path": "data/pdfs\\machine learning_paper_89.pdf",
    "text_excerpt": "MODELING GENERALIZATION IN MACHINE LEARNING :\nA M ETHODOLOGICAL AND COMPUTATIONAL STUDY\nPietro Barbiero\nCambridge University\nUnited Kingdom\nbarbiero@tutanota.comGiovanni Squillero\nPolitecnico di Torino\nItaly\nsquillero@polito.itAlberto Tonda\nUniversité Paris-Saclay, INRAE\nFrance\nalberto.tonda@inrae.fr\nJune 30, 2020\nABSTRACT\nAs machine learning becomes more and more available to the general public, theoretical questions are\nturning into pressing practical issues. Possibly, one of the most relevant concerns is the assessment of\nour conﬁdence in trusting machine learning predictions. In many real-world cases, it is of utmost\nimportance to estimate the capabilities of a machine learning algorithm to generalize, i.e., to provide\naccurate predictions on unseen data, depending on the characteristics of the target problem. In this\nwork we perform a meta-analysis of 109 publicly-available classiﬁcation data sets, modeling machine\nlearning generalization as a function of a variety of data set characteristics, ranging from number of\nsamples to intrinsic dimensionality, from class-wise feature skewness to F1evaluated on test samples\nfalling outside the convex hull of the training set. Experimental results demonstrate the relevance of\nusing the concept of the convex hull of the training data in assessing machine learning generalization,\nby emphasizing the difference between interpolated and extrapolated predictions. Besides several\npredictable correlations, we observe unexpectedly weak associations between the generalization\nability of machine learning models and all metrics related to dimensionality, thus challenging the\ncommon assumption that the curse of dimensionality might impair generalization in machine learning.\nKeywords Convex hull\u0001Curse of dimensionality \u0001Data set characteristics \u0001Extrapolation\u0001Generalization\u0001\nInterpolation\u0001Machine Learning \u0001Symbolic regression\n1 Introduction\nThe term machine learning (ML) traditionally includes algorithms that are able to improve their",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "196": {
    "pdf_path": "data/pdfs\\machine learning_paper_9.pdf",
    "text_excerpt": "Understanding Bias in Machine Learning\nJindong Gu1;2, Daniela Oelke2\n1The University of Munich\n2Siemens AG, Corporate Technology\nAbstract. Bias is known to be an impediment to fair decisions in many\ndomains such as human resources, the public sector, health care etc. Re-\ncently, hope has been expressed that the use of machine learning methods\nfor taking such decisions would diminish or even resolve the problem. At\nthe same time, machine learning experts warn that machine learning\nmodels can be biased as well.\nIn this article, our goal is to explain the issue of bias in machine learning\nfrom a technical perspective and to illustrate the impact that biased data\ncan have on a machine learning model. To reach such a goal, we develop\ninteractive plots to visualizing the bias learned from synthetic data. The\ninteractive plots are available1.\nKeywords: Bias in Machine Learning \u0001Visualization in Explainable AI.\n1 How does bias get into a machine learning model?\nTo be able to let the machine take a decision automatically, we have to teach it\nhow to do it right. One way to do so is to formulate explicitly as a rule when\nto take which decision. However, many situations are too complex for this. So\nwhat can we do? A central idea of machine learning is that the machine learns\nthe rules and patterns by itself from examples. The examples are decisions that\nhumans have taken in the past together with the information about the subject\n(the data) that they based their decision upon. We call this kind of data training\ndata, because the machine uses it to learn to take a decision as a human would\nhave done it.\nNow it should be clear, why a machine learning model can be biased as well:\nIf the data or the decisions taken on it are biased and the machine uses them as\nan example, then the machine is going to incorporate this bias into the model.\nIt learns the bias from the examples given to it.\nSo the \frst thing that you should keep in mind is this: Bias gets into the\nmodel through the dat",
    "title": "Understanding Bias in Machine Learning",
    "abstract": "Bias is known to be an impediment to fair decisions in many domains such as\nhuman resources, the public sector, health care etc. Recently, hope has been\nexpressed that the use of machine learning methods for taking such decisions\nwould diminish or even resolve the problem. At the same time, machine learning\nexperts warn that machine learning models can be biased as well. In this\narticle, our goal is to explain the issue of bias in machine learning from a\ntechnical perspective and to illustrate the impact that biased data can have on\na machine learning model. To reach such a goal, we develop interactive plots to\nvisualizing the bias learned from synthetic data.",
    "link": "http://arxiv.org/abs/1909.01866v1",
    "published": "2019-09-02T20:36:19Z"
  },
  "197": {
    "pdf_path": "data/pdfs\\machine learning_paper_90.pdf",
    "text_excerpt": "Industrial practitioners’ mental models of adversarial machine learning\nLukas Bieringer\u0003\nQuantPiKathrin Grosse\u0003\nUniversity of Cagliari\nMichael Backes\nCISPA Helmholtz Center\nfor Information SecurityBattista Biggio\nUniversity of Cagliari,\nPluribus OneKatharina Krombholz\nCISPA Helmholtz Center\nfor Information Security\nAbstract\nAlthough machine learning is widely used in practice, little\nis known about practitioners’ understanding of potential se-\ncurity challenges. In this work, we close this substantial gap\nand contribute a qualitative study focusing on developers’\nmental models of the machine learning pipeline and poten-\ntially vulnerable components. Similar studies have helped\nin other security ﬁelds to discover root causes or improve\nrisk communication. Our study reveals two facets of practi-\ntioners’ mental models of machine learning security. Firstly,\npractitioners often confuse machine learning security with\nthreats and defences that are not directly related to machine\nlearning. Secondly, in contrast to most academic research,\nour participants perceive security of machine learning as not\nsolely related to individual models, but rather in the context of\nentire workﬂows that consist of multiple components. Jointly\nwith our additional ﬁndings, these two facets provide a foun-\ndation to substantiate mental models for machine learning\nsecurity and have implications for the integration of adver-\nsarial machine learning into corporate workﬂows, decreasing\npractitioners’ reported uncertainty, and appropriate regulatory\nframeworks for machine learning security.\n1 Introduction\nAdversarial machine learning (AML) studies the reliability\nof learning based systems in the context of an adversary [6,\n12, 69]. For example, tampering with some features often\n*First two authors contributed equally.\nCopyright is held by the author/owner. Permission to make digital or hard\ncopies of all or part of this work for personal or classroom use is granted\nwithout fee.\nUSENIX Symposium on Us",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "198": {
    "pdf_path": "data/pdfs\\machine learning_paper_91.pdf",
    "text_excerpt": "APPLYING MACHINE LEARNING TO LIFEINSURANCE :SOME\nKNOWLEDGE SHARING TO MASTER IT\nPREPRINT\nAntoine Chancel\nKnowledge Team\nSCOR\nParis, France\nachancel@scor.comLaura Bradier\nKnowledge Team\nSCOR\nSingapore\nlbradier@scor.comAntoine Ly\nKnowledge Team\nSCOR\nParis, France\naly@scor.comRazvan Ionescu\nKnowledge Team\nSCOR\nParis, France\nrionescu@scor.com\nLaurène Martin\nParis, France\nlaurene.martin@ensae.frMarguerite Saucé\nParis, France\nmarguerite.sauce@gmail.com\nSeptember 29, 2022\nKeywords Survival Modelling \u0001Machine Learning \u0001Life Insurance\u0001Python\u0001Statistical Modeling\nGeneral introduction\n1 Motivation and scope of this paper\nMachine Learning permeates many industries, which brings new sources of beneﬁts for companies. However, within\nthe life insurance industry, Machine Learning is not widely used in practice as over the past years statistical models\nhave shown their efﬁciency for risk assessment. Insurers may thus face difﬁculties assessing the value of the artiﬁcial\nintelligence.\nFocusing on the modiﬁcation of the life insurance industry over time highlights the stake of using Machine Learning for\ninsurers and beneﬁts that it can bring by unleashing data value.\nThis paper reviews traditional actuarial methodologies for survival modeling and extends them with Machine Learning\ntechniques. It points out differences with regular machine learning models and emphasizes the importance of speciﬁc\nimplementations to face censored data with the Machine Learning models family. In complement to this article, a\nPython library has been developed. Different open-source Machine Learning algorithms have been adjusted to adapt\nthe speciﬁcities of life insurance data, namely censoring and truncation. Such models can be easily applied from this\nSCOR library to accurately model life insurance risks. This library is brieﬂy presented in section 5\n1.1 Why consider Machine Learning for risk modeling?\nThe life insurance industry became more complex as the modern life insurance industry now offers all kin",
    "title": "Practical Black-Box Attacks against Machine Learning",
    "abstract": "Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",
    "link": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024",
    "published": "2016-02-08"
  },
  "199": {
    "pdf_path": "data/pdfs\\machine learning_paper_92.pdf",
    "text_excerpt": "MACHINE LEARNING AND DOMAIN DECOMPOSITION\nMETHODS - A SURVEY\nAXEL KLAWONN∗†, MARTIN LANSER∗†,ANDJANINE WEBER∗†\nAbstract. Hybrid algorithms, which combine black-box machine learning methods with ex-\nperience from traditional numerical methods and domain expertise from diverse application areas,\nare progressively gaining importance in scientific machine learning and various industrial domains,\nespecially in computational science and engineering. In the present survey, several promising avenues\nof research will be examined which focus on the combination of machine learning (ML) and domain\ndecomposition methods (DDMs). The aim of this survey is to provide an overview of existing work\nwithin this field and to structure it into domain decomposition for machine learning and machine\nlearning-enhanced domain decomposition, including: domain decomposition for classical machine\nlearning, domain decomposition to accelerate the training of physics-aware neural networks, machine\nlearning to enhance the convergence properties or computational efficiency of DDMs, and machine\nlearning as a discretization method in a DDM for the solution of PDEs. In each of these fields, we\nsummarize existing work and key advances within a common framework and, finally, disuss ongoing\nchallenges and opportunities for future research.\nKey words. scientific machine learning, domain decomposition, survey, hybrid modelling,\nphysics-aware neural networks\nAMS subject classifications. 65F10, 65N22, 65N55, 68T05, 68T07\n1. Introduction. Domain decomposition methods (DDMs) are divide-and-con-\nquer strategies which decompose a given problem into a number of smaller subprob-\nlems. This strategy often leads to easier parallelizable algorithms where the subprob-\nlems can be solved on different processors (cpu or gpu). Sometimes, an additional\nproblem is needed for parallel scalability, the so-called global problem, or to gather\nand connect local information obtained from the solution of the local subproblems.\nDDMs",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "200": {
    "pdf_path": "data/pdfs\\machine learning_paper_93.pdf",
    "text_excerpt": "arXiv:2409.03632v1  [cs.LG]  5 Sep 2024Beyond ModelInterpretability: Socio-Structural Explanati ons in Machine\nLearning\nANDREWSMART, GoogleResearch, USA\nATOOSA KASIRZADEH, GoogleResearch, USA\nWhat is it to interpret the outputs of an opaque machine learning model? One approach is to develop interpretable machine learn-\ning techniques. These techniques aim to show how machine learning models f unction by providing either model-centric local or\nglobal explanations, which can be based on mechanistic interpretations (revealing the inner working mechanisms of models) or\nnon-mechanistic approximations (showing input feature-output data rel ationships). In this paper, we draw on social philosophy to\nargue that interpreting machine learning outputs in certain normatively- salient domains could require appealing to a third type of\nexplanationthat wecall“socio-structural” explanation.Therel evanceof thisexplanationtype is motivated bythefact thatmachine\nlearningmodelsarenotisolatedentitiesbutareembeddedwithinandsha pedbysocialstructures.Socio-structural explanationsaim\nto illustrate how social structures contribute to and partially ex plain the outputs of machine learning models. We demonstrate the\nimportance of socio-structural explanations byexamining a racially bia sed healthcareallocation algorithm.Our proposal highlights\nthe need for transparency beyond model interpretability: understanding the outputs of machine learning systems could require a\nbroaderanalysis that extendsbeyondtheunderstandingof themachine l earningmodel itself.\nACMReference Format:\nAndrew Smart and Atoosa Kasirzadeh. 2024. Beyond Model Interpret ability: Socio-Structural Explanations in Machine Learning. 1,\n1 (September2024), 12pages.https://doi.org/XXXXXXX.XXXXXXX\n1 INTRODUCTION\nInordertoformulatealearningtheoryofmachinelearning, itmaybenecessarytomovefromseeingan\ninert model as the machine learner to seeing the human develo per—along with, and not separate from,\nhis orher modeland surroundingsocialr",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "201": {
    "pdf_path": "data/pdfs\\machine learning_paper_94.pdf",
    "text_excerpt": "arXiv:2502.01708v1  [cs.LG]  3 Feb 2025Aspects of Artiﬁcial Intelligence: Transforming Machine L earning\nSystems Naturally\nXiuzhan Guo∗\nAbstract\nIn this paper, we study the machine learning elements which w e are interested in together as a\nmachine learning system, consisting of a collection of mach ine learning elements and a collection of\nrelations between the elements. The relations we concern ar e algebraic operations, binary relations,\nand binary relations with composition that can be reasoned c ategorically. A machine learning system\ntransformation between two systems is a map between the syst ems, which preserves the relations we\nconcern. The system transformations given by quotient or cl ustering, representable functor, and Yoneda\nembedding are highlighted and discussed by machine learnin g examples. An adjunction between machine\nlearning systems, a special machine learning system transf ormation loop, provides the optimal way of\nsolving problems. Machine learning system transformation s are linked and compared by their maps at\n2-cell, natural transformations. New insights and structu res can be obtained from universal properties\nand algebraic structures given by monads, which are generat ed from adjunctions.\nKeywords – Machine Learning, Machine Learning System, Machine Learni ng System Transformation, Binary Relation, Di-\nrected Graph, Category, Functor, Transformation, Quotien t, Adjunction, Monad, Descent, Yoneda Embedding\nLet’s begin with the following sentences:\n•Observed bird nests in trees enduring heavy winds during my morning walking;\n•My cats dash straight to me at the moment they see me taking their lic kable treats from the box,\nwithout worrying about the optimal path;\n•We steer our cars instinctively, without calculating or measuring the exact degrees to turn the steering\nwheel;\n•Some people enjoy solving their problems by deﬁning objective funct ions, constraints, and searching\nfor the optimal solutions;\n•···.\nPeople might have diﬀerent feeling",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "202": {
    "pdf_path": "data/pdfs\\machine learning_paper_95.pdf",
    "text_excerpt": "Storey V.C., Parsons, J. Castellanos A., Tremblay M., Lukyanenko R., Maass, W., Castillo, A. ( 2025). Conceptual \nModeling for Machine Learning: Using Domain Knowledge to Improve Performance and Process Transparency. \nData & Knowledge Engineering , pp 1-41. \n \n \nDomain Knowledge in Artificial Intelligence:  \nUsing Conceptual Modeling to Increase Machine \nLearning Accuracy and Explainability  \n \n \nVeda C. Storey, Jeffrey Parsons, Arturo Castellanos Bueso, Monica Chiarini \nTremblay, Roman Lukyanenko, Alfred Castillo, Wolfgang Maass  \n \nAbstract  \nMachine learning enables the extract ion of  useful information from large, diverse datasets.  \nHowever, despite  many successful applications, machine learning continues to suffer from \nperformance and transparency issues. T hese challenges can  be partially attributed to the \nlimited use of domain knowledge by machine learning models. This research proposes \nusing the domain knowledge  represented in conceptual models  to improve the preparation \nof the data used  to train  machine learning models. We develop and demonstrate a method, \ncalled  the Conceptual Modeling for Machine Learning (CMML) , which is comprised of \nguidelines for data preparation in machine learning  and based on conceptual modeling \nconstructs and principles. To  assess the impact  of CMML on machine learning outcomes , \nwe first applied it to two real -world problems  to evaluate its impact on model performance. \nWe then  solicited an assessment by  data scientists on the applicability of the method . The se \nresults demonstrate the value of CMML for improving machine learning outcomes.  \nKeywords:  artificial i ntelligence, machine learning, C onceptual M odeling for Machine \nLearning (CMML) method, machine l earning model p erformance, t ransparency, data \npreparation, domain knowledge   \n \n1 Introduction \nMachine learning (ML) has been widely adopted to support a range of  business functions, industries , \nand daily tasks , although  challenges in ",
    "title": "Interpretable Machine Learning",
    "abstract": "Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.",
    "link": "https://www.semanticscholar.org/paper/b0c34618ffd1154f35863e2ce7250ac6b6f2c424",
    "published": "2019-11-07"
  },
  "203": {
    "pdf_path": "data/pdfs\\machine learning_paper_96.pdf",
    "text_excerpt": "LEARNING THEORY AND SUPPORT VECTOR MACHINES\nA PRIMER\nMichael Banf\nEducatedGuess.ai\nSiegen, Germany\nmichael@educatedguess.ai\nABSTRACT\nThe main goal of statistical learning theory is to provide a fundamental framework for the problem of\ndecision making and model construction based on sets of data. Here, we present a brief introduction\nto the fundamentals of statistical learning theory, in particular the difference between empirical and\nstructural risk minimization, including one of its most prominent implementations, i.e. the Support\nVector Machine.\nKeywords Machine Learning \u0001Statistical Learning Theory \u0001Supervised Learning\nIntroduction to Statistical Learning Theory\nThe main goal of statistical learning theory [ 4],[2] is to provide a fundamental framework for the problem of decision\nmaking and model construction based on sets of data. Assumptions can be made of the statistical nature about the\nunderlying phenomena. One of the original problems in applying statistical learning theory is that of binary pattern\nrecognition. Here, given two classes of entities, one has to assign a novel unclassiﬁed object to either of the two classes.\nThis problem can be formalized as follow: Suppose we are given mobservations, where each observation iconsists of\nthe dataxi2Rn;i= 1;:::;m , as well as a “ground truth” labeling yi2f\u0000 1;1g. Then, given the data\n(x1;y1);:::;(xm;ym)2X\u0002f\u0000 1;1g (1)\nwe want to estimate a decision function f!X\u0002f\u0000 1;1gthat is able to generalize, i.e. avoid model overﬁtting, to\nunseen data points, i.e. minimizing the expected risk :\nR(\u000b) =Z1\n2jy\u0000f(x;\u000b)jdP(x;y) (2)\nIt is assumed that some unknown probability distribution P(y;x), from which the data is, independently drawn and\nidentically distributed (iid), drawn, does exists. Therefore, if p(y;x)exists,dP(y;x)may be written as p(x;y)dxdy,\nin order to state the true error rate. Statistical learning theory proves the necessity to restrict the set of functions from\nwhichfcan be selected with respect to the capacity su",
    "title": "Optimization Methods for Large-Scale Machine Learning",
    "abstract": "This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.",
    "link": "https://www.semanticscholar.org/paper/d21703674ae562bae4a849a75847cdd9ead417df",
    "published": "2016-06-15"
  },
  "204": {
    "pdf_path": "data/pdfs\\machine learning_paper_97.pdf",
    "text_excerpt": "Financial Time Series Data Processing for Machine\nLearning\nFabrice Daniel\nArtiﬁcial Intelligence Department of Lusis, Paris, France\nfabrice.daniel@lusis.fr\nhttp://www.lusis.fr\nJune 2019\nABSTRACT\nThis article studies the ﬁnancial time series data processing\nfor machine learning. It introduces the most frequent scal-\ning methods, then compares the resulting stationarity and\npreservation of useful information for trend forecasting. It\nproposes an empirical test based on the capability to learn\nsimple data relationship with simple models. It also speaks\nabout the data split method speciﬁc to time series, avoid-\ning unwanted overﬁtting and proposes various labelling for\nclassiﬁcation and regression.\nKeywords : Machine Learning, Financial Time Series, Data\nProcessing\n1 INTRODUCTION\nIn the ﬁeld of machine learning Time Series are very special\ndata needed their speciﬁc processing and methods[1][2].\nOn top of that, Financial data adds a big challenge due\nto their proportion of randomness and their non-stationary\nnature[4][3].\nThere is a lot of research relative to the Financial Market\nforecast with Machine Learning[5].\nHowever, many studies only cover one type of data scal-\ning or labelling while the decisions made on this step can\nhave a huge impact on the results. Not only in term of pure\nmodel performances metrics but in term of capabilities to\nreally implement a proﬁtable trading strategy based on the\nmodel.\nThis study covers the following points:\n• Pre-processing and Stationarity\n• Pre-processing and preservation of useful prices rela-\ntionships\n• Labelling for classiﬁers and regressors2 STATIONARITY\nBefore to work on any price forecast model we need to pre-\nprocess our historical prices then we have to make sure the\nresulting data are stationary.\nWe evaluate three of the most frequent pre-processing,\nstarting by the price returns, then two scaling methods:\nMinMax andStandardization .\nFor this purpose, we use the SPY daily closing prices\nbetween 1993 and 2019.\nFigure 1:",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "205": {
    "pdf_path": "data/pdfs\\machine learning_paper_98.pdf",
    "text_excerpt": "Cerulli G. (2020) #, # 1{22\nMachine Learning using Stata/Python\nGiovanni Cerulli\nIRCrES-CNR\nRome, Italy\ngiovanni.cerulli@ircres.cnr.it\nAbstract. We present two related Stata modules, rmlstata and cmlstata ,\nfor \ftting popular Machine Learning (ML) methods both in a regression and a\nclassi\fcation setting. Using the recent Stata/Python integration platform (s\f)\nof Stata 16, these commands provide hyper-parameters' optimal tuning via K-fold\ncross-validation using greed search. More speci\fcally, they make use of the Python\nScikit-learn API to carry out both cross-validation and outcome/label prediction.\nKeywords: Machine Learning, Stata, Python, Optimal tuning\n1 Introduction\nMachine learning (ML) has emerged as a leading data science approach in many \felds of\nhuman activities, including business, engineering, medicine, advertisement, and scien-\nti\fc research. Placing itself in the intersection between statistics, computer science, and\narti\fcial intelligence, ML's main objective is turning information into valuable knowl-\nedge by \\letting the data speak\", limiting the model's prior assumptions, and promoting\na model-free philosophy. Relying on algorithms and computational techniques, more\nthan on analytic solutions, ML targets Big Data and complexity reduction, although\nsometimes at the expense of results' interpretability (Hastie, Tibshirani, and Friedman,\n2001; Varian, 2014).\nUnlike other software such as R, Python, Matlab, and SAS, Stata has not dedicated\nbuilt-in packages for \ftting ML algorithms, if one excludes the Lasso package of Stata\n16 (StataCorp, 2019). Recently, however, the Stata community has developed some\npopular ML routines that Stata users can suitably exploit. Among them, we mention\nSchonlau (2005) implementing a Boosting Stata plugin; Guenther and Schonlau (2016)\nproviding a command \ftting Support Vector Machines (SVM); Ahrens, Hansen, and\nScha\u000ber (2020) setting out the lassopack , a set of commands for model selection and\nprediction with regularized",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "206": {
    "pdf_path": "data/pdfs\\machine learning_paper_99.pdf",
    "text_excerpt": "Pen & Paper\nExercises in Machine Learning\nMichael U. Gutmann\nUniversity of EdinburgharXiv:2206.13446v1  [cs.LG]  27 Jun 2022This work is licensed under the Creative Commons Attribution 4.0 International License\n\u0000\u0001. To view a copy of this license, visit http://creativecommons.org/licenses/by/\n4.0/.Contents\nPreface vii\n1 Linear Algebra 1\n1.1 Gram–Schmidt orthogonalisation . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2 Linear transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.3 Eigenvalue decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.4 Trace, determinants and eigenvalues . . . . . . . . . . . . . . . . . . . . . . 9\n1.5 Eigenvalue decomposition for symmetric matrices . . . . . . . . . . . . . . . 9\n1.6 Power method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2 Optimisation 15\n2.1 Gradient of vector-valued functions . . . . . . . . . . . . . . . . . . . . . . . 16\n2.2 Newton’s method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.3 Gradient of matrix-valued functions . . . . . . . . . . . . . . . . . . . . . . . 21\n2.4 Gradient of the log-determinant . . . . . . . . . . . . . . . . . . . . . . . . . 24\n2.5 Descent directions for matrix-valued functions . . . . . . . . . . . . . . . . . 26\n3 Directed Graphical Models 27\n3.1 Directed graph concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.2 Canonical connections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.3 Ordered and local Markov properties, d-separation . . . . . . . . . . . . . . 32\n3.4 More on ordered and local Markov properties, d-separation . . . . . . . . . 34\n3.5 Chest clinic (based on Barber, 2012, Exercise 3.3) . . . . . . . . . . . . . . . . 36\n3.6 More on the chest clinic (based on Barber, 2012, Exercise 3.3) . . . . . . . . . 37\n3.7 Hidden Markov models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38iv CONTENTS\n3.8 Alternative characte",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  }
}
